{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2KSgEniMO/KcpDb4zYn7B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saoudyahya/NLP-TP/blob/main/Multi_agent_AgentiRag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "ZwRiiXMzP5Aw",
        "outputId": "e998d2ed-3e61-4d75-c896-7f1a2dcb53ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<svg viewBox=\"0 0 800 600\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "  <!-- Background -->\n",
              "  <rect width=\"800\" height=\"600\" fill=\"#f8f9fa\" rx=\"10\" ry=\"10\"/>\n",
              "\n",
              "  <!-- Title -->\n",
              "  <text x=\"400\" y=\"40\" font-family=\"Arial\" font-size=\"24\" text-anchor=\"middle\" font-weight=\"bold\">Multi-Agent RAG System Architecture</text>\n",
              "\n",
              "  <!-- User and Query -->\n",
              "  <rect x=\"350\" y=\"80\" width=\"100\" height=\"40\" rx=\"5\" ry=\"5\" fill=\"#6495ED\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"400\" y=\"105\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">User Query</text>\n",
              "\n",
              "  <!-- Query Router -->\n",
              "  <rect x=\"325\" y=\"160\" width=\"150\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#FF7F50\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"400\" y=\"190\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Query Router</text>\n",
              "\n",
              "  <!-- Connector from User Query to Router -->\n",
              "  <line x1=\"400\" y1=\"120\" x2=\"400\" y2=\"160\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"400,160 396,150 404,150\" fill=\"#000\"/>\n",
              "\n",
              "  <!-- Agent boxes -->\n",
              "  <rect x=\"100\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"160\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Math Agent</text>\n",
              "\n",
              "  <rect x=\"260\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"320\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Science Agent</text>\n",
              "\n",
              "  <rect x=\"420\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"480\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">History Agent</text>\n",
              "\n",
              "  <rect x=\"580\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"640\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">General Agent</text>\n",
              "\n",
              "  <!-- Vector Store boxes -->\n",
              "  <rect x=\"100\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"160\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">Math Vector DB</text>\n",
              "\n",
              "  <rect x=\"260\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"320\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">Science Vector DB</text>\n",
              "\n",
              "  <rect x=\"420\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"480\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">History Vector DB</text>\n",
              "\n",
              "  <rect x=\"580\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"640\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">General Vector DB</text>\n",
              "\n",
              "  <!-- LLM Processing -->\n",
              "  <rect x=\"325\" y=\"460\" width=\"150\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#FF6347\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"400\" y=\"490\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">LLM Processing</text>\n",
              "\n",
              "  <!-- Response Synthesis -->\n",
              "  <rect x=\"325\" y=\"540\" width=\"150\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#4682B4\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"400\" y=\"570\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Response Synthesis</text>\n",
              "\n",
              "  <!-- Connectors from Router to Agents -->\n",
              "  <line x1=\"350\" y1=\"210\" x2=\"160\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"160,260 168,252 170,260\" fill=\"#000\"/>\n",
              "  <text x=\"240\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Math Query</text>\n",
              "\n",
              "  <line x1=\"375\" y1=\"210\" x2=\"320\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"320,260 325,250 330,255\" fill=\"#000\"/>\n",
              "  <text x=\"340\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Science Query</text>\n",
              "\n",
              "  <line x1=\"425\" y1=\"210\" x2=\"480\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"480,260 475,250 470,255\" fill=\"#000\"/>\n",
              "  <text x=\"460\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">History Query</text>\n",
              "\n",
              "  <line x1=\"450\" y1=\"210\" x2=\"640\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"640,260 632,252 630,260\" fill=\"#000\"/>\n",
              "  <text x=\"560\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">General Query</text>\n",
              "\n",
              "  <!-- Connectors from Agents to Vector Stores -->\n",
              "  <line x1=\"160\" y1=\"310\" x2=\"160\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"160,360 156,350 164,350\" fill=\"#000\"/>\n",
              "\n",
              "  <line x1=\"320\" y1=\"310\" x2=\"320\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"320,360 316,350 324,350\" fill=\"#000\"/>\n",
              "\n",
              "  <line x1=\"480\" y1=\"310\" x2=\"480\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"480,360 476,350 484,350\" fill=\"#000\"/>\n",
              "\n",
              "  <line x1=\"640\" y1=\"310\" x2=\"640\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"640,360 636,350 644,350\" fill=\"#000\"/>\n",
              "\n",
              "  <!-- Connectors from Vector Stores to LLM -->\n",
              "  <polyline points=\"160,410 160,485 325,485\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
              "  <polygon points=\"325,485 315,481 315,489\" fill=\"#000\"/>\n",
              "\n",
              "  <polyline points=\"320,410 320,470 325,470\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
              "  <polygon points=\"325,470 315,466 315,474\" fill=\"#000\"/>\n",
              "\n",
              "  <polyline points=\"480,410 480,470 475,470\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
              "  <polygon points=\"475,470 485,466 485,474\" fill=\"#000\"/>\n",
              "\n",
              "  <polyline points=\"640,410 640,485 475,485\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
              "  <polygon points=\"475,485 485,481 485,489\" fill=\"#000\"/>\n",
              "\n",
              "  <!-- Connector from LLM to Response -->\n",
              "  <line x1=\"400\" y1=\"510\" x2=\"400\" y2=\"540\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"400,540 396,530 404,530\" fill=\"#000\"/>\n",
              "\n",
              "  <!-- File Ingestion Process -->\n",
              "  <rect x=\"80\" y=\"120\" width=\"160\" height=\"40\" rx=\"5\" ry=\"5\" fill=\"#8A2BE2\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"160\" y=\"145\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">Document Ingestion</text>\n",
              "\n",
              "  <!-- Connector showing document flow -->\n",
              "  <polyline points=\"160,160 160,210 325,210\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\" stroke-dasharray=\"5,5\"/>\n",
              "  <polygon points=\"325,210 315,206 315,214\" fill=\"#000\"/>\n",
              "  <text x=\"240\" y=\"190\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Categorize & Store</text>\n",
              "\n",
              "  <!-- Output to User -->\n",
              "  <polyline points=\"400,590 400,610 650,610 650,100 450,100\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\" stroke-dasharray=\"5,5\"/>\n",
              "  <polygon points=\"450,100 460,96 460,104\" fill=\"#000\"/>\n",
              "  <text x=\"550\" y=\"605\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Response to User</text>\n",
              "</svg>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title AGENTIC RAG DEMO\n",
        "\n",
        "%%html\n",
        "<svg viewBox=\"0 0 800 600\" xmlns=\"http://www.w3.org/2000/svg\">\n",
        "  <!-- Background -->\n",
        "  <rect width=\"800\" height=\"600\" fill=\"#f8f9fa\" rx=\"10\" ry=\"10\"/>\n",
        "\n",
        "  <!-- Title -->\n",
        "  <text x=\"400\" y=\"40\" font-family=\"Arial\" font-size=\"24\" text-anchor=\"middle\" font-weight=\"bold\">Multi-Agent RAG System Architecture</text>\n",
        "\n",
        "  <!-- User and Query -->\n",
        "  <rect x=\"350\" y=\"80\" width=\"100\" height=\"40\" rx=\"5\" ry=\"5\" fill=\"#6495ED\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"400\" y=\"105\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">User Query</text>\n",
        "\n",
        "  <!-- Query Router -->\n",
        "  <rect x=\"325\" y=\"160\" width=\"150\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#FF7F50\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"400\" y=\"190\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Query Router</text>\n",
        "\n",
        "  <!-- Connector from User Query to Router -->\n",
        "  <line x1=\"400\" y1=\"120\" x2=\"400\" y2=\"160\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"400,160 396,150 404,150\" fill=\"#000\"/>\n",
        "\n",
        "  <!-- Agent boxes -->\n",
        "  <rect x=\"100\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"160\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Math Agent</text>\n",
        "\n",
        "  <rect x=\"260\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"320\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Science Agent</text>\n",
        "\n",
        "  <rect x=\"420\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"480\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">History Agent</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"640\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">General Agent</text>\n",
        "\n",
        "  <!-- Vector Store boxes -->\n",
        "  <rect x=\"100\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"160\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">Math Vector DB</text>\n",
        "\n",
        "  <rect x=\"260\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"320\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">Science Vector DB</text>\n",
        "\n",
        "  <rect x=\"420\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"480\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">History Vector DB</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"640\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">General Vector DB</text>\n",
        "\n",
        "  <!-- LLM Processing -->\n",
        "  <rect x=\"325\" y=\"460\" width=\"150\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#FF6347\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"400\" y=\"490\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">LLM Processing</text>\n",
        "\n",
        "  <!-- Response Synthesis -->\n",
        "  <rect x=\"325\" y=\"540\" width=\"150\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#4682B4\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"400\" y=\"570\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Response Synthesis</text>\n",
        "\n",
        "  <!-- Connectors from Router to Agents -->\n",
        "  <line x1=\"350\" y1=\"210\" x2=\"160\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"160,260 168,252 170,260\" fill=\"#000\"/>\n",
        "  <text x=\"240\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Math Query</text>\n",
        "\n",
        "  <line x1=\"375\" y1=\"210\" x2=\"320\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"320,260 325,250 330,255\" fill=\"#000\"/>\n",
        "  <text x=\"340\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Science Query</text>\n",
        "\n",
        "  <line x1=\"425\" y1=\"210\" x2=\"480\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"480,260 475,250 470,255\" fill=\"#000\"/>\n",
        "  <text x=\"460\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">History Query</text>\n",
        "\n",
        "  <line x1=\"450\" y1=\"210\" x2=\"640\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"640,260 632,252 630,260\" fill=\"#000\"/>\n",
        "  <text x=\"560\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">General Query</text>\n",
        "\n",
        "  <!-- Connectors from Agents to Vector Stores -->\n",
        "  <line x1=\"160\" y1=\"310\" x2=\"160\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"160,360 156,350 164,350\" fill=\"#000\"/>\n",
        "\n",
        "  <line x1=\"320\" y1=\"310\" x2=\"320\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"320,360 316,350 324,350\" fill=\"#000\"/>\n",
        "\n",
        "  <line x1=\"480\" y1=\"310\" x2=\"480\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"480,360 476,350 484,350\" fill=\"#000\"/>\n",
        "\n",
        "  <line x1=\"640\" y1=\"310\" x2=\"640\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"640,360 636,350 644,350\" fill=\"#000\"/>\n",
        "\n",
        "  <!-- Connectors from Vector Stores to LLM -->\n",
        "  <polyline points=\"160,410 160,485 325,485\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
        "  <polygon points=\"325,485 315,481 315,489\" fill=\"#000\"/>\n",
        "\n",
        "  <polyline points=\"320,410 320,470 325,470\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
        "  <polygon points=\"325,470 315,466 315,474\" fill=\"#000\"/>\n",
        "\n",
        "  <polyline points=\"480,410 480,470 475,470\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
        "  <polygon points=\"475,470 485,466 485,474\" fill=\"#000\"/>\n",
        "\n",
        "  <polyline points=\"640,410 640,485 475,485\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
        "  <polygon points=\"475,485 485,481 485,489\" fill=\"#000\"/>\n",
        "\n",
        "  <!-- Connector from LLM to Response -->\n",
        "  <line x1=\"400\" y1=\"510\" x2=\"400\" y2=\"540\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"400,540 396,530 404,530\" fill=\"#000\"/>\n",
        "\n",
        "  <!-- File Ingestion Process -->\n",
        "  <rect x=\"80\" y=\"120\" width=\"160\" height=\"40\" rx=\"5\" ry=\"5\" fill=\"#8A2BE2\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"160\" y=\"145\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">Document Ingestion</text>\n",
        "\n",
        "  <!-- Connector showing document flow -->\n",
        "  <polyline points=\"160,160 160,210 325,210\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\" stroke-dasharray=\"5,5\"/>\n",
        "  <polygon points=\"325,210 315,206 315,214\" fill=\"#000\"/>\n",
        "  <text x=\"240\" y=\"190\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Categorize & Store</text>\n",
        "\n",
        "  <!-- Output to User -->\n",
        "  <polyline points=\"400,590 400,610 650,610 650,100 450,100\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\" stroke-dasharray=\"5,5\"/>\n",
        "  <polygon points=\"450,100 460,96 460,104\" fill=\"#000\"/>\n",
        "  <text x=\"550\" y=\"605\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Response to User</text>\n",
        "</svg>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9QcKe1BMSw25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5427e89-955e-41f8-d83c-da0736037ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.51)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.23)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (4.13.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain_community langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n9CXDG-S212",
        "outputId": "b2875353-1e25-4e84-ebd0-00c6718f6353"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.11/dist-packages (20250327)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 PDFLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV1isfduS5_4",
        "outputId": "add520a7-5a2f-43d4-c013-04a46dcf335f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement PDFLoader (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for PDFLoader\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c59RnDFRS4s6",
        "outputId": "c6eb407a-3242-417d-d4ac-4c4fe4c8ee70"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-Pkq7QNS_2S",
        "outputId": "f1c94cd3-5a9c-4545-a1c2-3706bee74586"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.2)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.52b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import tempfile\n",
        "from typing import List, Dict, Any, Optional, Tuple, BinaryIO, Callable\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.vectorstores import FAISS, Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PDFMinerLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "Ow85DZ9SQIEZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RAGConfig:\n",
        "    def __init__(self):\n",
        "        # Local model configuration - using publicly available models\n",
        "        self.embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"  # Public embedding model\n",
        "        self.llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Public LLM model\n",
        "\n",
        "        # Model parameters\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.torch_dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "\n",
        "        # Vector database configuration\n",
        "        self.vector_db_path = \"chroma_db\"  # Changed from vector_db to chroma_db\n",
        "\n",
        "        # Document processing configuration\n",
        "        self.chunk_size = 500\n",
        "        self.chunk_overlap = 100\n",
        "\n",
        "        # Search configuration\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.5\n",
        "\n",
        "        # Agent configuration\n",
        "        self.max_iterations = 3  # Reduced for faster execution\n",
        "        self.thinking_steps = True\n",
        "\n",
        "        # Multi-agent configuration\n",
        "        self.agent_types = [\"math\", \"science\", \"history\", \"general\"]\n",
        "        self.vector_db_paths = {\n",
        "            \"math\": \"chroma_db_math\",\n",
        "            \"science\": \"chroma_db_science\",\n",
        "            \"history\": \"chroma_db_history\",\n",
        "            \"general\": \"chroma_db_general\"\n",
        "        }\n"
      ],
      "metadata": {
        "id": "aYOX5k0eQL2h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Embedding class for document and query encoding - implement Embeddings interface\n",
        "class SentenceTransformerEmbedding(Embeddings):\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading embedding model {config.embedding_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.embedding_model_name)\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            config.embedding_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(\"Embedding model loaded successfully\")\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
        "        return self._get_embeddings(texts)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for a query string.\"\"\"\n",
        "        embeddings = self._get_embeddings([text])\n",
        "        return embeddings[0]\n",
        "\n",
        "    def _get_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Internal method to generate embeddings for a list of texts.\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        for text in texts:\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # Use the mean of the last hidden state as the embedding\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().tolist()\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "FoxdDi5QQPll"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Document processor for loading and processing uploaded files\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap\n",
        "        )\n",
        "        self.temp_dir = None\n",
        "\n",
        "    def process_uploaded_file(self, file_obj: BinaryIO, filename: str) -> List[Document]:\n",
        "        \"\"\"Process a single uploaded file.\"\"\"\n",
        "        # Create a temporary directory if not already created\n",
        "        if self.temp_dir is None:\n",
        "            self.temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "        # Get file extension\n",
        "        _, file_extension = os.path.splitext(filename)\n",
        "        file_extension = file_extension.lower()\n",
        "\n",
        "        # Save the file to the temporary directory\n",
        "        temp_file_path = os.path.join(self.temp_dir, filename)\n",
        "        with open(temp_file_path, 'wb') as f:\n",
        "            f.write(file_obj.read())\n",
        "\n",
        "        # Select appropriate loader based on file extension\n",
        "        loader = None\n",
        "        if file_extension == '.txt':\n",
        "            loader = TextLoader(temp_file_path)\n",
        "        elif file_extension == '.pdf':\n",
        "            loader = PDFMinerLoader(temp_file_path)\n",
        "        elif file_extension == '.md':\n",
        "            loader = UnstructuredMarkdownLoader(temp_file_path)\n",
        "        elif file_extension == '.csv':\n",
        "            loader = CSVLoader(temp_file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "        # Load and process the document\n",
        "        documents = loader.load()\n",
        "        chunks = self.process_documents(documents)\n",
        "\n",
        "        return chunks"
      ],
      "metadata": {
        "id": "t0FyFpXMQTWD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Document processor for loading and processing uploaded files\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap\n",
        "        )\n",
        "        self.temp_dir = None\n",
        "\n",
        "    def process_uploaded_file(self, file_obj: BinaryIO, filename: str) -> List[Document]:\n",
        "        \"\"\"Process a single uploaded file.\"\"\"\n",
        "        # Create a temporary directory if not already created\n",
        "        if self.temp_dir is None:\n",
        "            self.temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "        # Get file extension\n",
        "        _, file_extension = os.path.splitext(filename)\n",
        "        file_extension = file_extension.lower()\n",
        "\n",
        "        # Save the file to the temporary directory\n",
        "        temp_file_path = os.path.join(self.temp_dir, filename)\n",
        "        with open(temp_file_path, 'wb') as f:\n",
        "            f.write(file_obj.read())\n",
        "\n",
        "        # Select appropriate loader based on file extension\n",
        "        loader = None\n",
        "        if file_extension == '.txt':\n",
        "            loader = TextLoader(temp_file_path)\n",
        "        elif file_extension == '.pdf':\n",
        "            loader = PDFMinerLoader(temp_file_path)\n",
        "        elif file_extension == '.md':\n",
        "            loader = UnstructuredMarkdownLoader(temp_file_path)\n",
        "        elif file_extension == '.csv':\n",
        "            loader = CSVLoader(temp_file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "        # Load and process the document\n",
        "        documents = loader.load()\n",
        "        chunks = self.process_documents(documents)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks for embedding.\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for doc in documents:\n",
        "            try:\n",
        "                doc_chunks = self.text_splitter.split_documents([doc])\n",
        "                chunks.extend(doc_chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {doc.metadata.get('source', 'unknown')}: {e}\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Remove temporary files when done.\"\"\"\n",
        "        if self.temp_dir and os.path.exists(self.temp_dir):\n",
        "            import shutil\n",
        "            shutil.rmtree(self.temp_dir)\n",
        "            self.temp_dir = None\n",
        "\n"
      ],
      "metadata": {
        "id": "5sEdx-l_QaY_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Vector store for document storage and retrieval\n",
        "class VectorStore:\n",
        "    def __init__(self, config: RAGConfig, embedding_model: SentenceTransformerEmbedding, agent_type: str = \"general\"):\n",
        "        self.config = config\n",
        "        self.embedding_model = embedding_model\n",
        "        self.vector_store = None\n",
        "        self.agent_type = agent_type\n",
        "\n",
        "        # Define a persistent directory for ChromaDB based on agent type\n",
        "        self.persist_directory = config.vector_db_paths.get(agent_type, config.vector_db_path)\n",
        "\n",
        "    def create_vector_store(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Create a vector store from documents.\"\"\"\n",
        "        # Use Chroma.from_documents method\n",
        "        self.vector_store = Chroma.from_documents(\n",
        "            documents,\n",
        "            self.embedding_model,\n",
        "            persist_directory=self.persist_directory\n",
        "        )\n",
        "\n",
        "        # Persist the data\n",
        "        self.vector_store.persist()\n",
        "\n",
        "    def add_documents(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Add documents to an existing vector store.\"\"\"\n",
        "        if self.vector_store is None:\n",
        "            # If no vector store exists, create a new one\n",
        "            self.create_vector_store(documents)\n",
        "        else:\n",
        "            # Add documents to existing vector store\n",
        "            self.vector_store.add_documents(documents)\n",
        "            # Persist the updated vector store\n",
        "            self.vector_store.persist()\n",
        "\n",
        "    def load_vector_store(self) -> bool:\n",
        "        \"\"\"Load the vector store if it exists.\"\"\"\n",
        "        # Check if the persist directory exists\n",
        "        if os.path.exists(self.persist_directory):\n",
        "            try:\n",
        "                self.vector_store = Chroma(\n",
        "                    persist_directory=self.persist_directory,\n",
        "                    embedding_function=self.embedding_model\n",
        "                )\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading vector store: {e}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "    def similarity_search(self, query: str) -> List[Document]:\n",
        "        \"\"\"Search for similar documents to the query.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not initialized. Please create or load a vector store first.\")\n",
        "\n",
        "        # Use similarity_search_with_score method\n",
        "        results = self.vector_store.similarity_search_with_score(\n",
        "            query,\n",
        "            k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        # Filter results by similarity threshold\n",
        "        # Note: ChromaDB returns distance (lower is better), similar to FAISS\n",
        "        filtered_results = [\n",
        "            doc for doc, score in results\n",
        "            if 1.0 / (1.0 + score) >= self.config.similarity_threshold  # Convert distance to similarity\n",
        "        ]\n",
        "\n",
        "        return filtered_results\n"
      ],
      "metadata": {
        "id": "rtJhZ_ldQeUm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Local LLM for RAG\n",
        "class LocalLLM:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading LLM model {config.llm_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.llm_model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            config.llm_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(f\"LLM model loaded successfully on {config.device}\")\n",
        "\n",
        "    def generate(self, prompt: str, system_message: str = None) -> str:\n",
        "        \"\"\"Generate text using local model.\"\"\"\n",
        "        # Format the messages\n",
        "        if system_message:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        else:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        try:\n",
        "            # Format messages for chat format\n",
        "            formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        except Exception as e:\n",
        "            # Fallback if the model doesn't support chat templates\n",
        "            print(f\"Chat template error: {e}, using simple prompt formatting\")\n",
        "            formatted_prompt = system_message + \"\\n\\n\" + prompt if system_message else prompt\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.config.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=300,  # Adjust based on use case\n",
        "                temperature=0.5,  # Lower temperature for faster convergence\n",
        "                do_sample=False,  # Use deterministic generation\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        # Decode the output, removing the input tokens\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        response_tokens = outputs[0][input_length:]\n",
        "        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "\n"
      ],
      "metadata": {
        "id": "Vmuyp_1XQhQY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Router class to direct queries to the appropriate agent\n",
        "class QueryRouter:\n",
        "    def __init__(self, config: RAGConfig, llm: LocalLLM):\n",
        "        self.config = config\n",
        "        self.llm = llm\n",
        "        self.agent_types = config.agent_types\n",
        "\n",
        "    def route_query(self, query: str) -> str:\n",
        "        \"\"\"Route the query to the appropriate agent type.\"\"\"\n",
        "        system_message = \"\"\"You are a query classifier. Your job is to determine which specialized agent\n",
        "        should handle a given query. Choose exactly one of the following categories:\n",
        "        - math: for mathematical questions, calculations, equations, etc.\n",
        "        - science: for questions about physics, chemistry, biology, etc.\n",
        "        - history: for questions about historical events, figures, periods, etc.\n",
        "        - general: for general knowledge, common sense, or any query that doesn't clearly fit the other categories.\n",
        "\n",
        "        Respond ONLY with the category name, nothing else.\"\"\"\n",
        "\n",
        "        prompt = f\"Query: {query}\\n\\nPlease classify this query into exactly one of these categories: math, science, history, or general.\"\n",
        "\n",
        "        response = self.llm.generate(prompt, system_message).strip().lower()\n",
        "\n",
        "        # Extract the category from the response (in case the model adds extra text)\n",
        "        for agent_type in self.agent_types:\n",
        "            if agent_type in response:\n",
        "                return agent_type\n",
        "\n",
        "        # Default to general if no clear category was detected\n",
        "        print(f\"No clear category detected in response: '{response}'. Defaulting to 'general'.\")\n",
        "        return \"general\"\n",
        "\n"
      ],
      "metadata": {
        "id": "cqhBk-cbQkny"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Specialized agent for a specific domain\n",
        "class SpecializedAgent:\n",
        "    def __init__(self, config: RAGConfig, agent_type: str, embedding_model: SentenceTransformerEmbedding, llm: LocalLLM):\n",
        "        self.config = config\n",
        "        self.agent_type = agent_type\n",
        "        self.embedding_model = embedding_model\n",
        "        self.document_processor = DocumentProcessor(config)\n",
        "        self.vector_store = VectorStore(config, embedding_model, agent_type)\n",
        "        self.llm = llm\n",
        "\n",
        "        # Specialized system prompts for different agent types\n",
        "        self.system_prompts = {\n",
        "            \"math\": \"\"\"You are a mathematics expert. Provide clear, step-by-step solutions to mathematical problems.\n",
        "            When analyzing equations or working with numbers, carefully break down each step of the calculation.\n",
        "            Explain mathematical concepts in an intuitive way with relevant examples.\"\"\",\n",
        "\n",
        "            \"science\": \"\"\"You are a science expert. Explain scientific concepts with precision and accuracy.\n",
        "            Relate scientific principles to real-world applications when possible.\n",
        "            Use appropriate terminology while making complex ideas accessible.\"\"\",\n",
        "\n",
        "            \"history\": \"\"\"You are a history expert. Provide nuanced historical context and accurate chronology.\n",
        "            Consider multiple perspectives when discussing historical events and figures.\n",
        "            Connect historical facts to broader themes and patterns where relevant.\"\"\",\n",
        "\n",
        "            \"general\": \"\"\"You are a knowledgeable assistant with access to a knowledge base.\n",
        "            Your task is to provide accurate, relevant information based on the query and the retrieved context.\n",
        "            Think step by step and analyze the retrieved information carefully before formulating your final response.\"\"\"\n",
        "        }\n",
        "\n",
        "        # Load the vector store if it exists\n",
        "        self.vector_store.load_vector_store()\n",
        "\n",
        "    def ingest_document(self, file_obj: BinaryIO, filename: str) -> None:\n",
        "        \"\"\"Ingest a document into this agent's knowledge base.\"\"\"\n",
        "        print(f\"Processing uploaded file for {self.agent_type} agent: {filename}...\")\n",
        "\n",
        "        try:\n",
        "            # Process the uploaded file\n",
        "            chunks = self.document_processor.process_uploaded_file(file_obj, filename)\n",
        "            print(f\"Created {len(chunks)} chunks from {filename}.\")\n",
        "\n",
        "            # Enrich metadata with agent type\n",
        "            for chunk in chunks:\n",
        "                chunk.metadata[\"agent_type\"] = self.agent_type\n",
        "\n",
        "            # Try to load the vector store first\n",
        "            vector_store_exists = self.vector_store.load_vector_store()\n",
        "\n",
        "            if vector_store_exists:\n",
        "                # Add the new documents to the existing vector store\n",
        "                print(f\"Adding documents to existing {self.agent_type} vector store...\")\n",
        "                self.vector_store.add_documents(chunks)\n",
        "            else:\n",
        "                # Create a new vector store if none exists\n",
        "                print(f\"Creating new {self.agent_type} vector store...\")\n",
        "                self.vector_store.create_vector_store(chunks)\n",
        "\n",
        "            print(f\"Successfully ingested {filename} for {self.agent_type} agent.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error ingesting file {filename} for {self.agent_type} agent: {e}\")\n",
        "            raise\n",
        "\n",
        "    def query(self, user_query: str) -> str:\n",
        "        \"\"\"Process a query using this specialized agent.\"\"\"\n",
        "        # Get the appropriate system message for this agent type\n",
        "        system_message = self.system_prompts.get(self.agent_type, self.system_prompts[\"general\"])\n",
        "\n",
        "        # Retrieve documents\n",
        "        try:\n",
        "            # First check if vector_store is initialized\n",
        "            if self.vector_store.vector_store is None:\n",
        "                print(f\"Vector store not initialized for {self.agent_type} agent. Attempting to load...\")\n",
        "                vector_store_loaded = self.vector_store.load_vector_store()\n",
        "                if not vector_store_loaded:\n",
        "                    print(f\"No existing vector store for {self.agent_type} agent. Proceeding without context retrieval.\")\n",
        "                    retrieved_docs = []\n",
        "            else:\n",
        "                results = self.vector_store.vector_store.similarity_search_with_score(\n",
        "                    user_query,\n",
        "                    k=self.config.top_k\n",
        "                )\n",
        "\n",
        "                # Display retrieved documents with scores\n",
        "                print(f\"\\nRetrieved Documents for {self.agent_type} agent:\")\n",
        "                if results:\n",
        "                    for i, (doc, score) in enumerate(results):\n",
        "                        print(f\"\\nDocument {i+1} (Score: {score}):\")\n",
        "                        print(doc.page_content)\n",
        "                        if hasattr(doc, 'metadata') and doc.metadata:\n",
        "                            print(f\"Metadata: {doc.metadata}\")\n",
        "                        print(\"-\" * 50)\n",
        "\n",
        "                    # Filter for actual processing\n",
        "                    retrieved_docs = [doc for doc, score in results\n",
        "                                    if 1.0 / (1.0 + score) >= self.config.similarity_threshold]\n",
        "                else:\n",
        "                    print(f\"No relevant documents found for {self.agent_type} agent.\")\n",
        "                    retrieved_docs = []\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error retrieving documents for {self.agent_type} agent: {e}\")\n",
        "            retrieved_docs = []\n",
        "\n",
        "        # Agentic thinking process with specialized knowledge\n",
        "        response = self._agentic_process(user_query, retrieved_docs, system_message)\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _agentic_process(self, user_query: str, retrieved_docs: List[Document], system_message: str) -> str:\n",
        "        \"\"\"Execute the agentic process for responding to queries with domain-specific expertise.\"\"\"\n",
        "        # Initial retrieval if not provided\n",
        "        if not retrieved_docs:\n",
        "            # Handle the case when no relevant documents are found\n",
        "            prompt = f\"\"\"Query: {user_query}\n",
        "\n",
        "            No relevant documents were found in the {self.agent_type} knowledge base. Please provide a general response based on your {self.agent_type} expertise.\n",
        "            \"\"\"\n",
        "            return self.llm.generate(prompt, system_message)\n",
        "\n",
        "        # For agentic reasoning, we'll use a multi-step process\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "        iteration_responses = []\n",
        "        current_query = user_query\n",
        "\n",
        "        for iteration in range(self.config.max_iterations):\n",
        "            print(f\"Iteration {iteration+1}/{self.config.max_iterations} for {self.agent_type} agent\")\n",
        "\n",
        "            # Check if we should continue\n",
        "            if iteration > 0 and not self._should_continue(current_query, iteration_responses[-1]):\n",
        "                break\n",
        "\n",
        "            # Generate thinking steps if enabled\n",
        "            thinking = \"\"\n",
        "            if self.config.thinking_steps:\n",
        "                thinking_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Think step by step about this query from a {self.agent_type} perspective. What are the key points to address?\n",
        "                What information from the context is most relevant? What additional {self.agent_type} knowledge might be needed?\n",
        "                \"\"\"\n",
        "                thinking = self.llm.generate(thinking_prompt, system_message)\n",
        "\n",
        "            # Generate the response\n",
        "            response_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            {thinking if thinking else \"\"}\n",
        "\n",
        "            Based on the context provided and your expertise in {self.agent_type}, please answer the query.\n",
        "            If the context doesn't contain enough information, acknowledge this and provide the best answer you can using your {self.agent_type} knowledge.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.llm.generate(response_prompt, system_message)\n",
        "            iteration_responses.append(response)\n",
        "\n",
        "            # Generate follow-up questions or refinements\n",
        "            refinement_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Your current response:\n",
        "            {response}\n",
        "\n",
        "            Are there aspects of the query that haven't been fully addressed from a {self.agent_type} perspective?\n",
        "            What follow-up questions would help provide a more complete answer? How could the search be refined?\n",
        "            \"\"\"\n",
        "\n",
        "            refinement = self.llm.generate(refinement_prompt, system_message)\n",
        "\n",
        "            # Extract a new query for the next iteration\n",
        "            new_query_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "            Current response:\n",
        "            {response}\n",
        "\n",
        "            Refinement thoughts:\n",
        "            {refinement}\n",
        "\n",
        "            Based on the above, formulate a new search query that would help address any gaps in the current response from a {self.agent_type} perspective.\n",
        "            Return ONLY the new query without any explanation.\n",
        "            If you believe the query has been fully addressed, return \"COMPLETE\".\n",
        "            \"\"\"\n",
        "\n",
        "            new_query = self.llm.generate(new_query_prompt, system_message).strip()\n",
        "\n",
        "            if new_query == \"COMPLETE\" or new_query.upper().startswith(\"COMPLETE\"):\n",
        "                break\n",
        "\n",
        "            # Perform a new search with the refined query\n",
        "            current_query = new_query\n",
        "            # Inside the iteration loop in _agentic_process\n",
        "            new_docs = self.vector_store.similarity_search(current_query)\n",
        "\n",
        "            if new_docs:\n",
        "                print(f\"\\nAdditional Documents for Iteration {iteration+1} ({self.agent_type}):\")\n",
        "                for i, doc in enumerate(new_docs):\n",
        "                    print(f\"\\nDocument {i+1}:\")\n",
        "                    print(doc.page_content)\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "                new_context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(new_docs)])\n",
        "                # Update context with new information\n",
        "                context = f\"{context}\\n\\nAdditional Context:\\n{new_context}\"\n",
        "\n",
        "        # Final synthesis\n",
        "        final_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "        Iterations of responses from {self.agent_type} agent:\n",
        "        {' '.join([f\"Iteration {i+1}: {resp}\" for i, resp in enumerate(iteration_responses)])}\n",
        "\n",
        "        Please provide a final, comprehensive response to the original query that synthesizes all the information gathered across iterations.\n",
        "        Apply your {self.agent_type} expertise to ensure the answer is accurate, complete, and well-explained.\n",
        "        \"\"\"\n",
        "\n",
        "        final_response = self.llm.generate(final_prompt, system_message)\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    def _should_continue(self, query: str, last_response: str) -> bool:\n",
        "        \"\"\"Determine if the agent should continue iterating.\"\"\"\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Current response from {self.agent_type} agent:\n",
        "        {last_response}\n",
        "\n",
        "        Does this response fully address the query from a {self.agent_type} perspective?\n",
        "        If yes, respond with \"COMPLETE\". If not, respond with \"CONTINUE\" and briefly explain why.\n",
        "        \"\"\"\n",
        "\n",
        "        decision = self.llm.generate(prompt)\n",
        "        return \"CONTINUE\" in decision.upper()\n",
        "\n"
      ],
      "metadata": {
        "id": "dBGzo-DMRA35"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Multi-agent RAG coordinator\n",
        "class MultiAgentRAG:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "\n",
        "        # Initialize shared components\n",
        "        self.embedding_model = SentenceTransformerEmbedding(config)\n",
        "        self.llm = LocalLLM(config)\n",
        "        self.document_processor = DocumentProcessor(config)\n",
        "\n",
        "        # Initialize the router\n",
        "        self.router = QueryRouter(config, self.llm)\n",
        "\n",
        "        # Initialize specialized agents\n",
        "        self.agents = {}\n",
        "        for agent_type in config.agent_types:\n",
        "            print(f\"Initializing {agent_type} agent...\")\n",
        "            self.agents[agent_type] = SpecializedAgent(config, agent_type, self.embedding_model, self.llm)\n",
        "\n",
        "    def ingest_uploaded_file(self, file_obj: BinaryIO, filename: str, agent_type: str = None) -> None:\n",
        "        \"\"\"Ingest a file into one or all agents' knowledge bases.\"\"\"\n",
        "        try:\n",
        "            # Save a copy of the original file content for multiple uses\n",
        "            file_content = file_obj.read()\n",
        "\n",
        "            if agent_type and agent_type in self.agents:\n",
        "                # Create a BytesIO object from the file content for the specific agent\n",
        "                print(f\"Ingesting document for {agent_type} agent: {filename}\")\n",
        "                file_copy = BytesIOWrapper(file_content)\n",
        "                self.agents[agent_type].ingest_document(file_copy, filename)\n",
        "            else:\n",
        "                # Determine the agent type from the filename or content\n",
        "                determined_agent_type = self._determine_document_type(filename, file_content)\n",
        "                print(f\"Auto-determined document type: {determined_agent_type}\")\n",
        "\n",
        "                # Create a BytesIO object from the file content\n",
        "                file_copy = BytesIOWrapper(file_content)\n",
        "                self.agents[determined_agent_type].ingest_document(file_copy, filename)\n",
        "\n",
        "            print(f\"Successfully ingested {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading and ingesting file: {e}\")\n",
        "            # Print more details to debug\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    def _determine_document_type(self, filename: str, file_content: bytes) -> str:\n",
        "        \"\"\"Determine the document type to route it to the appropriate agent.\"\"\"\n",
        "        # Basic method: use keywords in filename or first few lines\n",
        "        filename_lower = filename.lower()\n",
        "        content_sample = file_content[:5000].decode('utf-8', errors='ignore').lower()\n",
        "\n",
        "        # Simple keyword matching - can be replaced with more sophisticated analysis\n",
        "        if any(math_term in filename_lower or math_term in content_sample\n",
        "               for math_term in [\"math\", \"calculus\", \"algebra\", \"equation\", \"theorem\", \"number\"]):\n",
        "            return \"math\"\n",
        "        elif any(science_term in filename_lower or science_term in content_sample\n",
        "                for science_term in [\"science\", \"physics\", \"chemistry\", \"biology\", \"experiment\"]):\n",
        "            return \"science\"\n",
        "        elif any(history_term in filename_lower or history_term in content_sample\n",
        "                for history_term in [\"history\", \"ancient\", \"century\", \"timeline\", \"war\", \"civilization\"]):\n",
        "            return \"history\"\n",
        "        else:\n",
        "            return \"general\"\n",
        "\n",
        "    def query(self, user_query: str) -> str:\n",
        "        \"\"\"Process a query using the router and appropriate agent(s).\"\"\"\n",
        "        # First, route the query to determine which agent should handle it\n",
        "        start_time = time.time()\n",
        "        print(f\"Routing query: {user_query}\")\n",
        "        agent_type = self.router.route_query(user_query)\n",
        "        print(f\"Query routed to {agent_type} agent\")\n",
        "\n",
        "        # Get the response from the appropriate agent\n",
        "        response = self.agents[agent_type].query(user_query)\n",
        "\n",
        "        # Check if we need responses from other agents as well\n",
        "        if self._needs_multiple_agents(user_query, response):\n",
        "            print(\"Query might benefit from multiple agents' perspectives\")\n",
        "            all_responses = {}\n",
        "\n",
        "            # Collect responses from all agents or a subset of relevant ones\n",
        "            for other_type, agent in self.agents.items():\n",
        "                if other_type != agent_type:\n",
        "                    print(f\"Getting additional perspective from {other_type} agent\")\n",
        "                    all_responses[other_type] = agent.query(user_query)\n",
        "\n",
        "            # Synthesize the responses\n",
        "            response = self._synthesize_responses(user_query, agent_type, response, all_responses)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Total query processing time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _needs_multiple_agents(self, query: str, primary_response: str) -> bool:\n",
        "        \"\"\"Determine if a query needs insights from multiple agents.\"\"\"\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Primary response: {primary_response}\n",
        "\n",
        "        Analyze whether this query requires perspectives from multiple domains of knowledge.\n",
        "        Does it touch on multiple subjects (e.g., both math and science, or history and general knowledge)?\n",
        "        Would the answer benefit significantly from additional domain expertise?\n",
        "\n",
        "        Respond with \"YES\" if multiple agents should provide input, or \"NO\" if the primary response is sufficient.\n",
        "        \"\"\"\n",
        "\n",
        "        system_message = \"\"\"You are a query analyzer determining whether a question spans multiple domains of knowledge.\n",
        "        Be conservative - only recommend multiple agents if there's significant cross-domain value.\"\"\"\n",
        "\n",
        "        decision = self.llm.generate(prompt, system_message).strip().upper()\n",
        "        return \"YES\" in decision\n",
        "\n",
        "    def _synthesize_responses(self, query: str, primary_agent: str, primary_response: str,\n",
        "                              other_responses: Dict[str, str]) -> str:\n",
        "        \"\"\"Synthesize responses from multiple agents into a coherent answer.\"\"\"\n",
        "        # Format all the responses\n",
        "        responses_text = f\"Primary response ({primary_agent}):\\n{primary_response}\\n\\n\"\n",
        "\n",
        "        for agent_type, response in other_responses.items():\n",
        "            responses_text += f\"{agent_type.capitalize()} perspective:\\n{response}\\n\\n\"\n",
        "\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Different expert perspectives:\n",
        "        {responses_text}\n",
        "\n",
        "        Synthesize these different expert perspectives into a comprehensive, coherent response.\n",
        "        Highlight where the different perspectives complement each other and provide a more complete answer.\n",
        "        Ensure the final response is well-structured and addresses all aspects of the original query.\n",
        "        \"\"\"\n",
        "\n",
        "        system_message = \"\"\"You are a synthesis expert combining insights from multiple domain experts.\n",
        "        Create a unified response that preserves the valuable contributions from each expert while removing redundancies.\n",
        "        Acknowledge the different domains of expertise when they provide unique perspectives.\"\"\"\n",
        "\n",
        "        synthesized_response = self.llm.generate(prompt, system_message)\n",
        "\n",
        "        return synthesized_response\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up temporary files in all agents.\"\"\"\n",
        "        self.document_processor.cleanup()\n",
        "        for agent in self.agents.values():\n",
        "            agent.document_processor.cleanup()"
      ],
      "metadata": {
        "id": "xdI0QJdaRE-P"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility class for creating BytesIO-like objects from bytes\n",
        "class BytesIOWrapper:\n",
        "    def __init__(self, data: bytes):\n",
        "        self.data = data\n",
        "        self.position = 0\n",
        "\n",
        "    def read(self, size: int = -1) -> bytes:\n",
        "        if size == -1:\n",
        "            result = self.data[self.position:]\n",
        "            self.position = len(self.data)\n",
        "            return result\n",
        "        else:\n",
        "            result = self.data[self.position:self.position + size]\n",
        "            self.position += size\n",
        "            return result\n",
        "\n",
        "    def seek(self, position: int) -> int:\n",
        "        self.position = position\n",
        "        return self.position\n",
        "\n",
        "    def tell(self) -> int:\n",
        "        return self.position\n",
        "\n",
        "    def close(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.close()"
      ],
      "metadata": {
        "id": "8udarLKjRHNE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_in_colab():\n",
        "    from google.colab import files\n",
        "    import io\n",
        "    import time\n",
        "\n",
        "    config = RAGConfig()\n",
        "    print(f\"Initializing MultiAgentRAG with models on {config.device}\")\n",
        "    rag_system = MultiAgentRAG(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nMultiAgentRAG with Specialized Experts\")\n",
        "        print(\"1. Upload and ingest a file\")\n",
        "        print(\"2. Upload and ingest a file for a specific agent\")\n",
        "        print(\"3. Ask a question\")\n",
        "        print(\"4. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-4): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            try:\n",
        "                print(\"Please select a file to upload...\")\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                for filename, content in uploaded.items():\n",
        "                    file_obj = io.BytesIO(content)\n",
        "                    rag_system.ingest_uploaded_file(file_obj, filename)\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading and ingesting file: {e}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            try:\n",
        "                print(\"Available agents: \" + \", \".join(config.agent_types))\n",
        "                agent_type = input(\"Enter the agent type for this document: \").lower()\n",
        "\n",
        "                if agent_type not in config.agent_types:\n",
        "                    print(f\"Invalid agent type. Please choose from: {', '.join(config.agent_types)}\")\n",
        "                    continue\n",
        "\n",
        "                print(\"Please select a file to upload...\")\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                for filename, content in uploaded.items():\n",
        "                    file_obj = io.BytesIO(content)\n",
        "                    rag_system.ingest_uploaded_file(file_obj, filename, agent_type)\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading and ingesting file: {e}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            query = input(\"Enter your question: \")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                response = rag_system.query(query)\n",
        "                end_time = time.time()\n",
        "\n",
        "                print(f\"\\nResponse: {response}\")\n",
        "                print(f\"Query processed in {end_time - start_time:.2f} seconds\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing query: {e}\")\n",
        "\n",
        "        elif choice == \"4\":\n",
        "            print(\"Exiting MultiAgentRAG system. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter a number between 1 and 4.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_in_colab()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZvB6Xg_1RLet",
        "outputId": "43224782-7bf8-4761-cec1-d12ef216e417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing MultiAgentRAG with models on cpu\n",
            "Loading embedding model sentence-transformers/all-mpnet-base-v2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model loaded successfully\n",
            "Loading LLM model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "LLM model loaded successfully on cpu\n",
            "Initializing math agent...\n",
            "Initializing science agent...\n",
            "Initializing history agent...\n",
            "Initializing general agent...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-ace172245177>:40: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  self.vector_store = Chroma(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MultiAgentRAG with Specialized Experts\n",
            "1. Upload and ingest a file\n",
            "2. Upload and ingest a file for a specific agent\n",
            "3. Ask a question\n",
            "4. Exit\n",
            "Enter your choice (1-4): 2\n",
            "Available agents: math, science, history, general\n",
            "Enter the agent type for this document: math\n",
            "Please select a file to upload...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-254faa29-54b9-4c96-a1c2-768c3fb10392\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-254faa29-54b9-4c96-a1c2-768c3fb10392\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Basic Math Review Card.pdf to Basic Math Review Card (3).pdf\n",
            "Ingesting document for math agent: Basic Math Review Card (3).pdf\n",
            "Processing uploaded file for math agent: Basic Math Review Card (3).pdf...\n",
            "Created 118 chunks from Basic Math Review Card (3).pdf.\n",
            "Creating new math vector store...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-ace172245177>:22: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  self.vector_store.persist()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully ingested Basic Math Review Card (3).pdf for math agent.\n",
            "Successfully ingested Basic Math Review Card (3).pdf\n",
            "\n",
            "MultiAgentRAG with Specialized Experts\n",
            "1. Upload and ingest a file\n",
            "2. Upload and ingest a file for a specific agent\n",
            "3. Ask a question\n",
            "4. Exit\n",
            "Enter your choice (1-4): 2\n",
            "Available agents: math, science, history, general\n",
            "Enter the agent type for this document: history\n",
            "Please select a file to upload...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b9bfe167-e072-43ed-9718-1c248729b2bc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b9bfe167-e072-43ed-9718-1c248729b2bc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Morroco.pdf to Morroco (4).pdf\n",
            "Ingesting document for history agent: Morroco (4).pdf\n",
            "Processing uploaded file for history agent: Morroco (4).pdf...\n",
            "Created 7 chunks from Morroco (4).pdf.\n",
            "Creating new history vector store...\n",
            "Successfully ingested Morroco (4).pdf for history agent.\n",
            "Successfully ingested Morroco (4).pdf\n",
            "\n",
            "MultiAgentRAG with Specialized Experts\n",
            "1. Upload and ingest a file\n",
            "2. Upload and ingest a file for a specific agent\n",
            "3. Ask a question\n",
            "4. Exit\n",
            "Enter your choice (1-4): 3\n",
            "Enter your question: history of Morocco?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Routing query: history of Morocco?\n",
            "Query routed to history agent\n",
            "\n",
            "Retrieved Documents for history agent:\n",
            "\n",
            "Document 1 (Score: 4.919137001037598):\n",
            "History of Morocco\n",
            "\n",
            " Morocco is known to be inhabited since 8000 b.c.\n",
            " In the Classical Antiquity era, Morocco experienced \n",
            "\n",
            "waves of invaders included Phoenicians, \n",
            "Carthaginians, Romans, Vandals, Byzantines, Visigoths, \n",
            "Berbers and Arabs who bring Islam. \n",
            "\n",
            " Several dynasties have succeeded one another over \n",
            "\n",
            "the years: the Idrisside dynasty, the Almoravid dynasty, \n",
            "the Almohad dynasty, the Merinid dynasty, the Saadian \n",
            "dynasty and the Alaouite dynasty.\n",
            "Metadata: {'producer': 'Microsoft PowerPoint 2019', 'agent_type': 'history', 'creationdate': '2022-12-22T17:12:56+01:00', 'moddate': '2022-12-22T17:12:56+01:00', 'title': 'Prsentation PowerPoint', 'creator': 'Microsoft PowerPoint 2019', 'source': '/tmp/tmpg4m1vg4s/Morroco (4).pdf', 'author': 'Bahija GOUIMI', 'total_pages': 11}\n",
            "--------------------------------------------------\n",
            "\n",
            "Document 2 (Score: 6.214073181152344):\n",
            " Morocco has a unique geography that puts it in the \n",
            "\n",
            "center of the world.\n",
            "\fThe Kingdom of Morocco\n",
            "\n",
            " King: The King of Morocco is the Head of State of Morocco. The \n",
            "\n",
            "current King of Morocco is Mohammed VI.\n",
            "\n",
            " The national animal: The Berber Lion, the national emblem of \n",
            "\n",
            "Morocco.\n",
            "\n",
            " Population: Morocco has approximately 36 million inhabitants.\n",
            " Religion: islam is the official religion in Morocco. There are also \n",
            "\n",
            "Christians and more Moroccan Jews in Morocco.\n",
            "Metadata: {'title': 'Prsentation PowerPoint', 'creator': 'Microsoft PowerPoint 2019', 'creationdate': '2022-12-22T17:12:56+01:00', 'moddate': '2022-12-22T17:12:56+01:00', 'agent_type': 'history', 'total_pages': 11, 'author': 'Bahija GOUIMI', 'source': '/tmp/tmpg4m1vg4s/Morroco (4).pdf', 'producer': 'Microsoft PowerPoint 2019'}\n",
            "--------------------------------------------------\n",
            "\n",
            "Document 3 (Score: 6.318358421325684):\n",
            " Morocco was a French protectorate from 1912 to 1956\n",
            "\fLocation\n",
            "\n",
            " Morocco, officially the Kingdom of Morocco. In Arabic, \n",
            "\n",
            "the country is called Al-Maghrib. \n",
            "\n",
            " Morocco is located in the Maghreb, and is one of the \n",
            "\n",
            "most western countries in Africa. \n",
            "\n",
            " Situated in North-West Africa, Morocco is bordered \n",
            "\n",
            "to the north by the Strait of Gibraltar and the \n",
            "Mediterranean Sea, to the south by Mauritania, to \n",
            "the east by Algeria, and to the west by the Atlantic \n",
            "Ocean.\n",
            "Metadata: {'total_pages': 11, 'author': 'Bahija GOUIMI', 'source': '/tmp/tmpg4m1vg4s/Morroco (4).pdf', 'agent_type': 'history', 'title': 'Prsentation PowerPoint', 'creationdate': '2022-12-22T17:12:56+01:00', 'creator': 'Microsoft PowerPoint 2019', 'producer': 'Microsoft PowerPoint 2019', 'moddate': '2022-12-22T17:12:56+01:00'}\n",
            "--------------------------------------------------\n",
            "\n",
            "Document 4 (Score: 6.37505578994751):\n",
            " Morocco has been, for centuries, a \n",
            "\n",
            "meeting point for the Arabo-\n",
            "Islamic culture and civilization as \n",
            "well as a land of tolerance, \n",
            "dialogue and openness.\n",
            "\n",
            " Morocco has gained international \n",
            "consideration as a multicultural \n",
            "country, with several types of \n",
            "heritage recognized as World \n",
            "Heritage by UNESCO.\n",
            "\fMorocco Cities \n",
            "\n",
            "RABAT\n",
            "\n",
            "CASABLANCA\n",
            "\n",
            "FEZ\n",
            "\n",
            "CHEFCHAOUEN\n",
            "\n",
            "MARRAKECH\n",
            "\fMarrakech\n",
            "Metadata: {'source': '/tmp/tmpg4m1vg4s/Morroco (4).pdf', 'total_pages': 11, 'producer': 'Microsoft PowerPoint 2019', 'moddate': '2022-12-22T17:12:56+01:00', 'title': 'Prsentation PowerPoint', 'creationdate': '2022-12-22T17:12:56+01:00', 'agent_type': 'history', 'creator': 'Microsoft PowerPoint 2019', 'author': 'Bahija GOUIMI'}\n",
            "--------------------------------------------------\n",
            "\n",
            "Document 5 (Score: 6.743041515350342):\n",
            " Marrakech suffers the same episodes as the rest of \n",
            "Morocco where the Portuguese, the Spanish and \n",
            "the French attacked the country to control the natural \n",
            "resources and its privileged location as the door to Africa.\n",
            "\fMost visited tourist \n",
            "attractions in Marrakech\n",
            "\n",
            "Jama El-Fna Square\n",
            "\n",
            "Menara Basin\n",
            "\n",
            "Majorelle Garden\n",
            "\n",
            "Meseums \n",
            "\n",
            "Historical Monuments\n",
            "\n",
            "Medina and sooks\n",
            "\fMarrakech region\n",
            "\fMust-try dishes of Marrakech\n",
            "Metadata: {'title': 'Prsentation PowerPoint', 'creator': 'Microsoft PowerPoint 2019', 'moddate': '2022-12-22T17:12:56+01:00', 'source': '/tmp/tmpg4m1vg4s/Morroco (4).pdf', 'total_pages': 11, 'agent_type': 'history', 'author': 'Bahija GOUIMI', 'producer': 'Microsoft PowerPoint 2019', 'creationdate': '2022-12-22T17:12:56+01:00'}\n",
            "--------------------------------------------------\n",
            "Total query processing time: 166.09 seconds\n",
            "\n",
            "Response: <|assistant|>\n",
            "Morocco has a rich history that dates back to the 7th century when the Berber tribes established a kingdom in the region. The country has been ruled by various empires, including the Almoravid, Almohad, and Marinid dynasties, among others.\n",
            "\n",
            "The Moroccan history is marked by significant events such as the establishment of the Almoravid dynasty in the 11th century, the conquest of Granada by the Almohad Caliphate in the 12th century, and the establishment of the Marinid dynasty in the 14th century.\n",
            "\n",
            "During the 19th century, Morocco was ruled by the French, who introduced modernization and colonialism. The country experienced significant political and social upheaval during this period, with the establishment of the National Liberation Front in 1956 and the establishment of the modern Moroccan state in 1956.\n",
            "\n",
            "Today, Morocco is a modern and democratic country with a diverse population, including Berber, Arab, and African communities. The country has a rich cultural heritage, including the Berber music, art, and architecture.\n",
            "\n",
            "In conclusion, Morocco has a rich history that spans over 1,400 years. The country has been ruled by various empires, experienced\n",
            "Query processed in 166.09 seconds\n",
            "\n",
            "MultiAgentRAG with Specialized Experts\n",
            "1. Upload and ingest a file\n",
            "2. Upload and ingest a file for a specific agent\n",
            "3. Ask a question\n",
            "4. Exit\n"
          ]
        }
      ]
    }
  ]
}