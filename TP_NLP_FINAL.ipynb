{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOt+PBalDLVj3En7Keg2gNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saoudyahya/NLP-TP/blob/main/TP_NLP_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QcKe1BMSw25"
      },
      "outputs": [],
      "source": [
        "pip install langchain_community langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n9CXDG-S212",
        "outputId": "08314efa-134b-4ae7-d380-c608d05932b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/5.6 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20250327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 PDFLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV1isfduS5_4",
        "outputId": "78078783-544b-46b1-ff51-f3c72ee4029f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement PDFLoader (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for PDFLoader\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c59RnDFRS4s6",
        "outputId": "d05c1cd0-55d0-41d2-9967-8357cbba1ad0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9HFDmtRS8S8",
        "outputId": "1dfa58ab-e237-4af2-a2e2-6793eb5e210b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.23.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.23.3-py3-none-any.whl (46.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.3 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-Pkq7QNS_2S",
        "outputId": "6228a317-7e4c-4d0a-ed40-5317158fc0fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.23.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.1)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=b8bf5ee611ea15c02265b5b0e67cfff8bfcbac973c39618e0a82af7263664453\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, build, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.46.1\n",
            "    Uninstalling starlette-0.46.1:\n",
            "      Successfully uninstalled starlette-0.46.1\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.115.12\n",
            "    Uninstalling fastapi-0.115.12:\n",
            "      Successfully uninstalled fastapi-0.115.12\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.3 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.0 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-util-http-0.52b1 overrides-7.7.0 posthog-3.23.0 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.45.3 uvloop-0.21.0 watchfiles-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title AGENTIC RAG DEMO\n",
        "\n",
        "%%html\n",
        "\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<svg viewBox=\"0 0 800 600\" xmlns=\"http://www.w3.org/2000/svg\">\n",
        "  <!-- Background -->\n",
        "  <rect width=\"800\" height=\"600\" fill=\"#f8f9fa\" rx=\"10\" ry=\"10\"/>\n",
        "\n",
        "  <!-- Title -->\n",
        "  <text x=\"400\" y=\"40\" font-family=\"Arial\" font-size=\"24\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#333\">Agentic RAG System Architecture</text>\n",
        "\n",
        "  <!-- Document Processing Section -->\n",
        "  <rect x=\"40\" y=\"80\" width=\"200\" height=\"140\" fill=\"#e3f2fd\" stroke=\"#2196f3\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
        "  <text x=\"140\" y=\"100\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#1565c0\">Document Processing</text>\n",
        "  <text x=\"140\" y=\"125\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">DocumentProcessor</text>\n",
        "  <text x=\"140\" y=\"150\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Load documents</text>\n",
        "  <text x=\"140\" y=\"170\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Split into chunks</text>\n",
        "  <text x=\"140\" y=\"190\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Process metadata</text>\n",
        "\n",
        "  <!-- Embedding Model Section -->\n",
        "  <rect x=\"40\" y=\"240\" width=\"200\" height=\"140\" fill=\"#e8f5e9\" stroke=\"#4caf50\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
        "  <text x=\"140\" y=\"260\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#2e7d32\">Embedding Model</text>\n",
        "  <text x=\"140\" y=\"285\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">SentenceTransformerEmbedding</text>\n",
        "  <text x=\"140\" y=\"310\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Document embeddings</text>\n",
        "  <text x=\"140\" y=\"330\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Query embeddings</text>\n",
        "  <text x=\"140\" y=\"350\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Semantic encoding</text>\n",
        "\n",
        "  <!-- Vector Store Section -->\n",
        "  <rect x=\"300\" y=\"80\" width=\"200\" height=\"140\" fill=\"#fff3e0\" stroke=\"#ff9800\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
        "  <text x=\"400\" y=\"100\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#e65100\">Vector Store</text>\n",
        "  <text x=\"400\" y=\"125\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">Chroma</text>\n",
        "  <text x=\"400\" y=\"150\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Store document embeddings</text>\n",
        "  <text x=\"400\" y=\"170\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Similarity search</text>\n",
        "  <text x=\"400\" y=\"190\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Document retrieval</text>\n",
        "\n",
        "  <!-- LLM Section -->\n",
        "  <rect x=\"300\" y=\"240\" width=\"200\" height=\"140\" fill=\"#f3e5f5\" stroke=\"#9c27b0\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
        "  <text x=\"400\" y=\"260\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#6a1b9a\">Local LLM</text>\n",
        "  <text x=\"400\" y=\"285\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">TinyLlama-1.1B-Chat</text>\n",
        "  <text x=\"400\" y=\"310\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Response generation</text>\n",
        "  <text x=\"400\" y=\"330\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Query refinement</text>\n",
        "  <text x=\"400\" y=\"350\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Self-reflection</text>\n",
        "\n",
        "  <!-- Agentic Process Section -->\n",
        "  <rect x=\"560\" y=\"80\" width=\"200\" height=\"300\" fill=\"#e8eaf6\" stroke=\"#3f51b5\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
        "  <text x=\"660\" y=\"100\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#283593\">Agentic Process</text>\n",
        "  <text x=\"660\" y=\"125\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">_agentic_process</text>\n",
        "\n",
        "  <!-- Inside agentic process -->\n",
        "  <rect x=\"580\" y=\"140\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
        "  <text x=\"660\" y=\"160\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">1. Initial Document Retrieval</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"180\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
        "  <text x=\"660\" y=\"200\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">2. Thinking Steps</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"220\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
        "  <text x=\"660\" y=\"240\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">3. Generate Response</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"260\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
        "  <text x=\"660\" y=\"280\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">4. Evaluate Completeness</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"300\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
        "  <text x=\"660\" y=\"320\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">5. Query Refinement</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"340\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
        "  <text x=\"660\" y=\"360\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">6. Final Synthesis</text>\n",
        "\n",
        "  <!-- Main Workflow -->\n",
        "  <rect x=\"40\" y=\"400\" width=\"720\" height=\"160\" fill=\"#fce4ec\" stroke=\"#e91e63\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
        "  <text x=\"400\" y=\"420\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#c2185b\">Multi-Iteration Workflow</text>\n",
        "\n",
        "  <!-- Workflow steps - FIXED LEFT TO RIGHT FLOW -->\n",
        "  <circle cx=\"100\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"100\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">1</text>\n",
        "  <text x=\"100\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Query</text>\n",
        "\n",
        "  <polygon points=\"130,460 150,450 150,470\" fill=\"#880e4f\"/>\n",
        "\n",
        "  <circle cx=\"180\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"180\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">2</text>\n",
        "  <text x=\"180\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Retrieve</text>\n",
        "\n",
        "  <polygon points=\"210,460 230,450 230,470\" fill=\"#880e4f\"/>\n",
        "\n",
        "  <circle cx=\"260\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"260\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">3</text>\n",
        "  <text x=\"260\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Think</text>\n",
        "\n",
        "  <polygon points=\"290,460 310,450 310,470\" fill=\"#880e4f\"/>\n",
        "\n",
        "  <circle cx=\"340\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"340\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">4</text>\n",
        "  <text x=\"340\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Generate</text>\n",
        "\n",
        "  <polygon points=\"370,460 390,450 390,470\" fill=\"#880e4f\"/>\n",
        "\n",
        "  <circle cx=\"420\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"420\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">5</text>\n",
        "  <text x=\"420\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Evaluate</text>\n",
        "\n",
        "  <!-- Conditional branch -->\n",
        "  <path d=\"M 450,460 L 470,460 L 470,430 L 490,430\" fill=\"none\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"470\" y=\"420\" font-family=\"Arial\" font-size=\"10\" text-anchor=\"middle\">Complete</text>\n",
        "\n",
        "  <path d=\"M 450,460 L 470,460 L 470,490 L 490,490\" fill=\"none\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"470\" y=\"510\" font-family=\"Arial\" font-size=\"10\" text-anchor=\"middle\">Continue</text>\n",
        "\n",
        "  <circle cx=\"520\" cy=\"430\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"520\" y=\"435\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">6A</text>\n",
        "  <text x=\"520\" y=\"470\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Synthesize</text>\n",
        "\n",
        "  <circle cx=\"520\" cy=\"490\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"520\" y=\"495\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">6B</text>\n",
        "  <text x=\"520\" y=\"530\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Refine</text>\n",
        "\n",
        "  <!-- Feedback loop from Refine back to Retrieve -->\n",
        "  <path d=\"M 545,490 L 580,490 L 580,520 L 180,520 L 180,485\" fill=\"none\" stroke=\"#880e4f\" stroke-width=\"2\" stroke-dasharray=\"5,3\"/>\n",
        "  <polygon points=\"175,495 180,485 185,495\" fill=\"#880e4f\"/>\n",
        "\n",
        "  <polygon points=\"550,430 570,420 570,440\" fill=\"#880e4f\"/>\n",
        "\n",
        "  <circle cx=\"600\" cy=\"430\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"600\" y=\"435\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">7</text>\n",
        "  <text x=\"600\" y=\"470\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Response</text>\n",
        "\n",
        "  <!-- Connections between sections -->\n",
        "  <path d=\"M 140,220 L 140,240\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
        "  <path d=\"M 240,150 L 300,150\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
        "  <path d=\"M 240,310 L 300,310\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
        "  <path d=\"M 500,150 L 560,150\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
        "  <path d=\"M 500,310 L 560,250\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
        "</svg>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "zKvsSG6JXB00",
        "outputId": "99b6b465-660f-467f-b338-fce4071fd9a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
              "<svg viewBox=\"0 0 800 600\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "  <!-- Background -->\n",
              "  <rect width=\"800\" height=\"600\" fill=\"#f8f9fa\" rx=\"10\" ry=\"10\"/>\n",
              "\n",
              "  <!-- Title -->\n",
              "  <text x=\"400\" y=\"40\" font-family=\"Arial\" font-size=\"24\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#333\">Agentic RAG System Architecture</text>\n",
              "\n",
              "  <!-- Document Processing Section -->\n",
              "  <rect x=\"40\" y=\"80\" width=\"200\" height=\"140\" fill=\"#e3f2fd\" stroke=\"#2196f3\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
              "  <text x=\"140\" y=\"100\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#1565c0\">Document Processing</text>\n",
              "  <text x=\"140\" y=\"125\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">DocumentProcessor</text>\n",
              "  <text x=\"140\" y=\"150\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Load documents</text>\n",
              "  <text x=\"140\" y=\"170\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Split into chunks</text>\n",
              "  <text x=\"140\" y=\"190\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Process metadata</text>\n",
              "\n",
              "  <!-- Embedding Model Section -->\n",
              "  <rect x=\"40\" y=\"240\" width=\"200\" height=\"140\" fill=\"#e8f5e9\" stroke=\"#4caf50\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
              "  <text x=\"140\" y=\"260\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#2e7d32\">Embedding Model</text>\n",
              "  <text x=\"140\" y=\"285\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">SentenceTransformerEmbedding</text>\n",
              "  <text x=\"140\" y=\"310\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Document embeddings</text>\n",
              "  <text x=\"140\" y=\"330\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Query embeddings</text>\n",
              "  <text x=\"140\" y=\"350\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Semantic encoding</text>\n",
              "\n",
              "  <!-- Vector Store Section -->\n",
              "  <rect x=\"300\" y=\"80\" width=\"200\" height=\"140\" fill=\"#fff3e0\" stroke=\"#ff9800\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
              "  <text x=\"400\" y=\"100\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#e65100\">Vector Store</text>\n",
              "  <text x=\"400\" y=\"125\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">Chroma</text>\n",
              "  <text x=\"400\" y=\"150\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Store document embeddings</text>\n",
              "  <text x=\"400\" y=\"170\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Similarity search</text>\n",
              "  <text x=\"400\" y=\"190\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Document retrieval</text>\n",
              "\n",
              "  <!-- LLM Section -->\n",
              "  <rect x=\"300\" y=\"240\" width=\"200\" height=\"140\" fill=\"#f3e5f5\" stroke=\"#9c27b0\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
              "  <text x=\"400\" y=\"260\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#6a1b9a\">Local LLM</text>\n",
              "  <text x=\"400\" y=\"285\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">TinyLlama-1.1B-Chat</text>\n",
              "  <text x=\"400\" y=\"310\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Response generation</text>\n",
              "  <text x=\"400\" y=\"330\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Query refinement</text>\n",
              "  <text x=\"400\" y=\"350\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Self-reflection</text>\n",
              "\n",
              "  <!-- Agentic Process Section -->\n",
              "  <rect x=\"560\" y=\"80\" width=\"200\" height=\"300\" fill=\"#e8eaf6\" stroke=\"#3f51b5\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
              "  <text x=\"660\" y=\"100\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#283593\">Agentic Process</text>\n",
              "  <text x=\"660\" y=\"125\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">_agentic_process</text>\n",
              "\n",
              "  <!-- Inside agentic process -->\n",
              "  <rect x=\"580\" y=\"140\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
              "  <text x=\"660\" y=\"160\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">1. Initial Document Retrieval</text>\n",
              "\n",
              "  <rect x=\"580\" y=\"180\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
              "  <text x=\"660\" y=\"200\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">2. Thinking Steps</text>\n",
              "\n",
              "  <rect x=\"580\" y=\"220\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
              "  <text x=\"660\" y=\"240\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">3. Generate Response</text>\n",
              "\n",
              "  <rect x=\"580\" y=\"260\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
              "  <text x=\"660\" y=\"280\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">4. Evaluate Completeness</text>\n",
              "\n",
              "  <rect x=\"580\" y=\"300\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
              "  <text x=\"660\" y=\"320\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">5. Query Refinement</text>\n",
              "\n",
              "  <rect x=\"580\" y=\"340\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
              "  <text x=\"660\" y=\"360\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">6. Final Synthesis</text>\n",
              "\n",
              "  <!-- Main Workflow -->\n",
              "  <rect x=\"40\" y=\"400\" width=\"720\" height=\"160\" fill=\"#fce4ec\" stroke=\"#e91e63\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
              "  <text x=\"400\" y=\"420\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#c2185b\">Multi-Iteration Workflow</text>\n",
              "\n",
              "  <!-- Workflow steps - FIXED LEFT TO RIGHT FLOW -->\n",
              "  <circle cx=\"100\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"100\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">1</text>\n",
              "  <text x=\"100\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Query</text>\n",
              "\n",
              "  <polygon points=\"130,460 150,450 150,470\" fill=\"#880e4f\"/>\n",
              "\n",
              "  <circle cx=\"180\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"180\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">2</text>\n",
              "  <text x=\"180\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Retrieve</text>\n",
              "\n",
              "  <polygon points=\"210,460 230,450 230,470\" fill=\"#880e4f\"/>\n",
              "\n",
              "  <circle cx=\"260\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"260\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">3</text>\n",
              "  <text x=\"260\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Think</text>\n",
              "\n",
              "  <polygon points=\"290,460 310,450 310,470\" fill=\"#880e4f\"/>\n",
              "\n",
              "  <circle cx=\"340\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"340\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">4</text>\n",
              "  <text x=\"340\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Generate</text>\n",
              "\n",
              "  <polygon points=\"370,460 390,450 390,470\" fill=\"#880e4f\"/>\n",
              "\n",
              "  <circle cx=\"420\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"420\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">5</text>\n",
              "  <text x=\"420\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Evaluate</text>\n",
              "\n",
              "  <!-- Conditional branch -->\n",
              "  <path d=\"M 450,460 L 470,460 L 470,430 L 490,430\" fill=\"none\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"470\" y=\"420\" font-family=\"Arial\" font-size=\"10\" text-anchor=\"middle\">Complete</text>\n",
              "\n",
              "  <path d=\"M 450,460 L 470,460 L 470,490 L 490,490\" fill=\"none\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"470\" y=\"510\" font-family=\"Arial\" font-size=\"10\" text-anchor=\"middle\">Continue</text>\n",
              "\n",
              "  <circle cx=\"520\" cy=\"430\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"520\" y=\"435\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">6A</text>\n",
              "  <text x=\"520\" y=\"470\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Synthesize</text>\n",
              "\n",
              "  <circle cx=\"520\" cy=\"490\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"520\" y=\"495\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">6B</text>\n",
              "  <text x=\"520\" y=\"530\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Refine</text>\n",
              "\n",
              "  <!-- Feedback loop from Refine back to Retrieve -->\n",
              "  <path d=\"M 545,490 L 580,490 L 580,520 L 180,520 L 180,485\" fill=\"none\" stroke=\"#880e4f\" stroke-width=\"2\" stroke-dasharray=\"5,3\"/>\n",
              "  <polygon points=\"175,495 180,485 185,495\" fill=\"#880e4f\"/>\n",
              "\n",
              "  <polygon points=\"550,430 570,420 570,440\" fill=\"#880e4f\"/>\n",
              "\n",
              "  <circle cx=\"600\" cy=\"430\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"600\" y=\"435\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">7</text>\n",
              "  <text x=\"600\" y=\"470\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Response</text>\n",
              "\n",
              "  <!-- Connections between sections -->\n",
              "  <path d=\"M 140,220 L 140,240\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
              "  <path d=\"M 240,150 L 300,150\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
              "  <path d=\"M 240,310 L 300,310\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
              "  <path d=\"M 500,150 L 560,150\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
              "  <path d=\"M 500,310 L 560,250\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
              "</svg>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import tempfile\n",
        "from typing import List, Dict, Any, Optional, Tuple, BinaryIO\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PDFMinerLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "\n",
        "class RAGConfig:\n",
        "    def __init__(self):\n",
        "        # Local model configuration - using publicly available models\n",
        "        self.embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"  # Public embedding model\n",
        "        self.llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Public LLM model\n",
        "\n",
        "        # Model parameters\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.torch_dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "\n",
        "        # Vector database configuration\n",
        "        self.vector_db_path = \"chroma_db\"  # Changed from vector_db to chroma_db\n",
        "\n",
        "        # Document processing configuration\n",
        "        self.chunk_size = 500\n",
        "        self.chunk_overlap = 100\n",
        "\n",
        "        # Search configuration\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.5\n",
        "\n",
        "        # Agent configuration\n",
        "        self.max_iterations = 3  # Reduced for faster execution\n",
        "        self.thinking_steps = True\n",
        "\n",
        "# Embedding class for document and query encoding - implement Embeddings interface\n",
        "class SentenceTransformerEmbedding(Embeddings):\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading embedding model {config.embedding_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.embedding_model_name)\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            config.embedding_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(\"Embedding model loaded successfully\")\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
        "        return self._get_embeddings(texts)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for a query string.\"\"\"\n",
        "        embeddings = self._get_embeddings([text])\n",
        "        return embeddings[0]\n",
        "\n",
        "    def _get_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Internal method to generate embeddings for a list of texts.\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        for text in texts:\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # Use the mean of the last hidden state as the embedding\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().tolist()\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# Document processor for loading and processing uploaded files\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap\n",
        "        )\n",
        "        self.temp_dir = None\n",
        "\n",
        "    def process_uploaded_file(self, file_obj: BinaryIO, filename: str) -> List[Document]:\n",
        "        \"\"\"Process a single uploaded file.\"\"\"\n",
        "        # Create a temporary directory if not already created\n",
        "        if self.temp_dir is None:\n",
        "            self.temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "        # Get file extension\n",
        "        _, file_extension = os.path.splitext(filename)\n",
        "        file_extension = file_extension.lower()\n",
        "\n",
        "        # Save the file to the temporary directory\n",
        "        temp_file_path = os.path.join(self.temp_dir, filename)\n",
        "        with open(temp_file_path, 'wb') as f:\n",
        "            f.write(file_obj.read())\n",
        "\n",
        "        # Select appropriate loader based on file extension\n",
        "        loader = None\n",
        "        if file_extension == '.txt':\n",
        "            loader = TextLoader(temp_file_path)\n",
        "        elif file_extension == '.pdf':\n",
        "            loader = PDFMinerLoader(temp_file_path)\n",
        "        elif file_extension == '.md':\n",
        "            loader = UnstructuredMarkdownLoader(temp_file_path)\n",
        "        elif file_extension == '.csv':\n",
        "            loader = CSVLoader(temp_file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "        # Load and process the document\n",
        "        documents = loader.load()\n",
        "        chunks = self.process_documents(documents)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks for embedding.\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for doc in documents:\n",
        "            try:\n",
        "                doc_chunks = self.text_splitter.split_documents([doc])\n",
        "                chunks.extend(doc_chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {doc.metadata.get('source', 'unknown')}: {e}\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Remove temporary files when done.\"\"\"\n",
        "        if self.temp_dir and os.path.exists(self.temp_dir):\n",
        "            import shutil\n",
        "            shutil.rmtree(self.temp_dir)\n",
        "            self.temp_dir = None\n",
        "\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Vector store for document storage and retrieval\n",
        "class VectorStore:\n",
        "    def __init__(self, config: RAGConfig, embedding_model: SentenceTransformerEmbedding):\n",
        "        self.config = config\n",
        "        self.embedding_model = embedding_model\n",
        "        self.vector_store = None\n",
        "\n",
        "        # Define a persistent directory for ChromaDB\n",
        "        self.persist_directory = self.config.vector_db_path\n",
        "\n",
        "    def create_vector_store(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Create a vector store from documents.\"\"\"\n",
        "        # Use Chroma.from_documents method\n",
        "        self.vector_store = Chroma.from_documents(\n",
        "            documents,\n",
        "            self.embedding_model,\n",
        "            persist_directory=self.persist_directory\n",
        "        )\n",
        "\n",
        "        # Persist the data\n",
        "        self.vector_store.persist()\n",
        "\n",
        "    def add_documents(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Add documents to an existing vector store.\"\"\"\n",
        "        if self.vector_store is None:\n",
        "            # If no vector store exists, create a new one\n",
        "            self.create_vector_store(documents)\n",
        "        else:\n",
        "            # Add documents to existing vector store\n",
        "            self.vector_store.add_documents(documents)\n",
        "            # Persist the updated vector store\n",
        "            self.vector_store.persist()\n",
        "\n",
        "    def load_vector_store(self) -> bool:\n",
        "        \"\"\"Load the vector store if it exists.\"\"\"\n",
        "        # Check if the persist directory exists\n",
        "        if os.path.exists(self.persist_directory):\n",
        "            try:\n",
        "                self.vector_store = Chroma(\n",
        "                    persist_directory=self.persist_directory,\n",
        "                    embedding_function=self.embedding_model\n",
        "                )\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading vector store: {e}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "    def similarity_search(self, query: str) -> List[Document]:\n",
        "        \"\"\"Search for similar documents to the query.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not initialized. Please create or load a vector store first.\")\n",
        "\n",
        "        # Use similarity_search_with_score method\n",
        "        results = self.vector_store.similarity_search_with_score(\n",
        "            query,\n",
        "            k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        # Filter results by similarity threshold\n",
        "        # Note: ChromaDB returns distance (lower is better), similar to FAISS\n",
        "        filtered_results = [\n",
        "            doc for doc, score in results\n",
        "            if 1.0 / (1.0 + score) >= self.config.similarity_threshold  # Convert distance to similarity\n",
        "        ]\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "# Local LLM for RAG\n",
        "class LocalLLM:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading LLM model {config.llm_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.llm_model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            config.llm_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(f\"LLM model loaded successfully on {config.device}\")\n",
        "\n",
        "    def generate(self, prompt: str, system_message: str = None) -> str:\n",
        "        \"\"\"Generate text using local model.\"\"\"\n",
        "        # Format the messages\n",
        "        if system_message:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        else:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        try:\n",
        "            # Format messages for chat format\n",
        "            formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        except Exception as e:\n",
        "            # Fallback if the model doesn't support chat templates\n",
        "            print(f\"Chat template error: {e}, using simple prompt formatting\")\n",
        "            formatted_prompt = system_message + \"\\n\\n\" + prompt if system_message else prompt\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.config.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=300,  # Adjust based on use case\n",
        "                temperature=0.5,  # Lower temperature for faster convergence\n",
        "                do_sample=False,  # Use deterministic generation\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        # Decode the output, removing the input tokens\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        response_tokens = outputs[0][input_length:]\n",
        "        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "\n",
        "\n",
        "# Agentic RAG system that combines all components\n",
        "class AgenticRAG:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.embedding_model = SentenceTransformerEmbedding(config)\n",
        "        self.document_processor = DocumentProcessor(config)\n",
        "        self.vector_store = VectorStore(config, self.embedding_model)\n",
        "        self.llm = LocalLLM(config)\n",
        "\n",
        "    def ingest_uploaded_file(self, file_obj: BinaryIO, filename: str) -> None:\n",
        "        \"\"\"Ingest a single uploaded file.\"\"\"\n",
        "        print(f\"Processing uploaded file: {filename}...\")\n",
        "\n",
        "        try:\n",
        "            # Process the uploaded file\n",
        "            chunks = self.document_processor.process_uploaded_file(file_obj, filename)\n",
        "            print(f\"Created {len(chunks)} chunks from {filename}.\")\n",
        "\n",
        "            # Try to load the vector store first\n",
        "            vector_store_exists = self.vector_store.load_vector_store()\n",
        "\n",
        "            if vector_store_exists:\n",
        "                # Add the new documents to the existing vector store\n",
        "                print(\"Adding documents to existing vector store...\")\n",
        "                self.vector_store.add_documents(chunks)\n",
        "            else:\n",
        "                # Create a new vector store if none exists\n",
        "                print(\"Creating new vector store...\")\n",
        "                self.vector_store.create_vector_store(chunks)\n",
        "\n",
        "            print(f\"Successfully ingested {filename}.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error ingesting file {filename}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def query(self, user_query: str) -> str:\n",
        "        # Existing code...\n",
        "\n",
        "        # Retrieve documents with scores\n",
        "        results = self.vector_store.vector_store.similarity_search_with_score(\n",
        "            user_query,\n",
        "            k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        # Display retrieved documents with scores\n",
        "        print(\"\\nRetrieved Documents:\")\n",
        "        if results:\n",
        "            for i, (doc, score) in enumerate(results):\n",
        "                print(f\"\\nDocument {i+1} (Score: {score}):\")\n",
        "                print(doc.page_content)\n",
        "                if hasattr(doc, 'metadata') and doc.metadata:\n",
        "                    print(f\"Metadata: {doc.metadata}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "            # Filter for the actual processing\n",
        "            retrieved_docs = [doc for doc, score in results\n",
        "                            if 1.0 / (1.0 + score) >= self.config.similarity_threshold]\n",
        "        else:\n",
        "            print(\"No relevant documents found.\")\n",
        "            retrieved_docs = []\n",
        "\n",
        "        # Agentic thinking process\n",
        "        response = self._agentic_process(user_query, retrieved_docs)\n",
        "\n",
        "        return response\n",
        "    def _agentic_process(self, user_query: str, retrieved_docs=None) -> str:\n",
        "        \"\"\"Execute the agentic process for responding to queries.\"\"\"\n",
        "        system_message = \"\"\"You are an intelligent agent with access to a knowledge base.\n",
        "        Your task is to provide accurate, relevant information based on the query and the retrieved context.\n",
        "        Think step by step and analyze the retrieved information carefully before formulating your final response.\"\"\"\n",
        "\n",
        "        # Initial retrieval if not provided\n",
        "        if retrieved_docs is None:\n",
        "            retrieved_docs = self.vector_store.similarity_search(user_query)\n",
        "\n",
        "    # Rest of the method remains the same...\n",
        "        if not retrieved_docs:\n",
        "            # Handle the case when no relevant documents are found\n",
        "            prompt = f\"\"\"Query: {user_query}\n",
        "\n",
        "            No relevant documents were found in the knowledge base. Please provide a general response based on your knowledge.\n",
        "            \"\"\"\n",
        "            return self.llm.generate(prompt, system_message)\n",
        "\n",
        "        # For agentic reasoning, we'll use a multi-step process\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "        iteration_responses = []\n",
        "        current_query = user_query\n",
        "\n",
        "        for iteration in range(self.config.max_iterations):\n",
        "            print(f\"Iteration {iteration+1}/{self.config.max_iterations}\")\n",
        "\n",
        "            # Check if we should continue\n",
        "            if iteration > 0 and not self._should_continue(current_query, iteration_responses[-1]):\n",
        "                break\n",
        "\n",
        "            # Generate thinking steps if enabled\n",
        "            thinking = \"\"\n",
        "            if self.config.thinking_steps:\n",
        "                thinking_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Think step by step about this query. What are the key points to address? What information from the context is most relevant? What additional information might be needed?\n",
        "                \"\"\"\n",
        "                thinking = self.llm.generate(thinking_prompt, system_message)\n",
        "\n",
        "            # Generate the response\n",
        "            response_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            {thinking if thinking else \"\"}\n",
        "\n",
        "            Based on the context provided, please answer the query. If the context doesn't contain enough information, acknowledge this and provide the best answer you can.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.llm.generate(response_prompt, system_message)\n",
        "            iteration_responses.append(response)\n",
        "\n",
        "            # Generate follow-up questions or refinements\n",
        "            refinement_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Your current response:\n",
        "            {response}\n",
        "\n",
        "            Are there aspects of the query that haven't been fully addressed? What follow-up questions would help provide a more complete answer? How could the search be refined?\n",
        "            \"\"\"\n",
        "\n",
        "            refinement = self.llm.generate(refinement_prompt, system_message)\n",
        "\n",
        "            # Extract a new query for the next iteration\n",
        "            new_query_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "            Current response:\n",
        "            {response}\n",
        "\n",
        "            Refinement thoughts:\n",
        "            {refinement}\n",
        "\n",
        "            Based on the above, formulate a new search query that would help address any gaps in the current response. Return ONLY the new query without any explanation.\n",
        "            If you believe the query has been fully addressed, return \"COMPLETE\".\n",
        "            \"\"\"\n",
        "\n",
        "            new_query = self.llm.generate(new_query_prompt, system_message).strip()\n",
        "\n",
        "            if new_query == \"COMPLETE\" or new_query.upper().startswith(\"COMPLETE\"):\n",
        "                break\n",
        "\n",
        "            # Perform a new search with the refined query\n",
        "            current_query = new_query\n",
        "            # Inside the iteration loop in _agentic_process\n",
        "            new_docs = self.vector_store.similarity_search(current_query)\n",
        "\n",
        "            if new_docs:\n",
        "                print(f\"\\nAdditional Documents for Iteration {iteration+1}:\")\n",
        "                for i, doc in enumerate(new_docs):\n",
        "                    print(f\"\\nDocument {i+1}:\")\n",
        "                    print(doc.page_content)\n",
        "                    print(f\"Score: {doc.metadata.get('score', 'N/A')}\")\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "                new_context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(new_docs)])\n",
        "                # Update context with new information\n",
        "                context = f\"{context}\\n\\nAdditional Context:\\n{new_context}\"\n",
        "\n",
        "        # Final synthesis\n",
        "        final_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "        Iterations of responses:\n",
        "        {' '.join([f\"Iteration {i+1}: {resp}\" for i, resp in enumerate(iteration_responses)])}\n",
        "\n",
        "        Please provide a final, comprehensive response to the original query that synthesizes all the information gathered across iterations.\n",
        "        \"\"\"\n",
        "\n",
        "        final_response = self.llm.generate(final_prompt, system_message)\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    def _should_continue(self, query: str, last_response: str) -> bool:\n",
        "        \"\"\"Determine if the agent should continue iterating.\"\"\"\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Current response:\n",
        "        {last_response}\n",
        "\n",
        "        Does this response fully address the query? If yes, respond with \"COMPLETE\". If not, respond with \"CONTINUE\" and briefly explain why.\n",
        "        \"\"\"\n",
        "\n",
        "        decision = self.llm.generate(prompt)\n",
        "        return \"CONTINUE\" in decision.upper()\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up temporary files.\"\"\"\n",
        "        self.document_processor.cleanup()\n",
        "\n",
        "\n",
        "# Google Colab integration for file upload and RAG system\n",
        "def run_in_colab():\n",
        "    from google.colab import files\n",
        "    import io\n",
        "\n",
        "    config = RAGConfig()\n",
        "    print(f\"Initializing AgenticRAG with models on {config.device}\")\n",
        "    rag_system = AgenticRAG(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nAgenticRAG with Local LLM\")\n",
        "        print(\"1. Upload and ingest a file\")\n",
        "        print(\"2. Ask a question\")\n",
        "        print(\"3. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-3): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            try:\n",
        "                print(\"Please select a file to upload...\")\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                for filename, content in uploaded.items():\n",
        "                    file_obj = io.BytesIO(content)\n",
        "                    rag_system.ingest_uploaded_file(file_obj, filename)\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading and ingesting file: {e}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            query = input(\"Enter your question: \")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                response = rag_system.query(query)\n",
        "                end_time = time.time()\n",
        "\n",
        "                print(f\"\\nResponse (took {end_time - start_time:.2f} seconds):\")\n",
        "                print(response)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing query: {e}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Thank you for using AgenticRAG with Local LLM. Goodbye!\")\n",
        "            # Clean up any temporary files\n",
        "            rag_system.cleanup()\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "\n",
        "# For standalone execution outside of Colab\n",
        "def main():\n",
        "    try:\n",
        "        # Check if running in Google Colab\n",
        "        import google.colab\n",
        "        print(\"Running in Google Colab environment\")\n",
        "        run_in_colab()\n",
        "    except ImportError:\n",
        "        print(\"Not running in Google Colab, falling back to command-line interface\")\n",
        "        # Original command-line interface code here\n",
        "        # (You can keep the original code if needed, but it's not the focus now)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "twCszzE3TEw5",
        "outputId": "e6151563-d13b-4995-bf8b-28973c742933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e042ec5e003c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-e042ec5e003c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running in Google Colab environment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mrun_in_colab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not running in Google Colab, falling back to command-line interface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-e042ec5e003c>\u001b[0m in \u001b[0;36mrun_in_colab\u001b[0;34m()\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3. Exit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your choice (1-3): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title AGENTIC RAG DEMO\n",
        "\n",
        "%%html\n",
        "<svg viewBox=\"0 0 800 600\" xmlns=\"http://www.w3.org/2000/svg\">\n",
        "  <!-- Background -->\n",
        "  <rect width=\"800\" height=\"600\" fill=\"#f8f9fa\" rx=\"10\" ry=\"10\"/>\n",
        "\n",
        "  <!-- Title -->\n",
        "  <text x=\"400\" y=\"40\" font-family=\"Arial\" font-size=\"24\" text-anchor=\"middle\" font-weight=\"bold\">Multi-Agent RAG System Architecture</text>\n",
        "\n",
        "  <!-- User and Query -->\n",
        "  <rect x=\"350\" y=\"80\" width=\"100\" height=\"40\" rx=\"5\" ry=\"5\" fill=\"#6495ED\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"400\" y=\"105\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">User Query</text>\n",
        "\n",
        "  <!-- Query Router -->\n",
        "  <rect x=\"325\" y=\"160\" width=\"150\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#FF7F50\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"400\" y=\"190\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Query Router</text>\n",
        "\n",
        "  <!-- Connector from User Query to Router -->\n",
        "  <line x1=\"400\" y1=\"120\" x2=\"400\" y2=\"160\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"400,160 396,150 404,150\" fill=\"#000\"/>\n",
        "\n",
        "  <!-- Agent boxes -->\n",
        "  <rect x=\"100\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"160\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Math Agent</text>\n",
        "\n",
        "  <rect x=\"260\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"320\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Science Agent</text>\n",
        "\n",
        "  <rect x=\"420\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"480\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">History Agent</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"640\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">General Agent</text>\n",
        "\n",
        "  <!-- Vector Store boxes -->\n",
        "  <rect x=\"100\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"160\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">Math Vector DB</text>\n",
        "\n",
        "  <rect x=\"260\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"320\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">Science Vector DB</text>\n",
        "\n",
        "  <rect x=\"420\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"480\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">History Vector DB</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"640\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">General Vector DB</text>\n",
        "\n",
        "  <!-- LLM Processing -->\n",
        "  <rect x=\"325\" y=\"460\" width=\"150\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#FF6347\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"400\" y=\"490\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">LLM Processing</text>\n",
        "\n",
        "  <!-- Response Synthesis -->\n",
        "  <rect x=\"325\" y=\"540\" width=\"150\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#4682B4\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"400\" y=\"570\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Response Synthesis</text>\n",
        "\n",
        "  <!-- Connectors from Router to Agents -->\n",
        "  <line x1=\"350\" y1=\"210\" x2=\"160\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"160,260 168,252 170,260\" fill=\"#000\"/>\n",
        "  <text x=\"240\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Math Query</text>\n",
        "\n",
        "  <line x1=\"375\" y1=\"210\" x2=\"320\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"320,260 325,250 330,255\" fill=\"#000\"/>\n",
        "  <text x=\"340\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Science Query</text>\n",
        "\n",
        "  <line x1=\"425\" y1=\"210\" x2=\"480\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"480,260 475,250 470,255\" fill=\"#000\"/>\n",
        "  <text x=\"460\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">History Query</text>\n",
        "\n",
        "  <line x1=\"450\" y1=\"210\" x2=\"640\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"640,260 632,252 630,260\" fill=\"#000\"/>\n",
        "  <text x=\"560\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">General Query</text>\n",
        "\n",
        "  <!-- Connectors from Agents to Vector Stores -->\n",
        "  <line x1=\"160\" y1=\"310\" x2=\"160\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"160,360 156,350 164,350\" fill=\"#000\"/>\n",
        "\n",
        "  <line x1=\"320\" y1=\"310\" x2=\"320\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"320,360 316,350 324,350\" fill=\"#000\"/>\n",
        "\n",
        "  <line x1=\"480\" y1=\"310\" x2=\"480\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"480,360 476,350 484,350\" fill=\"#000\"/>\n",
        "\n",
        "  <line x1=\"640\" y1=\"310\" x2=\"640\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"640,360 636,350 644,350\" fill=\"#000\"/>\n",
        "\n",
        "  <!-- Connectors from Vector Stores to LLM -->\n",
        "  <polyline points=\"160,410 160,485 325,485\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
        "  <polygon points=\"325,485 315,481 315,489\" fill=\"#000\"/>\n",
        "\n",
        "  <polyline points=\"320,410 320,470 325,470\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
        "  <polygon points=\"325,470 315,466 315,474\" fill=\"#000\"/>\n",
        "\n",
        "  <polyline points=\"480,410 480,470 475,470\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
        "  <polygon points=\"475,470 485,466 485,474\" fill=\"#000\"/>\n",
        "\n",
        "  <polyline points=\"640,410 640,485 475,485\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
        "  <polygon points=\"475,485 485,481 485,489\" fill=\"#000\"/>\n",
        "\n",
        "  <!-- Connector from LLM to Response -->\n",
        "  <line x1=\"400\" y1=\"510\" x2=\"400\" y2=\"540\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <polygon points=\"400,540 396,530 404,530\" fill=\"#000\"/>\n",
        "\n",
        "  <!-- File Ingestion Process -->\n",
        "  <rect x=\"80\" y=\"120\" width=\"160\" height=\"40\" rx=\"5\" ry=\"5\" fill=\"#8A2BE2\" stroke=\"#000\" stroke-width=\"2\"/>\n",
        "  <text x=\"160\" y=\"145\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">Document Ingestion</text>\n",
        "\n",
        "  <!-- Connector showing document flow -->\n",
        "  <polyline points=\"160,160 160,210 325,210\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\" stroke-dasharray=\"5,5\"/>\n",
        "  <polygon points=\"325,210 315,206 315,214\" fill=\"#000\"/>\n",
        "  <text x=\"240\" y=\"190\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Categorize & Store</text>\n",
        "\n",
        "  <!-- Output to User -->\n",
        "  <polyline points=\"400,590 400,610 650,610 650,100 450,100\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\" stroke-dasharray=\"5,5\"/>\n",
        "  <polygon points=\"450,100 460,96 460,104\" fill=\"#000\"/>\n",
        "  <text x=\"550\" y=\"605\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Response to User</text>\n",
        "</svg>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "-S4MeGGLgdTh",
        "outputId": "34d60ced-3f6f-43aa-c47b-c1cd8fdc44b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<svg viewBox=\"0 0 800 600\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "  <!-- Background -->\n",
              "  <rect width=\"800\" height=\"600\" fill=\"#f8f9fa\" rx=\"10\" ry=\"10\"/>\n",
              "  \n",
              "  <!-- Title -->\n",
              "  <text x=\"400\" y=\"40\" font-family=\"Arial\" font-size=\"24\" text-anchor=\"middle\" font-weight=\"bold\">Multi-Agent RAG System Architecture</text>\n",
              "  \n",
              "  <!-- User and Query -->\n",
              "  <rect x=\"350\" y=\"80\" width=\"100\" height=\"40\" rx=\"5\" ry=\"5\" fill=\"#6495ED\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"400\" y=\"105\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">User Query</text>\n",
              "  \n",
              "  <!-- Query Router -->\n",
              "  <rect x=\"325\" y=\"160\" width=\"150\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#FF7F50\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"400\" y=\"190\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Query Router</text>\n",
              "  \n",
              "  <!-- Connector from User Query to Router -->\n",
              "  <line x1=\"400\" y1=\"120\" x2=\"400\" y2=\"160\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"400,160 396,150 404,150\" fill=\"#000\"/>\n",
              "  \n",
              "  <!-- Agent boxes -->\n",
              "  <rect x=\"100\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"160\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Math Agent</text>\n",
              "  \n",
              "  <rect x=\"260\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"320\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Science Agent</text>\n",
              "  \n",
              "  <rect x=\"420\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"480\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">History Agent</text>\n",
              "  \n",
              "  <rect x=\"580\" y=\"260\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#20B2AA\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"640\" y=\"290\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">General Agent</text>\n",
              "  \n",
              "  <!-- Vector Store boxes -->\n",
              "  <rect x=\"100\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"160\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">Math Vector DB</text>\n",
              "  \n",
              "  <rect x=\"260\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"320\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">Science Vector DB</text>\n",
              "  \n",
              "  <rect x=\"420\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"480\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">History Vector DB</text>\n",
              "  \n",
              "  <rect x=\"580\" y=\"360\" width=\"120\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#9370DB\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"640\" y=\"390\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">General Vector DB</text>\n",
              "  \n",
              "  <!-- LLM Processing -->\n",
              "  <rect x=\"325\" y=\"460\" width=\"150\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#FF6347\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"400\" y=\"490\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">LLM Processing</text>\n",
              "  \n",
              "  <!-- Response Synthesis -->\n",
              "  <rect x=\"325\" y=\"540\" width=\"150\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#4682B4\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"400\" y=\"570\" font-family=\"Arial\" font-size=\"16\" text-anchor=\"middle\" fill=\"white\">Response Synthesis</text>\n",
              "  \n",
              "  <!-- Connectors from Router to Agents -->\n",
              "  <line x1=\"350\" y1=\"210\" x2=\"160\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"160,260 168,252 170,260\" fill=\"#000\"/>\n",
              "  <text x=\"240\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Math Query</text>\n",
              "  \n",
              "  <line x1=\"375\" y1=\"210\" x2=\"320\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"320,260 325,250 330,255\" fill=\"#000\"/>\n",
              "  <text x=\"340\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Science Query</text>\n",
              "  \n",
              "  <line x1=\"425\" y1=\"210\" x2=\"480\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"480,260 475,250 470,255\" fill=\"#000\"/>\n",
              "  <text x=\"460\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">History Query</text>\n",
              "  \n",
              "  <line x1=\"450\" y1=\"210\" x2=\"640\" y2=\"260\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"640,260 632,252 630,260\" fill=\"#000\"/>\n",
              "  <text x=\"560\" y=\"230\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">General Query</text>\n",
              "  \n",
              "  <!-- Connectors from Agents to Vector Stores -->\n",
              "  <line x1=\"160\" y1=\"310\" x2=\"160\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"160,360 156,350 164,350\" fill=\"#000\"/>\n",
              "  \n",
              "  <line x1=\"320\" y1=\"310\" x2=\"320\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"320,360 316,350 324,350\" fill=\"#000\"/>\n",
              "  \n",
              "  <line x1=\"480\" y1=\"310\" x2=\"480\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"480,360 476,350 484,350\" fill=\"#000\"/>\n",
              "  \n",
              "  <line x1=\"640\" y1=\"310\" x2=\"640\" y2=\"360\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"640,360 636,350 644,350\" fill=\"#000\"/>\n",
              "  \n",
              "  <!-- Connectors from Vector Stores to LLM -->\n",
              "  <polyline points=\"160,410 160,485 325,485\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
              "  <polygon points=\"325,485 315,481 315,489\" fill=\"#000\"/>\n",
              "  \n",
              "  <polyline points=\"320,410 320,470 325,470\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
              "  <polygon points=\"325,470 315,466 315,474\" fill=\"#000\"/>\n",
              "  \n",
              "  <polyline points=\"480,410 480,470 475,470\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
              "  <polygon points=\"475,470 485,466 485,474\" fill=\"#000\"/>\n",
              "  \n",
              "  <polyline points=\"640,410 640,485 475,485\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\"/>\n",
              "  <polygon points=\"475,485 485,481 485,489\" fill=\"#000\"/>\n",
              "  \n",
              "  <!-- Connector from LLM to Response -->\n",
              "  <line x1=\"400\" y1=\"510\" x2=\"400\" y2=\"540\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <polygon points=\"400,540 396,530 404,530\" fill=\"#000\"/>\n",
              "  \n",
              "  <!-- File Ingestion Process -->\n",
              "  <rect x=\"80\" y=\"120\" width=\"160\" height=\"40\" rx=\"5\" ry=\"5\" fill=\"#8A2BE2\" stroke=\"#000\" stroke-width=\"2\"/>\n",
              "  <text x=\"160\" y=\"145\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\" fill=\"white\">Document Ingestion</text>\n",
              "  \n",
              "  <!-- Connector showing document flow -->\n",
              "  <polyline points=\"160,160 160,210 325,210\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\" stroke-dasharray=\"5,5\"/>\n",
              "  <polygon points=\"325,210 315,206 315,214\" fill=\"#000\"/>\n",
              "  <text x=\"240\" y=\"190\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Categorize & Store</text>\n",
              "  \n",
              "  <!-- Output to User -->\n",
              "  <polyline points=\"400,590 400,610 650,610 650,100 450,100\" stroke=\"#000\" stroke-width=\"2\" fill=\"none\" stroke-dasharray=\"5,5\"/>\n",
              "  <polygon points=\"450,100 460,96 460,104\" fill=\"#000\"/>\n",
              "  <text x=\"550\" y=\"605\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Response to User</text>\n",
              "</svg>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import tempfile\n",
        "from typing import List, Dict, Any, Optional, Tuple, BinaryIO, Callable\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.vectorstores import FAISS, Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PDFMinerLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "\n",
        "class RAGConfig:\n",
        "    def __init__(self):\n",
        "        # Local model configuration - using publicly available models\n",
        "        self.embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"  # Public embedding model\n",
        "        self.llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Public LLM model\n",
        "\n",
        "        # Model parameters\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.torch_dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "\n",
        "        # Vector database configuration\n",
        "        self.vector_db_path = \"chroma_db\"  # Changed from vector_db to chroma_db\n",
        "\n",
        "        # Document processing configuration\n",
        "        self.chunk_size = 500\n",
        "        self.chunk_overlap = 100\n",
        "\n",
        "        # Search configuration\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.5\n",
        "\n",
        "        # Agent configuration\n",
        "        self.max_iterations = 3  # Reduced for faster execution\n",
        "        self.thinking_steps = True\n",
        "\n",
        "        # Multi-agent configuration\n",
        "        self.agent_types = [\"math\", \"science\", \"history\", \"general\"]\n",
        "        self.vector_db_paths = {\n",
        "            \"math\": \"chroma_db_math\",\n",
        "            \"science\": \"chroma_db_science\",\n",
        "            \"history\": \"chroma_db_history\",\n",
        "            \"general\": \"chroma_db_general\"\n",
        "        }\n",
        "\n",
        "\n",
        "# Embedding class for document and query encoding - implement Embeddings interface\n",
        "class SentenceTransformerEmbedding(Embeddings):\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading embedding model {config.embedding_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.embedding_model_name)\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            config.embedding_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(\"Embedding model loaded successfully\")\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
        "        return self._get_embeddings(texts)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for a query string.\"\"\"\n",
        "        embeddings = self._get_embeddings([text])\n",
        "        return embeddings[0]\n",
        "\n",
        "    def _get_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Internal method to generate embeddings for a list of texts.\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        for text in texts:\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # Use the mean of the last hidden state as the embedding\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().tolist()\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# Document processor for loading and processing uploaded files\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap\n",
        "        )\n",
        "        self.temp_dir = None\n",
        "\n",
        "    def process_uploaded_file(self, file_obj: BinaryIO, filename: str) -> List[Document]:\n",
        "        \"\"\"Process a single uploaded file.\"\"\"\n",
        "        # Create a temporary directory if not already created\n",
        "        if self.temp_dir is None:\n",
        "            self.temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "        # Get file extension\n",
        "        _, file_extension = os.path.splitext(filename)\n",
        "        file_extension = file_extension.lower()\n",
        "\n",
        "        # Save the file to the temporary directory\n",
        "        temp_file_path = os.path.join(self.temp_dir, filename)\n",
        "        with open(temp_file_path, 'wb') as f:\n",
        "            f.write(file_obj.read())\n",
        "\n",
        "        # Select appropriate loader based on file extension\n",
        "        loader = None\n",
        "        if file_extension == '.txt':\n",
        "            loader = TextLoader(temp_file_path)\n",
        "        elif file_extension == '.pdf':\n",
        "            loader = PDFMinerLoader(temp_file_path)\n",
        "        elif file_extension == '.md':\n",
        "            loader = UnstructuredMarkdownLoader(temp_file_path)\n",
        "        elif file_extension == '.csv':\n",
        "            loader = CSVLoader(temp_file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "        # Load and process the document\n",
        "        documents = loader.load()\n",
        "        chunks = self.process_documents(documents)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks for embedding.\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for doc in documents:\n",
        "            try:\n",
        "                doc_chunks = self.text_splitter.split_documents([doc])\n",
        "                chunks.extend(doc_chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {doc.metadata.get('source', 'unknown')}: {e}\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Remove temporary files when done.\"\"\"\n",
        "        if self.temp_dir and os.path.exists(self.temp_dir):\n",
        "            import shutil\n",
        "            shutil.rmtree(self.temp_dir)\n",
        "            self.temp_dir = None\n",
        "\n",
        "\n",
        "# Vector store for document storage and retrieval\n",
        "class VectorStore:\n",
        "    def __init__(self, config: RAGConfig, embedding_model: SentenceTransformerEmbedding, agent_type: str = \"general\"):\n",
        "        self.config = config\n",
        "        self.embedding_model = embedding_model\n",
        "        self.vector_store = None\n",
        "        self.agent_type = agent_type\n",
        "\n",
        "        # Define a persistent directory for ChromaDB based on agent type\n",
        "        self.persist_directory = config.vector_db_paths.get(agent_type, config.vector_db_path)\n",
        "\n",
        "    def create_vector_store(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Create a vector store from documents.\"\"\"\n",
        "        # Use Chroma.from_documents method\n",
        "        self.vector_store = Chroma.from_documents(\n",
        "            documents,\n",
        "            self.embedding_model,\n",
        "            persist_directory=self.persist_directory\n",
        "        )\n",
        "\n",
        "        # Persist the data\n",
        "        self.vector_store.persist()\n",
        "\n",
        "    def add_documents(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Add documents to an existing vector store.\"\"\"\n",
        "        if self.vector_store is None:\n",
        "            # If no vector store exists, create a new one\n",
        "            self.create_vector_store(documents)\n",
        "        else:\n",
        "            # Add documents to existing vector store\n",
        "            self.vector_store.add_documents(documents)\n",
        "            # Persist the updated vector store\n",
        "            self.vector_store.persist()\n",
        "\n",
        "    def load_vector_store(self) -> bool:\n",
        "        \"\"\"Load the vector store if it exists.\"\"\"\n",
        "        # Check if the persist directory exists\n",
        "        if os.path.exists(self.persist_directory):\n",
        "            try:\n",
        "                self.vector_store = Chroma(\n",
        "                    persist_directory=self.persist_directory,\n",
        "                    embedding_function=self.embedding_model\n",
        "                )\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading vector store: {e}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "    def similarity_search(self, query: str) -> List[Document]:\n",
        "        \"\"\"Search for similar documents to the query.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not initialized. Please create or load a vector store first.\")\n",
        "\n",
        "        # Use similarity_search_with_score method\n",
        "        results = self.vector_store.similarity_search_with_score(\n",
        "            query,\n",
        "            k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        # Filter results by similarity threshold\n",
        "        # Note: ChromaDB returns distance (lower is better), similar to FAISS\n",
        "        filtered_results = [\n",
        "            doc for doc, score in results\n",
        "            if 1.0 / (1.0 + score) >= self.config.similarity_threshold  # Convert distance to similarity\n",
        "        ]\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "\n",
        "# Local LLM for RAG\n",
        "class LocalLLM:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading LLM model {config.llm_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.llm_model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            config.llm_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(f\"LLM model loaded successfully on {config.device}\")\n",
        "\n",
        "    def generate(self, prompt: str, system_message: str = None) -> str:\n",
        "        \"\"\"Generate text using local model.\"\"\"\n",
        "        # Format the messages\n",
        "        if system_message:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        else:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        try:\n",
        "            # Format messages for chat format\n",
        "            formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        except Exception as e:\n",
        "            # Fallback if the model doesn't support chat templates\n",
        "            print(f\"Chat template error: {e}, using simple prompt formatting\")\n",
        "            formatted_prompt = system_message + \"\\n\\n\" + prompt if system_message else prompt\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.config.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=300,  # Adjust based on use case\n",
        "                temperature=0.5,  # Lower temperature for faster convergence\n",
        "                do_sample=False,  # Use deterministic generation\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        # Decode the output, removing the input tokens\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        response_tokens = outputs[0][input_length:]\n",
        "        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "\n",
        "\n",
        "# Router class to direct queries to the appropriate agent\n",
        "class QueryRouter:\n",
        "    def __init__(self, config: RAGConfig, llm: LocalLLM):\n",
        "        self.config = config\n",
        "        self.llm = llm\n",
        "        self.agent_types = config.agent_types\n",
        "\n",
        "    def route_query(self, query: str) -> str:\n",
        "        \"\"\"Route the query to the appropriate agent type.\"\"\"\n",
        "        system_message = \"\"\"You are a query classifier. Your job is to determine which specialized agent\n",
        "        should handle a given query. Choose exactly one of the following categories:\n",
        "        - math: for mathematical questions, calculations, equations, etc.\n",
        "        - science: for questions about physics, chemistry, biology, etc.\n",
        "        - history: for questions about historical events, figures, periods, etc.\n",
        "        - general: for general knowledge, common sense, or any query that doesn't clearly fit the other categories.\n",
        "\n",
        "        Respond ONLY with the category name, nothing else.\"\"\"\n",
        "\n",
        "        prompt = f\"Query: {query}\\n\\nPlease classify this query into exactly one of these categories: math, science, history, or general.\"\n",
        "\n",
        "        response = self.llm.generate(prompt, system_message).strip().lower()\n",
        "\n",
        "        # Extract the category from the response (in case the model adds extra text)\n",
        "        for agent_type in self.agent_types:\n",
        "            if agent_type in response:\n",
        "                return agent_type\n",
        "\n",
        "        # Default to general if no clear category was detected\n",
        "        print(f\"No clear category detected in response: '{response}'. Defaulting to 'general'.\")\n",
        "        return \"general\"\n",
        "\n",
        "\n",
        "# Specialized agent for a specific domain\n",
        "class SpecializedAgent:\n",
        "    def __init__(self, config: RAGConfig, agent_type: str, embedding_model: SentenceTransformerEmbedding, llm: LocalLLM):\n",
        "        self.config = config\n",
        "        self.agent_type = agent_type\n",
        "        self.embedding_model = embedding_model\n",
        "        self.document_processor = DocumentProcessor(config)\n",
        "        self.vector_store = VectorStore(config, embedding_model, agent_type)\n",
        "        self.llm = llm\n",
        "\n",
        "        # Specialized system prompts for different agent types\n",
        "        self.system_prompts = {\n",
        "            \"math\": \"\"\"You are a mathematics expert. Provide clear, step-by-step solutions to mathematical problems.\n",
        "            When analyzing equations or working with numbers, carefully break down each step of the calculation.\n",
        "            Explain mathematical concepts in an intuitive way with relevant examples.\"\"\",\n",
        "\n",
        "            \"science\": \"\"\"You are a science expert. Explain scientific concepts with precision and accuracy.\n",
        "            Relate scientific principles to real-world applications when possible.\n",
        "            Use appropriate terminology while making complex ideas accessible.\"\"\",\n",
        "\n",
        "            \"history\": \"\"\"You are a history expert. Provide nuanced historical context and accurate chronology.\n",
        "            Consider multiple perspectives when discussing historical events and figures.\n",
        "            Connect historical facts to broader themes and patterns where relevant.\"\"\",\n",
        "\n",
        "            \"general\": \"\"\"You are a knowledgeable assistant with access to a knowledge base.\n",
        "            Your task is to provide accurate, relevant information based on the query and the retrieved context.\n",
        "            Think step by step and analyze the retrieved information carefully before formulating your final response.\"\"\"\n",
        "        }\n",
        "\n",
        "        # Load the vector store if it exists\n",
        "        self.vector_store.load_vector_store()\n",
        "\n",
        "    def ingest_document(self, file_obj: BinaryIO, filename: str) -> None:\n",
        "        \"\"\"Ingest a document into this agent's knowledge base.\"\"\"\n",
        "        print(f\"Processing uploaded file for {self.agent_type} agent: {filename}...\")\n",
        "\n",
        "        try:\n",
        "            # Process the uploaded file\n",
        "            chunks = self.document_processor.process_uploaded_file(file_obj, filename)\n",
        "            print(f\"Created {len(chunks)} chunks from {filename}.\")\n",
        "\n",
        "            # Enrich metadata with agent type\n",
        "            for chunk in chunks:\n",
        "                chunk.metadata[\"agent_type\"] = self.agent_type\n",
        "\n",
        "            # Try to load the vector store first\n",
        "            vector_store_exists = self.vector_store.load_vector_store()\n",
        "\n",
        "            if vector_store_exists:\n",
        "                # Add the new documents to the existing vector store\n",
        "                print(f\"Adding documents to existing {self.agent_type} vector store...\")\n",
        "                self.vector_store.add_documents(chunks)\n",
        "            else:\n",
        "                # Create a new vector store if none exists\n",
        "                print(f\"Creating new {self.agent_type} vector store...\")\n",
        "                self.vector_store.create_vector_store(chunks)\n",
        "\n",
        "            print(f\"Successfully ingested {filename} for {self.agent_type} agent.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error ingesting file {filename} for {self.agent_type} agent: {e}\")\n",
        "            raise\n",
        "\n",
        "    def query(self, user_query: str) -> str:\n",
        "        \"\"\"Process a query using this specialized agent.\"\"\"\n",
        "        # Get the appropriate system message for this agent type\n",
        "        system_message = self.system_prompts.get(self.agent_type, self.system_prompts[\"general\"])\n",
        "\n",
        "        # Retrieve documents\n",
        "        try:\n",
        "            results = self.vector_store.vector_store.similarity_search_with_score(\n",
        "                user_query,\n",
        "                k=self.config.top_k\n",
        "            )\n",
        "\n",
        "            # Display retrieved documents with scores\n",
        "            print(f\"\\nRetrieved Documents for {self.agent_type} agent:\")\n",
        "            if results:\n",
        "                for i, (doc, score) in enumerate(results):\n",
        "                    print(f\"\\nDocument {i+1} (Score: {score}):\")\n",
        "                    print(doc.page_content)\n",
        "                    if hasattr(doc, 'metadata') and doc.metadata:\n",
        "                        print(f\"Metadata: {doc.metadata}\")\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "                # Filter for actual processing\n",
        "                retrieved_docs = [doc for doc, score in results\n",
        "                                 if 1.0 / (1.0 + score) >= self.config.similarity_threshold]\n",
        "            else:\n",
        "                print(f\"No relevant documents found for {self.agent_type} agent.\")\n",
        "                retrieved_docs = []\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error retrieving documents for {self.agent_type} agent: {e}\")\n",
        "            retrieved_docs = []\n",
        "\n",
        "        # Agentic thinking process with specialized knowledge\n",
        "        response = self._agentic_process(user_query, retrieved_docs, system_message)\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _agentic_process(self, user_query: str, retrieved_docs: List[Document], system_message: str) -> str:\n",
        "        \"\"\"Execute the agentic process for responding to queries with domain-specific expertise.\"\"\"\n",
        "        # Initial retrieval if not provided\n",
        "        if not retrieved_docs:\n",
        "            # Handle the case when no relevant documents are found\n",
        "            prompt = f\"\"\"Query: {user_query}\n",
        "\n",
        "            No relevant documents were found in the {self.agent_type} knowledge base. Please provide a general response based on your {self.agent_type} expertise.\n",
        "            \"\"\"\n",
        "            return self.llm.generate(prompt, system_message)\n",
        "\n",
        "        # For agentic reasoning, we'll use a multi-step process\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "        iteration_responses = []\n",
        "        current_query = user_query\n",
        "\n",
        "        for iteration in range(self.config.max_iterations):\n",
        "            print(f\"Iteration {iteration+1}/{self.config.max_iterations} for {self.agent_type} agent\")\n",
        "\n",
        "            # Check if we should continue\n",
        "            if iteration > 0 and not self._should_continue(current_query, iteration_responses[-1]):\n",
        "                break\n",
        "\n",
        "            # Generate thinking steps if enabled\n",
        "            thinking = \"\"\n",
        "            if self.config.thinking_steps:\n",
        "                thinking_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Think step by step about this query from a {self.agent_type} perspective. What are the key points to address?\n",
        "                What information from the context is most relevant? What additional {self.agent_type} knowledge might be needed?\n",
        "                \"\"\"\n",
        "                thinking = self.llm.generate(thinking_prompt, system_message)\n",
        "\n",
        "            # Generate the response\n",
        "            response_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            {thinking if thinking else \"\"}\n",
        "\n",
        "            Based on the context provided and your expertise in {self.agent_type}, please answer the query.\n",
        "            If the context doesn't contain enough information, acknowledge this and provide the best answer you can using your {self.agent_type} knowledge.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.llm.generate(response_prompt, system_message)\n",
        "            iteration_responses.append(response)\n",
        "\n",
        "            # Generate follow-up questions or refinements\n",
        "            refinement_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Your current response:\n",
        "            {response}\n",
        "\n",
        "            Are there aspects of the query that haven't been fully addressed from a {self.agent_type} perspective?\n",
        "            What follow-up questions would help provide a more complete answer? How could the search be refined?\n",
        "            \"\"\"\n",
        "\n",
        "            refinement = self.llm.generate(refinement_prompt, system_message)\n",
        "\n",
        "            # Extract a new query for the next iteration\n",
        "            new_query_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "            Current response:\n",
        "            {response}\n",
        "\n",
        "            Refinement thoughts:\n",
        "            {refinement}\n",
        "\n",
        "            Based on the above, formulate a new search query that would help address any gaps in the current response from a {self.agent_type} perspective.\n",
        "            Return ONLY the new query without any explanation.\n",
        "            If you believe the query has been fully addressed, return \"COMPLETE\".\n",
        "            \"\"\"\n",
        "\n",
        "            new_query = self.llm.generate(new_query_prompt, system_message).strip()\n",
        "\n",
        "            if new_query == \"COMPLETE\" or new_query.upper().startswith(\"COMPLETE\"):\n",
        "                break\n",
        "\n",
        "            # Perform a new search with the refined query\n",
        "            current_query = new_query\n",
        "            # Inside the iteration loop in _agentic_process\n",
        "            new_docs = self.vector_store.similarity_search(current_query)\n",
        "\n",
        "            if new_docs:\n",
        "                print(f\"\\nAdditional Documents for Iteration {iteration+1} ({self.agent_type}):\")\n",
        "                for i, doc in enumerate(new_docs):\n",
        "                    print(f\"\\nDocument {i+1}:\")\n",
        "                    print(doc.page_content)\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "                new_context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(new_docs)])\n",
        "                # Update context with new information\n",
        "                context = f\"{context}\\n\\nAdditional Context:\\n{new_context}\"\n",
        "\n",
        "        # Final synthesis\n",
        "        final_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "        Iterations of responses from {self.agent_type} agent:\n",
        "        {' '.join([f\"Iteration {i+1}: {resp}\" for i, resp in enumerate(iteration_responses)])}\n",
        "\n",
        "        Please provide a final, comprehensive response to the original query that synthesizes all the information gathered across iterations.\n",
        "        Apply your {self.agent_type} expertise to ensure the answer is accurate, complete, and well-explained.\n",
        "        \"\"\"\n",
        "\n",
        "        final_response = self.llm.generate(final_prompt, system_message)\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    def _should_continue(self, query: str, last_response: str) -> bool:\n",
        "        \"\"\"Determine if the agent should continue iterating.\"\"\"\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Current response from {self.agent_type} agent:\n",
        "        {last_response}\n",
        "\n",
        "        Does this response fully address the query from a {self.agent_type} perspective?\n",
        "        If yes, respond with \"COMPLETE\". If not, respond with \"CONTINUE\" and briefly explain why.\n",
        "        \"\"\"\n",
        "\n",
        "        decision = self.llm.generate(prompt)\n",
        "        return \"CONTINUE\" in decision.upper()\n",
        "\n",
        "\n",
        "# Multi-agent RAG coordinator\n",
        "class MultiAgentRAG:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "\n",
        "        # Initialize shared components\n",
        "        self.embedding_model = SentenceTransformerEmbedding(config)\n",
        "        self.llm = LocalLLM(config)\n",
        "        self.document_processor = DocumentProcessor(config)\n",
        "\n",
        "        # Initialize the router\n",
        "        self.router = QueryRouter(config, self.llm)\n",
        "\n",
        "        # Initialize specialized agents\n",
        "        self.agents = {}\n",
        "        for agent_type in config.agent_types:\n",
        "            print(f\"Initializing {agent_type} agent...\")\n",
        "            self.agents[agent_type] = SpecializedAgent(config, agent_type, self.embedding_model, self.llm)\n",
        "\n",
        "    def ingest_uploaded_file(self, file_obj: BinaryIO, filename: str, agent_type: str = None) -> None:\n",
        "        \"\"\"Ingest a file into one or all agents' knowledge bases.\"\"\"\n",
        "        # Save a copy of the original file content for multiple uses\n",
        "        file_content = file_obj.read()\n",
        "\n",
        "        if agent_type and agent_type in self.agents:\n",
        "            # Ingest into a specific agent's knowledge base\n",
        "            file_copy = BinaryIO(BytesIOWrapper(file_content))\n",
        "            self.agents[agent_type].ingest_document(file_copy, filename)\n",
        "        else:\n",
        "            # Determine the agent type from the filename or content\n",
        "            determined_agent_type = self._determine_document_type(filename, file_content)\n",
        "            print(f\"Auto-determined document type: {determined_agent_type}\")\n",
        "\n",
        "            # Create a copy of the file content and ingest it\n",
        "            file_copy = BytesIOWrapper(file_content)\n",
        "            self.agents[determined_agent_type].ingest_document(file_copy, filename)\n",
        "\n",
        "    def _determine_document_type(self, filename: str, file_content: bytes) -> str:\n",
        "        \"\"\"Determine the document type to route it to the appropriate agent.\"\"\"\n",
        "        # Basic method: use keywords in filename or first few lines\n",
        "        filename_lower = filename.lower()\n",
        "        content_sample = file_content[:5000].decode('utf-8', errors='ignore').lower()\n",
        "\n",
        "        # Simple keyword matching - can be replaced with more sophisticated analysis\n",
        "        if any(math_term in filename_lower or math_term in content_sample\n",
        "               for math_term in [\"math\", \"calculus\", \"algebra\", \"equation\", \"theorem\", \"number\"]):\n",
        "            return \"math\"\n",
        "        elif any(science_term in filename_lower or science_term in content_sample\n",
        "                for science_term in [\"science\", \"physics\", \"chemistry\", \"biology\", \"experiment\"]):\n",
        "            return \"science\"\n",
        "        elif any(history_term in filename_lower or history_term in content_sample\n",
        "                for history_term in [\"history\", \"ancient\", \"century\", \"timeline\", \"war\", \"civilization\"]):\n",
        "            return \"history\"\n",
        "        else:\n",
        "            return \"general\"\n",
        "\n",
        "    def query(self, user_query: str) -> str:\n",
        "        \"\"\"Process a query using the router and appropriate agent(s).\"\"\"\n",
        "        # First, route the query to determine which agent should handle it\n",
        "        start_time = time.time()\n",
        "        print(f\"Routing query: {user_query}\")\n",
        "        agent_type = self.router.route_query(user_query)\n",
        "        print(f\"Query routed to {agent_type} agent\")\n",
        "\n",
        "        # Get the response from the appropriate agent\n",
        "        response = self.agents[agent_type].query(user_query)\n",
        "\n",
        "        # Check if we need responses from other agents as well\n",
        "        if self._needs_multiple_agents(user_query, response):\n",
        "            print(\"Query might benefit from multiple agents' perspectives\")\n",
        "            all_responses = {}\n",
        "\n",
        "            # Collect responses from all agents or a subset of relevant ones\n",
        "            for other_type, agent in self.agents.items():\n",
        "                if other_type != agent_type:\n",
        "                    print(f\"Getting additional perspective from {other_type} agent\")\n",
        "                    all_responses[other_type] = agent.query(user_query)\n",
        "\n",
        "            # Synthesize the responses\n",
        "            response = self._synthesize_responses(user_query, agent_type, response, all_responses)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Total query processing time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _needs_multiple_agents(self, query: str, primary_response: str) -> bool:\n",
        "        \"\"\"Determine if a query needs insights from multiple agents.\"\"\"\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Primary response: {primary_response}\n",
        "\n",
        "        Analyze whether this query requires perspectives from multiple domains of knowledge.\n",
        "        Does it touch on multiple subjects (e.g., both math and science, or history and general knowledge)?\n",
        "        Would the answer benefit significantly from additional domain expertise?\n",
        "\n",
        "        Respond with \"YES\" if multiple agents should provide input, or \"NO\" if the primary response is sufficient.\n",
        "        \"\"\"\n",
        "\n",
        "        system_message = \"\"\"You are a query analyzer determining whether a question spans multiple domains of knowledge.\n",
        "        Be conservative - only recommend multiple agents if there's significant cross-domain value.\"\"\"\n",
        "\n",
        "        decision = self.llm.generate(prompt, system_message).strip().upper()\n",
        "        return \"YES\" in decision\n",
        "\n",
        "    def _synthesize_responses(self, query: str, primary_agent: str, primary_response: str,\n",
        "                              other_responses: Dict[str, str]) -> str:\n",
        "        \"\"\"Synthesize responses from multiple agents into a coherent answer.\"\"\"\n",
        "        # Format all the responses\n",
        "        responses_text = f\"Primary response ({primary_agent}):\\n{primary_response}\\n\\n\"\n",
        "\n",
        "        for agent_type, response in other_responses.items():\n",
        "            responses_text += f\"{agent_type.capitalize()} perspective:\\n{response}\\n\\n\"\n",
        "\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Different expert perspectives:\n",
        "        {responses_text}\n",
        "\n",
        "        Synthesize these different expert perspectives into a comprehensive, coherent response.\n",
        "        Highlight where the different perspectives complement each other and provide a more complete answer.\n",
        "        Ensure the final response is well-structured and addresses all aspects of the original query.\n",
        "        \"\"\"\n",
        "\n",
        "        system_message = \"\"\"You are a synthesis expert combining insights from multiple domain experts.\n",
        "        Create a unified response that preserves the valuable contributions from each expert while removing redundancies.\n",
        "        Acknowledge the different domains of expertise when they provide unique perspectives.\"\"\"\n",
        "\n",
        "        synthesized_response = self.llm.generate(prompt, system_message)\n",
        "\n",
        "        return synthesized_response\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up temporary files in all agents.\"\"\"\n",
        "        self.document_processor.cleanup()\n",
        "        for agent in self.agents.values():\n",
        "            agent.document_processor.cleanup()\n",
        "\n",
        "\n",
        "# Utility class for creating BytesIO objects from bytes\n",
        "class BytesIOWrapper(BinaryIO):\n",
        "    def __init__(self, data: bytes):\n",
        "        self.data = data\n",
        "        self.position = 0\n",
        "\n",
        "    def read(self, size: int = -1) -> bytes:\n",
        "        if size == -1:\n",
        "            result = self.data[self.position:]\n",
        "            self.position = len(self.data)\n",
        "            return result\n",
        "        else:\n",
        "            result = self.data[self.position:self.position + size]\n",
        "            self.position += size\n",
        "            return result\n",
        "\n",
        "    def seek(self, position: int) -> int:\n",
        "        self.position = position\n",
        "        return self.position\n",
        "\n",
        "    def tell(self) -> int:\n",
        "        return self.position\n",
        "\n",
        "    def close(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.close()\n",
        "\n",
        "\n",
        "def run_in_colab():\n",
        "    from google.colab import files\n",
        "    import io\n",
        "    import time\n",
        "\n",
        "    config = RAGConfig()\n",
        "    print(f\"Initializing MultiAgentRAG with models on {config.device}\")\n",
        "    rag_system = MultiAgentRAG(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nMultiAgentRAG with Specialized Experts\")\n",
        "        print(\"1. Upload and ingest a file\")\n",
        "        print(\"2. Upload and ingest a file for a specific agent\")\n",
        "        print(\"3. Ask a question\")\n",
        "        print(\"4. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-4): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            try:\n",
        "                print(\"Please select a file to upload...\")\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                for filename, content in uploaded.items():\n",
        "                    file_obj = io.BytesIO(content)\n",
        "                    rag_system.ingest_uploaded_file(file_obj, filename)\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading and ingesting file: {e}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            try:\n",
        "                print(\"Available agents: \" + \", \".join(config.agent_types))\n",
        "                agent_type = input(\"Enter the agent type for this document: \").lower()\n",
        "\n",
        "                if agent_type not in config.agent_types:\n",
        "                    print(f\"Invalid agent type. Please choose from: {', '.join(config.agent_types)}\")\n",
        "                    continue\n",
        "\n",
        "                print(\"Please select a file to upload...\")\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                for filename, content in uploaded.items():\n",
        "                    file_obj = io.BytesIO(content)\n",
        "                    rag_system.ingest_uploaded_file(file_obj, filename, agent_type)\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading and ingesting file: {e}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            query = input(\"Enter your question: \")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                response = rag_system.query(query)\n",
        "                end_time = time.time()\n",
        "\n",
        "                print(f\"\\nResponse: {response}\")\n",
        "                print(f\"Query processed in {end_time - start_time:.2f} seconds\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing query: {e}\")\n",
        "\n",
        "        elif choice == \"4\":\n",
        "            print(\"Exiting MultiAgentRAG system. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter a number between 1 and 4.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_in_colab()"
      ],
      "metadata": {
        "id": "2PE6E5EYds3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAMKeW2ZmGBS",
        "outputId": "40f4b3a7-2493-463f-8057-35f55a572c8a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.25-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.51)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.23)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.11.2)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.3.25-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.4/142.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.3.25 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 ormsgpack-1.9.1 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygraphviz -qq # Install the python binding\n",
        "!apt-get update -qq && apt-get install -y graphviz graphviz-dev -qq # Install the system library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDawMEH_55D2",
        "outputId": "82545773-a053-4fbf-a2b3-1ccfbe7c1b0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pygraphviz (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import tempfile\n",
        "import uuid\n",
        "from typing import List, Dict, Any, Optional, Tuple, BinaryIO, Callable, Union, Annotated\n",
        "# from typing import Literal  # Import Literal from the typing module\n",
        "from typing_extensions import Literal\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.vectorstores import FAISS, Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PDFMinerLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from typing import TypedDict # Import TypedDict from the typing module\n",
        "# Enhanced LangGraph imports and configuration\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "import langgraph.checkpoint as checkpoint\n",
        "from langgraph.graph.message import MessagesState\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt.tool_node import ToolNode # Remove the import of Tool# This should be the correct import statement\n",
        "#from langgraph.graph.agentbox import AgentBox  # Original import with potential error\n",
        "#from langgraph.graph.agentbox import AgentBox  # Original import with potential error\n",
        "# ... other imports ...\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "import langgraph.checkpoint as checkpoint\n",
        "# ... other code ...\n",
        "import asyncio\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Utility class for file-like objects from bytes\n",
        "class BytesIOWrapper:\n",
        "    def __init__(self, content: bytes):\n",
        "        self.content = content\n",
        "        self.position = 0\n",
        "\n",
        "    def read(self, size=None):\n",
        "        if size is None:\n",
        "            result = self.content[self.position:]\n",
        "            self.position = len(self.content)\n",
        "            return result\n",
        "        else:\n",
        "            result = self.content[self.position:self.position + size]\n",
        "            self.position += size\n",
        "            return result\n",
        "\n",
        "    def seek(self, position, whence=0):\n",
        "        if whence == 0:\n",
        "            self.position = position\n",
        "        elif whence == 1:\n",
        "            self.position += position\n",
        "        elif whence == 2:\n",
        "            self.position = len(self.content) + position\n",
        "\n",
        "\n",
        "class RAGConfig:\n",
        "    def __init__(self):\n",
        "        # Local model configuration - using publicly available models\n",
        "        self.embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"  # Public embedding model\n",
        "        self.llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Public LLM model\n",
        "\n",
        "        # Model parameters\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.torch_dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "\n",
        "        # Vector database configuration\n",
        "        self.vector_db_path = \"chroma_db\"  # Changed from vector_db to chroma_db\n",
        "\n",
        "        # Document processing configuration\n",
        "        self.chunk_size = 500\n",
        "        self.chunk_overlap = 100\n",
        "\n",
        "        # Search configuration\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.5\n",
        "\n",
        "        # Agent configuration\n",
        "        self.max_iterations = 3  # Reduced for faster execution\n",
        "        self.thinking_steps = True\n",
        "\n",
        "        # Multi-agent configuration\n",
        "        self.agent_types = [\"math\", \"science\", \"history\", \"general\"]\n",
        "        self.vector_db_paths = {\n",
        "            \"math\": \"chroma_db_math\",\n",
        "            \"science\": \"chroma_db_science\",\n",
        "            \"history\": \"chroma_db_history\",\n",
        "            \"general\": \"chroma_db_general\"\n",
        "        }\n",
        "\n",
        "\n",
        "# Embedding class for document and query encoding - implement Embeddings interface\n",
        "class SentenceTransformerEmbedding(Embeddings):\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading embedding model {config.embedding_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.embedding_model_name)\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            config.embedding_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(\"Embedding model loaded successfully\")\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
        "        return self._get_embeddings(texts)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for a query string.\"\"\"\n",
        "        embeddings = self._get_embeddings([text])\n",
        "        return embeddings[0]\n",
        "\n",
        "    def _get_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Internal method to generate embeddings for a list of texts.\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        for text in texts:\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # Use the mean of the last hidden state as the embedding\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().tolist()\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# Document processor for loading and processing uploaded files\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap\n",
        "        )\n",
        "        self.temp_dir = None\n",
        "\n",
        "    def process_uploaded_file(self, file_obj: BinaryIO, filename: str) -> List[Document]:\n",
        "        \"\"\"Process a single uploaded file.\"\"\"\n",
        "        # Create a temporary directory if not already created\n",
        "        if self.temp_dir is None:\n",
        "            self.temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "        # Get file extension\n",
        "        _, file_extension = os.path.splitext(filename)\n",
        "        file_extension = file_extension.lower()\n",
        "\n",
        "        # Save the file to the temporary directory\n",
        "        temp_file_path = os.path.join(self.temp_dir, filename)\n",
        "        with open(temp_file_path, 'wb') as f:\n",
        "            f.write(file_obj.read())\n",
        "\n",
        "        # Select appropriate loader based on file extension\n",
        "        loader = None\n",
        "        if file_extension == '.txt':\n",
        "            loader = TextLoader(temp_file_path)\n",
        "        elif file_extension == '.pdf':\n",
        "            loader = PDFMinerLoader(temp_file_path)\n",
        "        elif file_extension == '.md':\n",
        "            loader = UnstructuredMarkdownLoader(temp_file_path)\n",
        "        elif file_extension == '.csv':\n",
        "            loader = CSVLoader(temp_file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "        # Load and process the document\n",
        "        documents = loader.load()\n",
        "        chunks = self.process_documents(documents)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks for embedding.\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for doc in documents:\n",
        "            try:\n",
        "                doc_chunks = self.text_splitter.split_documents([doc])\n",
        "                chunks.extend(doc_chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {doc.metadata.get('source', 'unknown')}: {e}\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Remove temporary files when done.\"\"\"\n",
        "        if self.temp_dir and os.path.exists(self.temp_dir):\n",
        "            import shutil\n",
        "            shutil.rmtree(self.temp_dir)\n",
        "            self.temp_dir = None\n",
        "\n",
        "\n",
        "# Vector store for document storage and retrieval\n",
        "class VectorStore:\n",
        "    def __init__(self, config: RAGConfig, embedding_model: SentenceTransformerEmbedding, agent_type: str = \"general\"):\n",
        "        self.config = config\n",
        "        self.embedding_model = embedding_model\n",
        "        self.vector_store = None\n",
        "        self.agent_type = agent_type\n",
        "\n",
        "        # Define a persistent directory for ChromaDB based on agent type\n",
        "        self.persist_directory = config.vector_db_paths.get(agent_type, config.vector_db_path)\n",
        "\n",
        "    def create_vector_store(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Create a vector store from documents.\"\"\"\n",
        "        # Use Chroma.from_documents method\n",
        "        self.vector_store = Chroma.from_documents(\n",
        "            documents,\n",
        "            self.embedding_model,\n",
        "            persist_directory=self.persist_directory\n",
        "        )\n",
        "\n",
        "        # Persist the data\n",
        "        self.vector_store.persist()\n",
        "\n",
        "    def add_documents(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Add documents to an existing vector store.\"\"\"\n",
        "        if self.vector_store is None:\n",
        "            # If no vector store exists, create a new one\n",
        "            self.create_vector_store(documents)\n",
        "        else:\n",
        "            # Add documents to existing vector store\n",
        "            self.vector_store.add_documents(documents)\n",
        "            # Persist the updated vector store\n",
        "            self.vector_store.persist()\n",
        "\n",
        "    def load_vector_store(self) -> bool:\n",
        "        \"\"\"Load the vector store if it exists.\"\"\"\n",
        "        # Check if the persist directory exists\n",
        "        if os.path.exists(self.persist_directory):\n",
        "            try:\n",
        "                self.vector_store = Chroma(\n",
        "                    persist_directory=self.persist_directory,\n",
        "                    embedding_function=self.embedding_model\n",
        "                )\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading vector store: {e}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "    def similarity_search(self, query: str) -> List[Document]:\n",
        "        \"\"\"Search for similar documents to the query.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not initialized. Please create or load a vector store first.\")\n",
        "\n",
        "        # Use similarity_search_with_score method\n",
        "        results = self.vector_store.similarity_search_with_score(\n",
        "            query,\n",
        "            k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        # Filter results by similarity threshold\n",
        "        # Note: ChromaDB returns distance (lower is better), similar to FAISS\n",
        "        filtered_results = [\n",
        "            doc for doc, score in results\n",
        "            if 1.0 / (1.0 + score) >= self.config.similarity_threshold  # Convert distance to similarity\n",
        "        ]\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "\n",
        "# Local LLM for RAG\n",
        "class LocalLLM:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading LLM model {config.llm_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.llm_model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            config.llm_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(f\"LLM model loaded successfully on {config.device}\")\n",
        "\n",
        "    def generate(self, prompt: str, system_message: str = None) -> str:\n",
        "        \"\"\"Generate text using local model.\"\"\"\n",
        "        # Format the messages\n",
        "        if system_message:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        else:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        try:\n",
        "            # Format messages for chat format\n",
        "            formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        except Exception as e:\n",
        "            # Fallback if the model doesn't support chat templates\n",
        "            print(f\"Chat template error: {e}, using simple prompt formatting\")\n",
        "            formatted_prompt = system_message + \"\\n\\n\" + prompt if system_message else prompt\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.config.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=300,  # Adjust based on use case\n",
        "                temperature=0.5,  # Lower temperature for faster convergence\n",
        "                do_sample=False,  # Use deterministic generation\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        # Decode the output, removing the input tokens\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        response_tokens = outputs[0][input_length:]\n",
        "        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "\n",
        "\n",
        "# Enhanced LangGraph state implementation\n",
        "class RAGState(TypedDict):\n",
        "    \"\"\"LangGraph state definition for RAG workflow.\"\"\"\n",
        "    query: str\n",
        "    agent_type: str\n",
        "    context: Optional[List[Document]]\n",
        "    response: Optional[str]\n",
        "    thread_id: str\n",
        "    history: List[Dict[str, Any]]\n",
        "    agent_responses: Dict[str, str]\n",
        "    final_response: Optional[str]\n",
        "    status: Literal[\"ROUTING\", \"RETRIEVING\", \"PROCESSING\", \"SYNTHESIZING\", \"COMPLETE\"]\n",
        "    messages: Optional[List[Dict[str, Any]]]  # Support for messages format\n",
        "    tools: Optional[List[Dict[str, Any]]]     # Support for tools\n",
        "    next_steps: Optional[List[str]]          # Track planned steps\n",
        "\n",
        "\n",
        "# LangGraph node implementations\n",
        "class RouterNode:\n",
        "    \"\"\"LangGraph node for routing queries to appropriate agents.\"\"\"\n",
        "\n",
        "    def __init__(self, config: RAGConfig, llm: LocalLLM):\n",
        "        self.config = config\n",
        "        self.llm = llm\n",
        "        self.agent_types = config.agent_types\n",
        "\n",
        "    def __call__(self, state: RAGState) -> RAGState:\n",
        "        \"\"\"Route the query to the appropriate agent type.\"\"\"\n",
        "        query = state[\"query\"]\n",
        "\n",
        "        system_message = \"\"\"You are a query classifier. Your job is to determine which specialized agent\n",
        "        should handle a given query. Choose exactly one of the following categories:\n",
        "        - math: for mathematical questions, calculations, equations, etc.\n",
        "        - science: for questions about physics, chemistry, biology, etc.\n",
        "        - history: for questions about historical events, figures, periods, etc.\n",
        "        - general: for general knowledge, common sense, or any query that doesn't clearly fit the other categories.\n",
        "\n",
        "        Respond ONLY with the category name, nothing else.\"\"\"\n",
        "\n",
        "        prompt = f\"Query: {query}\\n\\nPlease classify this query into exactly one of these categories: math, science, history, or general.\"\n",
        "\n",
        "        response = self.llm.generate(prompt, system_message).strip().lower()\n",
        "\n",
        "        # Extract the category from the response (in case the model adds extra text)\n",
        "        selected_agent_type = \"general\"  # Default\n",
        "        for agent_type in self.agent_types:\n",
        "            if agent_type in response:\n",
        "                selected_agent_type = agent_type\n",
        "                break\n",
        "\n",
        "        print(f\"Query '{query}' routed to {selected_agent_type} agent\")\n",
        "\n",
        "        # Update the state\n",
        "        state[\"agent_type\"] = selected_agent_type\n",
        "        state[\"status\"] = \"RETRIEVING\"\n",
        "        state[\"history\"].append({\n",
        "            \"step\": \"routing\",\n",
        "            \"agent_type\": selected_agent_type,\n",
        "            \"timestamp\": time.time()\n",
        "        })\n",
        "\n",
        "        # Update messages for LangGraph tracking\n",
        "        if \"messages\" not in state or state[\"messages\"] is None:\n",
        "            state[\"messages\"] = []\n",
        "\n",
        "        state[\"messages\"].append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"Query routed to {selected_agent_type} agent.\"\n",
        "        })\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "class RetrievalNode:\n",
        "    \"\"\"LangGraph node for retrieving documents from a vector store.\"\"\"\n",
        "\n",
        "    def __init__(self, agent_type: str, vector_store: VectorStore, config: RAGConfig):\n",
        "        self.agent_type = agent_type\n",
        "        self.vector_store = vector_store\n",
        "        self.config = config\n",
        "\n",
        "    def __call__(self, state: RAGState) -> RAGState:\n",
        "        \"\"\"Retrieve relevant documents for the query.\"\"\"\n",
        "        query = state[\"query\"]\n",
        "\n",
        "        try:\n",
        "            # Retrieve documents from the vector store\n",
        "            results = self.vector_store.vector_store.similarity_search_with_score(\n",
        "                query,\n",
        "                k=self.config.top_k\n",
        "            )\n",
        "\n",
        "            # Display retrieved documents with scores\n",
        "            print(f\"\\nRetrieved Documents for {self.agent_type} agent:\")\n",
        "            retrieved_docs = []\n",
        "\n",
        "            if results:\n",
        "                for i, (doc, score) in enumerate(results):\n",
        "                    print(f\"\\nDocument {i+1} (Score: {score}):\")\n",
        "                    print(doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content)\n",
        "                    if hasattr(doc, 'metadata') and doc.metadata:\n",
        "                        print(f\"Metadata: {doc.metadata}\")\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "                # Filter for actual processing\n",
        "                retrieved_docs = [doc for doc, score in results\n",
        "                                if 1.0 / (1.0 + score) >= self.config.similarity_threshold]\n",
        "            else:\n",
        "                print(f\"No relevant documents found for {self.agent_type} agent.\")\n",
        "\n",
        "            # Update the state\n",
        "            state[\"context\"] = retrieved_docs\n",
        "            state[\"status\"] = \"PROCESSING\"\n",
        "            state[\"history\"].append({\n",
        "                \"step\": \"retrieval\",\n",
        "                \"agent_type\": self.agent_type,\n",
        "                \"doc_count\": len(retrieved_docs),\n",
        "                \"timestamp\": time.time()\n",
        "            })\n",
        "\n",
        "            # Update messages for LangGraph tracking\n",
        "            if \"messages\" not in state or state[\"messages\"] is None:\n",
        "                state[\"messages\"] = []\n",
        "\n",
        "            state[\"messages\"].append({\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"Retrieved {len(retrieved_docs)} relevant documents.\"\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error retrieving documents for {self.agent_type} agent: {e}\")\n",
        "            state[\"context\"] = []\n",
        "            state[\"status\"] = \"PROCESSING\"\n",
        "            state[\"history\"].append({\n",
        "                \"step\": \"retrieval_error\",\n",
        "                \"agent_type\": self.agent_type,\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": time.time()\n",
        "            })\n",
        "\n",
        "            # Update messages for LangGraph tracking\n",
        "            if \"messages\" not in state or state[\"messages\"] is None:\n",
        "                state[\"messages\"] = []\n",
        "\n",
        "            state[\"messages\"].append({\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"Error retrieving documents: {str(e)}\"\n",
        "            })\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "class ProcessingNode:\n",
        "    \"\"\"LangGraph node for processing queries with retrieved context.\"\"\"\n",
        "\n",
        "    def __init__(self, agent_type: str, llm: LocalLLM, config: RAGConfig):\n",
        "        self.agent_type = agent_type\n",
        "        self.llm = llm\n",
        "        self.config = config\n",
        "\n",
        "        # Specialized system prompts for different agent types\n",
        "        self.system_prompts = {\n",
        "            \"math\": \"\"\"You are a mathematics expert. Provide clear, step-by-step solutions to mathematical problems.\n",
        "            When analyzing equations or working with numbers, carefully break down each step of the calculation.\n",
        "            Explain mathematical concepts in an intuitive way with relevant examples.\"\"\",\n",
        "\n",
        "            \"science\": \"\"\"You are a science expert. Explain scientific concepts with precision and accuracy.\n",
        "            Relate scientific principles to real-world applications when possible.\n",
        "            Use appropriate terminology while making complex ideas accessible.\"\"\",\n",
        "\n",
        "            \"history\": \"\"\"You are a history expert. Provide nuanced historical context and accurate chronology.\n",
        "            Consider multiple perspectives when discussing historical events and figures.\n",
        "            Connect historical facts to broader themes and patterns where relevant.\"\"\",\n",
        "\n",
        "            \"general\": \"\"\"You are a knowledgeable assistant with access to a knowledge base.\n",
        "            Your task is to provide accurate, relevant information based on the query and the retrieved context.\n",
        "            Think step by step and analyze the retrieved information carefully before formulating your final response.\"\"\"\n",
        "        }\n",
        "\n",
        "    def __call__(self, state: RAGState) -> RAGState:\n",
        "        \"\"\"Process the query with retrieved documents.\"\"\"\n",
        "        query = state[\"query\"]\n",
        "        retrieved_docs = state[\"context\"] or []\n",
        "        system_message = self.system_prompts.get(self.agent_type, self.system_prompts[\"general\"])\n",
        "\n",
        "        # Generate response based on retrieved documents\n",
        "        if not retrieved_docs:\n",
        "            # Handle the case when no relevant documents are found\n",
        "            prompt = f\"\"\"Query: {query}\n",
        "\n",
        "            No relevant documents were found in the {self.agent_type} knowledge base.\n",
        "            Please provide a general response based on your {self.agent_type} expertise.\n",
        "            \"\"\"\n",
        "            response = self.llm.generate(prompt, system_message)\n",
        "        else:\n",
        "            # Generate response from retrieved documents\n",
        "            context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "            # Generate thinking steps if enabled\n",
        "            thinking = \"\"\n",
        "            if self.config.thinking_steps:\n",
        "                thinking_prompt = f\"\"\"Query: {query}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Think step by step about this query from a {self.agent_type} perspective.\n",
        "                What are the key points to address? What information from the context is most relevant?\n",
        "                What additional {self.agent_type} knowledge might be needed?\n",
        "                \"\"\"\n",
        "                thinking = self.llm.generate(thinking_prompt, system_message)\n",
        "\n",
        "            # Generate the response\n",
        "            response_prompt = f\"\"\"Query: {query}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            {thinking if thinking else \"\"}\n",
        "\n",
        "            Based on the context provided and your expertise in {self.agent_type}, please answer the query.\n",
        "            If the context doesn't contain enough information, acknowledge this and provide the best answer\n",
        "            you can using your {self.agent_type} knowledge.\n",
        "            \"\"\"\n",
        "            response = self.llm.generate(response_prompt, system_message)\n",
        "\n",
        "        # Update the state\n",
        "        state[\"response\"] = response\n",
        "        if \"agent_responses\" not in state:\n",
        "            state[\"agent_responses\"] = {}\n",
        "        state[\"agent_responses\"][self.agent_type] = response\n",
        "        state[\"history\"].append({\n",
        "            \"step\": \"processing\",\n",
        "            \"agent_type\": self.agent_type,\n",
        "            \"response_length\": len(response),\n",
        "            \"timestamp\": time.time()\n",
        "        })\n",
        "\n",
        "        # Update messages for LangGraph tracking\n",
        "        if \"messages\" not in state or state[\"messages\"] is None:\n",
        "            state[\"messages\"] = []\n",
        "\n",
        "        state[\"messages\"].append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": response,\n",
        "            \"metadata\": {\"agent_type\": self.agent_type}\n",
        "        })\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "class DecisionNode:\n",
        "    \"\"\"LangGraph node for deciding next steps in the workflow.\"\"\"\n",
        "\n",
        "    def __init__(self, agent_type: str, llm: LocalLLM, config: RAGConfig):\n",
        "        self.agent_type = agent_type\n",
        "        self.llm = llm\n",
        "        self.config = config\n",
        "\n",
        "    def __call__(self, state: RAGState) -> Dict[str, Any]:\n",
        "        \"\"\"Decide if additional agents should be consulted.\"\"\"\n",
        "        query = state[\"query\"]\n",
        "        response = state[\"response\"]\n",
        "\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Current response from {self.agent_type} agent:\n",
        "        {response}\n",
        "\n",
        "        Analyze whether this query requires perspectives from other domains of knowledge.\n",
        "        Does it touch on multiple subjects beyond {self.agent_type} (e.g., both math and science,\n",
        "        or history and general knowledge)? Would the answer benefit significantly from additional domain expertise?\n",
        "\n",
        "        For each domain (math, science, history, general), assign a relevance score from 0-10,\n",
        "        where 0 means completely irrelevant and 10 means highly relevant.\n",
        "\n",
        "        Format your response as a JSON object with domain names as keys and scores as values.\n",
        "        Example: {{\"math\": 8, \"science\": 5, \"history\": 0, \"general\": 3}}\n",
        "        \"\"\"\n",
        "\n",
        "        system_message = \"\"\"You are a query analyzer determining whether a question spans multiple domains of knowledge.\n",
        "        Be selective - only assign high scores to domains that would provide significant value for this query.\"\"\"\n",
        "\n",
        "        try:\n",
        "            scores_text = self.llm.generate(prompt, system_message).strip()\n",
        "            # Extract JSON from the response (in case there's extra text)\n",
        "            json_match = re.search(r'\\{.*\\}', scores_text, re.DOTALL)\n",
        "            if json_match:\n",
        "                scores = json.loads(json_match.group(0))\n",
        "            else:\n",
        "                scores = {agent_type: 0 for agent_type in self.config.agent_types}\n",
        "                scores[self.agent_type] = 10  # Default high score for current agent\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing agent scores: {e}\")\n",
        "            scores = {agent_type: 0 for agent_type in self.config.agent_types}\n",
        "            scores[self.agent_type] = 10  # Default high score for current agent\n",
        "\n",
        "        # Determine the agent with the highest score (besides the current one)\n",
        "        scores[self.agent_type] = 0  # Zero out current agent to find the next best\n",
        "        next_agent_type = max(scores, key=scores.get)\n",
        "        next_agent_score = scores[next_agent_type]\n",
        "\n",
        "        # Update state history\n",
        "        state[\"history\"].append({\n",
        "            \"step\": \"routing_decision\",\n",
        "            \"current_agent\": self.agent_type,\n",
        "            \"scores\": scores,\n",
        "            \"timestamp\": time.time()\n",
        "        })\n",
        "\n",
        "        # Update messages for LangGraph tracking\n",
        "        if \"messages\" not in state or state[\"messages\"] is None:\n",
        "            state[\"messages\"] = []\n",
        "\n",
        "        # If the next best agent has a score above threshold, route to it\n",
        "        if next_agent_score >= 7:\n",
        "            state[\"messages\"].append({\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"Routing to {next_agent_type} agent for additional perspective.\"\n",
        "            })\n",
        "            return {\"next\": next_agent_type}\n",
        "        else:\n",
        "            # If no other agent is needed, move to synthesis\n",
        "            state[\"messages\"].append({\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Moving to synthesis phase.\"\n",
        "            })\n",
        "            return {\"next\": \"synthesis\"}\n",
        "\n",
        "\n",
        "class SynthesisNode:\n",
        "    \"\"\"LangGraph node for synthesizing responses.\"\"\"\n",
        "\n",
        "    def __init__(self, llm: LocalLLM, config: RAGConfig):\n",
        "        self.llm = llm\n",
        "        self.config = config\n",
        "\n",
        "    def __call__(self, state: RAGState) -> RAGState:\n",
        "        \"\"\"Synthesize responses from multiple agents into a coherent answer.\"\"\"\n",
        "        query = state[\"query\"]\n",
        "        agent_responses = state[\"agent_responses\"]\n",
        "\n",
        "        if len(agent_responses) <= 1:\n",
        "            # If only one agent responded, use that response directly\n",
        "            primary_agent = list(agent_responses.keys())[0]\n",
        "            state[\"final_response\"] = agent_responses[primary_agent]\n",
        "        else:\n",
        "            # Format all the responses for synthesis\n",
        "            responses_text = \"\"\n",
        "            for agent_type, response in agent_responses.items():\n",
        "                responses_text += f\"{agent_type.capitalize()} perspective:\\n{response}\\n\\n\"\n",
        "\n",
        "            prompt = f\"\"\"Query: {query}\n",
        "\n",
        "            Different expert perspectives:\n",
        "            {responses_text}\n",
        "\n",
        "            Synthesize these different expert perspectives into a comprehensive, coherent response.\n",
        "            Highlight where the different perspectives complement each other and provide a more complete answer.\n",
        "            Ensure the final response is well-structured and addresses all aspects of the original query.\n",
        "            \"\"\"\n",
        "\n",
        "            system_message = \"\"\"You are a synthesis expert combining insights from multiple domain experts.\n",
        "            Create a unified response that preserves the valuable contributions from each expert while removing redundancies.\n",
        "            Acknowledge the different domains of expertise when they provide unique perspectives.\"\"\"\n",
        "\n",
        "            synthesized_response = self.llm.generate(prompt, system_message)\n",
        "            state[\"final_response\"] = synthesized_response\n",
        "\n",
        "        state[\"status\"] = \"COMPLETE\"\n",
        "        state[\"history\"].append({\n",
        "            \"step\": \"synthesis\",\n",
        "            \"agent_count\": len(agent_responses),\n",
        "            \"timestamp\": time.time()\n",
        "        })\n",
        "\n",
        "        # Update messages for LangGraph tracking\n",
        "        if \"messages\" not in state or state[\"messages\"] is None:\n",
        "            state[\"messages\"] = []\n",
        "\n",
        "        state[\"messages\"].append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": state[\"final_response\"],\n",
        "            \"metadata\": {\"synthesized\": True}\n",
        "        })\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "# LangGraph tool definition for specialized tools\n",
        "class MathTool(BaseModel):\n",
        "    \"\"\"Tool for performing mathematical calculations.\"\"\"\n",
        "    expression: str = Field(..., description=\"The mathematical expression to evaluate\")\n",
        "\n",
        "    def execute(self) -> str:\n",
        "        \"\"\"Safely evaluate the mathematical expression.\"\"\"\n",
        "        try:\n",
        "            # Use a restricted environment to evaluate the expression\n",
        "            # This is a simplified implementation - in production code, use a safer approach\n",
        "            import math\n",
        "            allowed_names = {\n",
        "                k: v for k, v in math.__dict__.items()\n",
        "                if not k.startswith('__')\n",
        "            }\n",
        "            result = eval(self.expression, {\"__builtins__\": {}}, allowed_names)\n",
        "            return f\"Result: {result}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error evaluating expression: {str(e)}\"\n",
        "\n",
        "class MultiAgentRAG:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "\n",
        "        # Initialize shared components\n",
        "        self.embedding_model = SentenceTransformerEmbedding(config)\n",
        "        self.llm = LocalLLM(config)\n",
        "        self.document_processor = DocumentProcessor(config)\n",
        "\n",
        "        # Initialize agent components\n",
        "        self.router = RouterNode(config, self.llm)\n",
        "        self.synthesizer = SynthesisNode(self.llm, config)\n",
        "\n",
        "        # Initialize specialized agents\n",
        "        self.agents = {}\n",
        "        self.vector_stores = {}\n",
        "        self.retrieval_nodes = {}\n",
        "        self.processing_nodes = {}\n",
        "        self.decision_nodes = {}\n",
        "\n",
        "        for agent_type in config.agent_types:\n",
        "            print(f\"Initializing {agent_type} agent...\")\n",
        "            # Create vector store for this agent type\n",
        "            self.vector_stores[agent_type] = VectorStore(config, self.embedding_model, agent_type)\n",
        "            self.vector_stores[agent_type].load_vector_store()\n",
        "\n",
        "            # Create LangGraph nodes for this agent type\n",
        "            self.retrieval_nodes[agent_type] = RetrievalNode(agent_type, self.vector_stores[agent_type], config)\n",
        "            self.processing_nodes[agent_type] = ProcessingNode(agent_type, self.llm, config)\n",
        "            self.decision_nodes[agent_type] = DecisionNode(agent_type, self.llm, config)\n",
        "\n",
        "        # Create LangGraph workflow\n",
        "        self.workflow = self._build_workflow()\n",
        "\n",
        "        # Initialize the checkpoint system for state persistence\n",
        "        self.checkpointer = MemorySaver()\n",
        "        self.app = self.workflow.compile(checkpointer=self.checkpointer)\n",
        "\n",
        "    def _build_workflow(self) -> StateGraph:\n",
        "      # Create a new StateGraph with the enhanced state type\n",
        "      workflow = StateGraph(RAGState)\n",
        "\n",
        "      # Add the router node\n",
        "      workflow.add_node(\"router\", self.router)\n",
        "\n",
        "      # Add synthesis node\n",
        "      workflow.add_node(\"synthesizer\", self.synthesizer)\n",
        "\n",
        "      # Add agent nodes for each agent type\n",
        "      for agent_type in self.config.agent_types:\n",
        "          # Add retrieval node\n",
        "          workflow.add_node(f\"{agent_type}_retrieval\", self.retrieval_nodes[agent_type])\n",
        "\n",
        "          # Add processing node\n",
        "          workflow.add_node(f\"{agent_type}_processing\", self.processing_nodes[agent_type])\n",
        "\n",
        "          # Add decision node\n",
        "          workflow.add_node(f\"{agent_type}_decision\", self.decision_nodes[agent_type])\n",
        "\n",
        "      # Define the edges in the workflow\n",
        "\n",
        "      # Start with the router\n",
        "      workflow.set_entry_point(\"router\")\n",
        "\n",
        "      # Connect router to appropriate retrieval nodes using conditional edges\n",
        "      workflow.add_conditional_edges(\n",
        "          \"router\",\n",
        "          lambda state: state[\"agent_type\"],  # Route based on agent_type value\n",
        "          {agent_type: f\"{agent_type}_retrieval\" for agent_type in self.config.agent_types}\n",
        "      )\n",
        "\n",
        "      # Connect retrieval to processing and processing to decision for each agent\n",
        "      for agent_type in self.config.agent_types:\n",
        "          # Connect retrieval to processing\n",
        "          workflow.add_edge(f\"{agent_type}_retrieval\", f\"{agent_type}_processing\")\n",
        "\n",
        "          # Connect processing to decision\n",
        "          workflow.add_edge(f\"{agent_type}_processing\", f\"{agent_type}_decision\")\n",
        "\n",
        "          # Connect decision to other agent retrievals or synthesis\n",
        "          workflow.add_conditional_edges(\n",
        "              f\"{agent_type}_decision\",\n",
        "              lambda output: output[\"next\"],\n",
        "              {\n",
        "                  \"synthesis\": \"synthesizer\",\n",
        "                  **{other_agent: f\"{other_agent}_retrieval\" for other_agent in self.config.agent_types}\n",
        "              }\n",
        "          )\n",
        "\n",
        "      # End at synthesis\n",
        "      workflow.add_edge(\"synthesizer\", END)\n",
        "\n",
        "      return workflow\n",
        "\n",
        "    def process_query(self, query: str, thread_id: str = None) -> Dict[str, Any]:\n",
        "      if thread_id is None:\n",
        "          thread_id = str(uuid.uuid4())\n",
        "      checkpoint_ns = \"default_ns\"  # Use a meaningful namespace identifier\n",
        "      checkpoint_id = f\"chk_{thread_id}\"  # Unique checkpoint identifier based on thread ID\n",
        "\n",
        "      state = {\n",
        "          \"query\": query,\n",
        "          \"agent_type\": \"\",\n",
        "          \"context\": None,\n",
        "          \"response\": None,\n",
        "          \"thread_id\": thread_id,\n",
        "          \"checkpoint_ns\": checkpoint_ns,\n",
        "          \"checkpoint_id\": checkpoint_id,\n",
        "          \"history\": [],\n",
        "          \"agent_responses\": {},\n",
        "          \"final_response\": None,\n",
        "          \"status\": \"ROUTING\",\n",
        "          \"messages\": [],\n",
        "          \"tools\": None,\n",
        "          \"next_steps\": []\n",
        "      }\n",
        "\n",
        "      result = self.app.invoke(\n",
        "          state,\n",
        "          config={\"configurable\": {  # <--- This is the crucial part\n",
        "              \"thread_id\": thread_id,\n",
        "              \"checkpoint_ns\": checkpoint_ns,\n",
        "              \"checkpoint_id\": checkpoint_id\n",
        "          }}\n",
        "      )\n",
        "      return result\n",
        "\n",
        "\n",
        "    def add_documents(self, file_obj: BinaryIO, filename: str, agent_type: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"Add documents to appropriate vector stores.\"\"\"\n",
        "        try:\n",
        "            # Process the uploaded file\n",
        "            documents = self.document_processor.process_uploaded_file(file_obj, filename)\n",
        "\n",
        "            results = {}\n",
        "\n",
        "            if agent_type and agent_type in self.config.agent_types:\n",
        "                # Add to specific agent vector store\n",
        "                self.vector_stores[agent_type].add_documents(documents)\n",
        "                results[agent_type] = len(documents)\n",
        "            else:\n",
        "                # Add to all vector stores\n",
        "                for agent_type in self.config.agent_types:\n",
        "                    self.vector_stores[agent_type].add_documents(documents)\n",
        "                    results[agent_type] = len(documents)\n",
        "\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"message\": f\"Added {len(documents)} documents\",\n",
        "                \"details\": results\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": f\"Error adding documents: {str(e)}\"\n",
        "            }\n",
        "    # Inside the MultiAgentRAG class definition:\n",
        "\n",
        "    def get_graph_image(self, output_path: str = None) -> bytes:\n",
        "        \"\"\"\n",
        "        Generates a PNG image representation of the LangGraph workflow.\n",
        "\n",
        "        Args:\n",
        "            output_path: Optional path to save the PNG image file.\n",
        "\n",
        "        Returns:\n",
        "            PNG image data as bytes, or None if generation fails.\n",
        "\n",
        "        Raises:\n",
        "            ImportError: If pygraphviz or graphviz are not installed.\n",
        "        \"\"\"\n",
        "        print(\"Attempting to generate graph visualization...\")\n",
        "        try:\n",
        "            # ***** FIX HERE *****\n",
        "            # Call get_graph() on the compiled app (self.app), NOT the workflow definition\n",
        "            graph = self.app.get_graph()\n",
        "\n",
        "            # Draw the graph to PNG bytes\n",
        "            png_bytes = graph.draw_png()\n",
        "\n",
        "            if output_path:\n",
        "                try:\n",
        "                    with open(output_path, \"wb\") as f:\n",
        "                        f.write(png_bytes)\n",
        "                    print(f\"Workflow graph saved to {output_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving graph to {output_path}: {e}\")\n",
        "\n",
        "            print(\"Graph image generated successfully.\")\n",
        "            return png_bytes\n",
        "\n",
        "        # Add specific handling for AttributeError if get_graph itself is wrong\n",
        "        except AttributeError as ae:\n",
        "             if 'get_graph' in str(ae):\n",
        "                 print(\"\\nERROR: The method 'get_graph()' was not found on the compiled app.\")\n",
        "                 print(\"The LangGraph API might have changed. Please check the LangGraph documentation for the current way to visualize graphs.\")\n",
        "             else:\n",
        "                 print(f\"\\nAn AttributeError occurred during graph generation: {ae}\") # Print other AttributeErrors\n",
        "             # Try ASCII fallback on the app too\n",
        "             try:\n",
        "                 print(\"\\nAttempting ASCII fallback:\")\n",
        "                 # ***** FIX HERE for fallback *****\n",
        "                 ascii_art = self.app.get_graph().draw_ascii() # Use self.app here too\n",
        "                 print(ascii_art)\n",
        "             except Exception as ascii_e:\n",
        "                 print(f\"Could not generate ASCII graph: {ascii_e}\")\n",
        "             return None\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"\\nERROR: Missing Dependencies for Graph Visualization!\")\n",
        "            print(\"Please ensure 'pygraphviz' Python package and the 'graphviz' system library are installed.\")\n",
        "            print(\"In Colab/Debian/Ubuntu, run:\")\n",
        "            print(\"!pip install pygraphviz -qq\")\n",
        "            print(\"!apt-get update -qq && apt-get install -y graphviz graphviz-dev -qq\")\n",
        "            # Optional: Provide ASCII fallback (also on self.app)\n",
        "            try:\n",
        "                print(\"\\nAttempting ASCII fallback:\")\n",
        "                # ***** FIX HERE for fallback *****\n",
        "                ascii_art = self.app.get_graph().draw_ascii() # Use self.app here too\n",
        "                print(ascii_art)\n",
        "            except Exception as ascii_e:\n",
        "                print(f\"Could not generate ASCII graph: {ascii_e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            # Catch potential errors from draw_png if graphviz is installed but fails\n",
        "             print(f\"\\nAn unexpected error occurred during graph generation: {e}\")\n",
        "             print(\"Ensure the Graphviz library executables (like 'dot') are in your system's PATH.\")\n",
        "             # Optional: Provide ASCII fallback (also on self.app)\n",
        "             try:\n",
        "                 print(\"\\nAttempting ASCII fallback:\")\n",
        "                 # ***** FIX HERE for fallback *****\n",
        "                 ascii_art = self.app.get_graph().draw_ascii() # Use self.app here too\n",
        "                 print(ascii_art)\n",
        "             except Exception as ascii_e:\n",
        "                 print(f\"Could not generate ASCII graph: {ascii_e}\")\n",
        "             return None\n",
        "\n",
        "    # The display_graph method doesn't need changes as it calls get_graph_image\n",
        "    def display_graph(self):\n",
        "        \"\"\"Generates and displays the graph image in a Colab/Jupyter environment.\"\"\"\n",
        "        try:\n",
        "            from IPython.display import Image, display\n",
        "            # Generate the image bytes using the now corrected get_graph_image\n",
        "            png_bytes = self.get_graph_image()\n",
        "\n",
        "            if png_bytes:\n",
        "                print(\"Displaying graph...\")\n",
        "                display(Image(png_bytes))\n",
        "            else:\n",
        "                # get_graph_image already prints error messages / ASCII fallback\n",
        "                print(\"Could not generate or display graph image.\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"Could not import IPython.display. Are you in a Colab/Jupyter environment?\")\n",
        "            print(\"Try calling get_graph_image(output_path='workflow.png') to save the file instead.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while trying to display the graph: {e}\")\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up resources.\"\"\"\n",
        "        self.document_processor.cleanup()\n",
        "def run_in_colab():\n",
        "    from google.colab import files\n",
        "    import io\n",
        "    import time\n",
        "    import uuid\n",
        "    # Import necessary display tools if you call display_graph\n",
        "    # from IPython.display import Image, display # Import moved inside display_graph\n",
        "\n",
        "    config = RAGConfig()\n",
        "    print(f\"Initializing MultiAgentRAG with models on {config.device}\")\n",
        "    rag_system = MultiAgentRAG(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nMultiAgentRAG with Specialized Experts\")\n",
        "        print(\"1. Upload and ingest a file\")\n",
        "        print(\"2. Upload and ingest a file for a specific agent\")\n",
        "        print(\"3. Ask a question\")\n",
        "        print(\"4. Show Workflow Graph\") # New Option\n",
        "        print(\"5. Exit\") # Adjusted number\n",
        "\n",
        "        choice = input(\"Enter your choice (1-5): \") # Adjusted range\n",
        "\n",
        "        if choice == \"1\":\n",
        "            # ... (keep existing code for choice 1)\n",
        "            try:\n",
        "                print(\"Please select a file to upload...\")\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                for filename, content in uploaded.items():\n",
        "                    file_obj = io.BytesIO(content)\n",
        "                    result = rag_system.add_documents(file_obj, filename)\n",
        "                    print(f\"Result: {result}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading and ingesting file: {e}\")\n",
        "\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            # ... (keep existing code for choice 2)\n",
        "            try:\n",
        "                print(\"Available agents: \" + \", \".join(config.agent_types))\n",
        "                agent_type = input(\"Enter the agent type for this document: \").lower()\n",
        "\n",
        "                if agent_type not in config.agent_types:\n",
        "                    print(f\"Invalid agent type. Please choose from: {', '.join(config.agent_types)}\")\n",
        "                    continue\n",
        "\n",
        "                print(\"Please select a file to upload...\")\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                for filename, content in uploaded.items():\n",
        "                    file_obj = io.BytesIO(content)\n",
        "                    result = rag_system.add_documents(file_obj, filename, agent_type)\n",
        "                    print(f\"Result: {result}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading and ingesting file: {e}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            # ... (keep existing code for choice 3 - using process_query)\n",
        "            query = input(\"Enter your question: \")\n",
        "            try:\n",
        "                thread_id = str(uuid.uuid4())\n",
        "                print(f\"Processing with thread ID: {thread_id}\")\n",
        "\n",
        "                start_time = time.time()\n",
        "                result = rag_system.process_query(query, thread_id=thread_id) # Use the fixed call\n",
        "                end_time = time.time()\n",
        "\n",
        "                final_response = result.get('final_response', 'No response generated')\n",
        "                status = result.get('status', 'Unknown status')\n",
        "\n",
        "                print(f\"\\nFinal Response: {final_response}\")\n",
        "                print(f\"Status: {status}\")\n",
        "                print(f\"Query processed in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                print(f\"Error processing query: {e}\")\n",
        "                print(traceback.format_exc())\n",
        "\n",
        "        elif choice == \"4\": # New handler for showing the graph\n",
        "            print(\"Generating and displaying workflow graph...\")\n",
        "            rag_system.display_graph() # Call the new method\n",
        "\n",
        "        elif choice == \"5\": # Adjusted exit choice\n",
        "            print(\"Exiting MultiAgentRAG system. Cleaning up resources...\")\n",
        "            rag_system.cleanup()\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter a number between 1 and 5.\") # Adjusted range\n",
        "\n",
        "# Assuming RAGConfig is defined elsewhere and necessary imports are present\n",
        "if __name__ == \"__main__\":\n",
        "    run_in_colab()"
      ],
      "metadata": {
        "id": "WWMcCudKivZb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}