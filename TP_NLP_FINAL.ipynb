{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOX+NsNHtHjXG72K/o9nJN+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saoudyahya/NLP-TP/blob/main/TP_NLP_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QcKe1BMSw25",
        "outputId": "dc16417d-23c9-44c2-81ff-2fc082ca1280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.23)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (4.13.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.50\n",
            "    Uninstalling langchain-core-0.3.50:\n",
            "      Successfully uninstalled langchain-core-0.3.50\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.7\n",
            "    Uninstalling langchain-text-splitters-0.3.7:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.7\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.22\n",
            "    Uninstalling langchain-0.3.22:\n",
            "      Successfully uninstalled langchain-0.3.22\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.23 langchain-core-0.3.51 langchain-text-splitters-0.3.8 langchain_community-0.3.21 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain_community langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n9CXDG-S212",
        "outputId": "7aaf3afc-ae0a-4790-84dd-69b9a97bd7ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20250327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 PDFLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV1isfduS5_4",
        "outputId": "335d0fb2-bdaf-4e28-f302-fc0e40cf3ff9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement PDFLoader (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for PDFLoader\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c59RnDFRS4s6",
        "outputId": "b099ff22-8c31-4420-f40c-a4b141106015"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9HFDmtRS8S8",
        "outputId": "4ffa017b-9ed5-4902-b57d-10a9bc02faa6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.23.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.23.3-py3-none-any.whl (46.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.3 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-Pkq7QNS_2S",
        "outputId": "85734462-9419-4263-9461-40cde65e59f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.23.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.1)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=c0de5bb2c4056941e90852553447dd201a6f30195aeba84a7e10c74ae84eef65\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, build, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.46.1\n",
            "    Uninstalling starlette-0.46.1:\n",
            "      Successfully uninstalled starlette-0.46.1\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.115.12\n",
            "    Uninstalling fastapi-0.115.12:\n",
            "      Successfully uninstalled fastapi-0.115.12\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.2 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.0 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-util-http-0.52b1 overrides-7.7.0 posthog-3.23.0 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.45.3 uvloop-0.21.0 watchfiles-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title AGENTIC RAG DEMO\n",
        "\n",
        "%%html\n",
        "\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<svg viewBox=\"0 0 800 600\" xmlns=\"http://www.w3.org/2000/svg\">\n",
        "  <!-- Background -->\n",
        "  <rect width=\"800\" height=\"600\" fill=\"#f8f9fa\" rx=\"10\" ry=\"10\"/>\n",
        "\n",
        "  <!-- Title -->\n",
        "  <text x=\"400\" y=\"40\" font-family=\"Arial\" font-size=\"24\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#333\">Agentic RAG System Architecture</text>\n",
        "\n",
        "  <!-- Document Processing Section -->\n",
        "  <rect x=\"40\" y=\"80\" width=\"200\" height=\"140\" fill=\"#e3f2fd\" stroke=\"#2196f3\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
        "  <text x=\"140\" y=\"100\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#1565c0\">Document Processing</text>\n",
        "  <text x=\"140\" y=\"125\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">DocumentProcessor</text>\n",
        "  <text x=\"140\" y=\"150\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Load documents</text>\n",
        "  <text x=\"140\" y=\"170\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Split into chunks</text>\n",
        "  <text x=\"140\" y=\"190\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Process metadata</text>\n",
        "\n",
        "  <!-- Embedding Model Section -->\n",
        "  <rect x=\"40\" y=\"240\" width=\"200\" height=\"140\" fill=\"#e8f5e9\" stroke=\"#4caf50\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
        "  <text x=\"140\" y=\"260\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#2e7d32\">Embedding Model</text>\n",
        "  <text x=\"140\" y=\"285\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">SentenceTransformerEmbedding</text>\n",
        "  <text x=\"140\" y=\"310\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Document embeddings</text>\n",
        "  <text x=\"140\" y=\"330\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Query embeddings</text>\n",
        "  <text x=\"140\" y=\"350\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Semantic encoding</text>\n",
        "\n",
        "  <!-- Vector Store Section -->\n",
        "  <rect x=\"300\" y=\"80\" width=\"200\" height=\"140\" fill=\"#fff3e0\" stroke=\"#ff9800\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
        "  <text x=\"400\" y=\"100\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#e65100\">Vector Store</text>\n",
        "  <text x=\"400\" y=\"125\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">Chroma</text>\n",
        "  <text x=\"400\" y=\"150\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Store document embeddings</text>\n",
        "  <text x=\"400\" y=\"170\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Similarity search</text>\n",
        "  <text x=\"400\" y=\"190\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Document retrieval</text>\n",
        "\n",
        "  <!-- LLM Section -->\n",
        "  <rect x=\"300\" y=\"240\" width=\"200\" height=\"140\" fill=\"#f3e5f5\" stroke=\"#9c27b0\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
        "  <text x=\"400\" y=\"260\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#6a1b9a\">Local LLM</text>\n",
        "  <text x=\"400\" y=\"285\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">TinyLlama-1.1B-Chat</text>\n",
        "  <text x=\"400\" y=\"310\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Response generation</text>\n",
        "  <text x=\"400\" y=\"330\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Query refinement</text>\n",
        "  <text x=\"400\" y=\"350\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Self-reflection</text>\n",
        "\n",
        "  <!-- Agentic Process Section -->\n",
        "  <rect x=\"560\" y=\"80\" width=\"200\" height=\"300\" fill=\"#e8eaf6\" stroke=\"#3f51b5\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
        "  <text x=\"660\" y=\"100\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#283593\">Agentic Process</text>\n",
        "  <text x=\"660\" y=\"125\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">_agentic_process</text>\n",
        "\n",
        "  <!-- Inside agentic process -->\n",
        "  <rect x=\"580\" y=\"140\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
        "  <text x=\"660\" y=\"160\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">1. Initial Document Retrieval</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"180\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
        "  <text x=\"660\" y=\"200\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">2. Thinking Steps</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"220\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
        "  <text x=\"660\" y=\"240\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">3. Generate Response</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"260\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
        "  <text x=\"660\" y=\"280\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">4. Evaluate Completeness</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"300\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
        "  <text x=\"660\" y=\"320\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">5. Query Refinement</text>\n",
        "\n",
        "  <rect x=\"580\" y=\"340\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
        "  <text x=\"660\" y=\"360\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">6. Final Synthesis</text>\n",
        "\n",
        "  <!-- Main Workflow -->\n",
        "  <rect x=\"40\" y=\"400\" width=\"720\" height=\"160\" fill=\"#fce4ec\" stroke=\"#e91e63\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
        "  <text x=\"400\" y=\"420\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#c2185b\">Multi-Iteration Workflow</text>\n",
        "\n",
        "  <!-- Workflow steps - FIXED LEFT TO RIGHT FLOW -->\n",
        "  <circle cx=\"100\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"100\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">1</text>\n",
        "  <text x=\"100\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Query</text>\n",
        "\n",
        "  <polygon points=\"130,460 150,450 150,470\" fill=\"#880e4f\"/>\n",
        "\n",
        "  <circle cx=\"180\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"180\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">2</text>\n",
        "  <text x=\"180\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Retrieve</text>\n",
        "\n",
        "  <polygon points=\"210,460 230,450 230,470\" fill=\"#880e4f\"/>\n",
        "\n",
        "  <circle cx=\"260\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"260\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">3</text>\n",
        "  <text x=\"260\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Think</text>\n",
        "\n",
        "  <polygon points=\"290,460 310,450 310,470\" fill=\"#880e4f\"/>\n",
        "\n",
        "  <circle cx=\"340\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"340\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">4</text>\n",
        "  <text x=\"340\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Generate</text>\n",
        "\n",
        "  <polygon points=\"370,460 390,450 390,470\" fill=\"#880e4f\"/>\n",
        "\n",
        "  <circle cx=\"420\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"420\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">5</text>\n",
        "  <text x=\"420\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Evaluate</text>\n",
        "\n",
        "  <!-- Conditional branch -->\n",
        "  <path d=\"M 450,460 L 470,460 L 470,430 L 490,430\" fill=\"none\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"470\" y=\"420\" font-family=\"Arial\" font-size=\"10\" text-anchor=\"middle\">Complete</text>\n",
        "\n",
        "  <path d=\"M 450,460 L 470,460 L 470,490 L 490,490\" fill=\"none\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"470\" y=\"510\" font-family=\"Arial\" font-size=\"10\" text-anchor=\"middle\">Continue</text>\n",
        "\n",
        "  <circle cx=\"520\" cy=\"430\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"520\" y=\"435\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">6A</text>\n",
        "  <text x=\"520\" y=\"470\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Synthesize</text>\n",
        "\n",
        "  <circle cx=\"520\" cy=\"490\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"520\" y=\"495\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">6B</text>\n",
        "  <text x=\"520\" y=\"530\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Refine</text>\n",
        "\n",
        "  <!-- Feedback loop from Refine back to Retrieve -->\n",
        "  <path d=\"M 545,490 L 580,490 L 580,520 L 180,520 L 180,485\" fill=\"none\" stroke=\"#880e4f\" stroke-width=\"2\" stroke-dasharray=\"5,3\"/>\n",
        "  <polygon points=\"175,495 180,485 185,495\" fill=\"#880e4f\"/>\n",
        "\n",
        "  <polygon points=\"550,430 570,420 570,440\" fill=\"#880e4f\"/>\n",
        "\n",
        "  <circle cx=\"600\" cy=\"430\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
        "  <text x=\"600\" y=\"435\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">7</text>\n",
        "  <text x=\"600\" y=\"470\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Response</text>\n",
        "\n",
        "  <!-- Connections between sections -->\n",
        "  <path d=\"M 140,220 L 140,240\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
        "  <path d=\"M 240,150 L 300,150\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
        "  <path d=\"M 240,310 L 300,310\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
        "  <path d=\"M 500,150 L 560,150\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
        "  <path d=\"M 500,310 L 560,250\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
        "</svg>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "zKvsSG6JXB00",
        "outputId": "16addd23-ad8f-44ee-a404-46e9fd5fe99d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
              "<svg viewBox=\"0 0 800 600\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "  <!-- Background -->\n",
              "  <rect width=\"800\" height=\"600\" fill=\"#f8f9fa\" rx=\"10\" ry=\"10\"/>\n",
              "  \n",
              "  <!-- Title -->\n",
              "  <text x=\"400\" y=\"40\" font-family=\"Arial\" font-size=\"24\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#333\">Agentic RAG System Architecture</text>\n",
              "  \n",
              "  <!-- Document Processing Section -->\n",
              "  <rect x=\"40\" y=\"80\" width=\"200\" height=\"140\" fill=\"#e3f2fd\" stroke=\"#2196f3\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
              "  <text x=\"140\" y=\"100\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#1565c0\">Document Processing</text>\n",
              "  <text x=\"140\" y=\"125\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">DocumentProcessor</text>\n",
              "  <text x=\"140\" y=\"150\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Load documents</text>\n",
              "  <text x=\"140\" y=\"170\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Split into chunks</text>\n",
              "  <text x=\"140\" y=\"190\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Process metadata</text>\n",
              "  \n",
              "  <!-- Embedding Model Section -->\n",
              "  <rect x=\"40\" y=\"240\" width=\"200\" height=\"140\" fill=\"#e8f5e9\" stroke=\"#4caf50\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
              "  <text x=\"140\" y=\"260\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#2e7d32\">Embedding Model</text>\n",
              "  <text x=\"140\" y=\"285\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">SentenceTransformerEmbedding</text>\n",
              "  <text x=\"140\" y=\"310\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Document embeddings</text>\n",
              "  <text x=\"140\" y=\"330\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Query embeddings</text>\n",
              "  <text x=\"140\" y=\"350\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Semantic encoding</text>\n",
              "  \n",
              "  <!-- Vector Store Section -->\n",
              "  <rect x=\"300\" y=\"80\" width=\"200\" height=\"140\" fill=\"#fff3e0\" stroke=\"#ff9800\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
              "  <text x=\"400\" y=\"100\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#e65100\">Vector Store</text>\n",
              "  <text x=\"400\" y=\"125\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">Chroma</text>\n",
              "  <text x=\"400\" y=\"150\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Store document embeddings</text>\n",
              "  <text x=\"400\" y=\"170\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Similarity search</text>\n",
              "  <text x=\"400\" y=\"190\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Document retrieval</text>\n",
              "  \n",
              "  <!-- LLM Section -->\n",
              "  <rect x=\"300\" y=\"240\" width=\"200\" height=\"140\" fill=\"#f3e5f5\" stroke=\"#9c27b0\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
              "  <text x=\"400\" y=\"260\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#6a1b9a\">Local LLM</text>\n",
              "  <text x=\"400\" y=\"285\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">TinyLlama-1.1B-Chat</text>\n",
              "  <text x=\"400\" y=\"310\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Response generation</text>\n",
              "  <text x=\"400\" y=\"330\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Query refinement</text>\n",
              "  <text x=\"400\" y=\"350\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">- Self-reflection</text>\n",
              "  \n",
              "  <!-- Agentic Process Section -->\n",
              "  <rect x=\"560\" y=\"80\" width=\"200\" height=\"300\" fill=\"#e8eaf6\" stroke=\"#3f51b5\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
              "  <text x=\"660\" y=\"100\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#283593\">Agentic Process</text>\n",
              "  <text x=\"660\" y=\"125\" font-family=\"Arial\" font-size=\"14\" text-anchor=\"middle\">_agentic_process</text>\n",
              "  \n",
              "  <!-- Inside agentic process -->\n",
              "  <rect x=\"580\" y=\"140\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
              "  <text x=\"660\" y=\"160\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">1. Initial Document Retrieval</text>\n",
              "  \n",
              "  <rect x=\"580\" y=\"180\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
              "  <text x=\"660\" y=\"200\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">2. Thinking Steps</text>\n",
              "  \n",
              "  <rect x=\"580\" y=\"220\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
              "  <text x=\"660\" y=\"240\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">3. Generate Response</text>\n",
              "  \n",
              "  <rect x=\"580\" y=\"260\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
              "  <text x=\"660\" y=\"280\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">4. Evaluate Completeness</text>\n",
              "  \n",
              "  <rect x=\"580\" y=\"300\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
              "  <text x=\"660\" y=\"320\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">5. Query Refinement</text>\n",
              "  \n",
              "  <rect x=\"580\" y=\"340\" width=\"160\" height=\"30\" fill=\"#c5cae9\" stroke=\"#3f51b5\" stroke-width=\"1\" rx=\"3\" ry=\"3\"/>\n",
              "  <text x=\"660\" y=\"360\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">6. Final Synthesis</text>\n",
              "  \n",
              "  <!-- Main Workflow -->\n",
              "  <rect x=\"40\" y=\"400\" width=\"720\" height=\"160\" fill=\"#fce4ec\" stroke=\"#e91e63\" stroke-width=\"2\" rx=\"5\" ry=\"5\"/>\n",
              "  <text x=\"400\" y=\"420\" font-family=\"Arial\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#c2185b\">Multi-Iteration Workflow</text>\n",
              "  \n",
              "  <!-- Workflow steps - FIXED LEFT TO RIGHT FLOW -->\n",
              "  <circle cx=\"100\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"100\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">1</text>\n",
              "  <text x=\"100\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Query</text>\n",
              "  \n",
              "  <polygon points=\"130,460 150,450 150,470\" fill=\"#880e4f\"/>\n",
              "  \n",
              "  <circle cx=\"180\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"180\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">2</text>\n",
              "  <text x=\"180\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Retrieve</text>\n",
              "  \n",
              "  <polygon points=\"210,460 230,450 230,470\" fill=\"#880e4f\"/>\n",
              "  \n",
              "  <circle cx=\"260\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"260\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">3</text>\n",
              "  <text x=\"260\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Think</text>\n",
              "  \n",
              "  <polygon points=\"290,460 310,450 310,470\" fill=\"#880e4f\"/>\n",
              "  \n",
              "  <circle cx=\"340\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"340\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">4</text>\n",
              "  <text x=\"340\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Generate</text>\n",
              "  \n",
              "  <polygon points=\"370,460 390,450 390,470\" fill=\"#880e4f\"/>\n",
              "  \n",
              "  <circle cx=\"420\" cy=\"460\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"420\" y=\"465\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">5</text>\n",
              "  <text x=\"420\" y=\"500\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Evaluate</text>\n",
              "  \n",
              "  <!-- Conditional branch -->\n",
              "  <path d=\"M 450,460 L 470,460 L 470,430 L 490,430\" fill=\"none\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"470\" y=\"420\" font-family=\"Arial\" font-size=\"10\" text-anchor=\"middle\">Complete</text>\n",
              "  \n",
              "  <path d=\"M 450,460 L 470,460 L 470,490 L 490,490\" fill=\"none\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"470\" y=\"510\" font-family=\"Arial\" font-size=\"10\" text-anchor=\"middle\">Continue</text>\n",
              "  \n",
              "  <circle cx=\"520\" cy=\"430\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"520\" y=\"435\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">6A</text>\n",
              "  <text x=\"520\" y=\"470\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Synthesize</text>\n",
              "  \n",
              "  <circle cx=\"520\" cy=\"490\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"520\" y=\"495\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">6B</text>\n",
              "  <text x=\"520\" y=\"530\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Refine</text>\n",
              "  \n",
              "  <!-- Feedback loop from Refine back to Retrieve -->\n",
              "  <path d=\"M 545,490 L 580,490 L 580,520 L 180,520 L 180,485\" fill=\"none\" stroke=\"#880e4f\" stroke-width=\"2\" stroke-dasharray=\"5,3\"/>\n",
              "  <polygon points=\"175,495 180,485 185,495\" fill=\"#880e4f\"/>\n",
              "  \n",
              "  <polygon points=\"550,430 570,420 570,440\" fill=\"#880e4f\"/>\n",
              "  \n",
              "  <circle cx=\"600\" cy=\"430\" r=\"25\" fill=\"#e91e63\" stroke=\"#880e4f\" stroke-width=\"2\"/>\n",
              "  <text x=\"600\" y=\"435\" font-family=\"Arial\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"white\">7</text>\n",
              "  <text x=\"600\" y=\"470\" font-family=\"Arial\" font-size=\"12\" text-anchor=\"middle\">Response</text>\n",
              "  \n",
              "  <!-- Connections between sections -->\n",
              "  <path d=\"M 140,220 L 140,240\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
              "  <path d=\"M 240,150 L 300,150\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
              "  <path d=\"M 240,310 L 300,310\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
              "  <path d=\"M 500,150 L 560,150\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
              "  <path d=\"M 500,310 L 560,250\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" stroke-dasharray=\"4,2\"/>\n",
              "</svg>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import tempfile\n",
        "from typing import List, Dict, Any, Optional, Tuple, BinaryIO\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PDFMinerLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "\n",
        "class RAGConfig:\n",
        "    def __init__(self):\n",
        "        # Local model configuration - using publicly available models\n",
        "        self.embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"  # Public embedding model\n",
        "        self.llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Public LLM model\n",
        "\n",
        "        # Model parameters\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.torch_dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "\n",
        "        # Vector database configuration\n",
        "        self.vector_db_path = \"chroma_db\"  # Changed from vector_db to chroma_db\n",
        "\n",
        "        # Document processing configuration\n",
        "        self.chunk_size = 500\n",
        "        self.chunk_overlap = 100\n",
        "\n",
        "        # Search configuration\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.5\n",
        "\n",
        "        # Agent configuration\n",
        "        self.max_iterations = 3  # Reduced for faster execution\n",
        "        self.thinking_steps = True\n",
        "\n",
        "# Embedding class for document and query encoding - implement Embeddings interface\n",
        "class SentenceTransformerEmbedding(Embeddings):\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading embedding model {config.embedding_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.embedding_model_name)\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            config.embedding_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(\"Embedding model loaded successfully\")\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
        "        return self._get_embeddings(texts)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for a query string.\"\"\"\n",
        "        embeddings = self._get_embeddings([text])\n",
        "        return embeddings[0]\n",
        "\n",
        "    def _get_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Internal method to generate embeddings for a list of texts.\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        for text in texts:\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # Use the mean of the last hidden state as the embedding\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().tolist()\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# Document processor for loading and processing uploaded files\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap\n",
        "        )\n",
        "        self.temp_dir = None\n",
        "\n",
        "    def process_uploaded_file(self, file_obj: BinaryIO, filename: str) -> List[Document]:\n",
        "        \"\"\"Process a single uploaded file.\"\"\"\n",
        "        # Create a temporary directory if not already created\n",
        "        if self.temp_dir is None:\n",
        "            self.temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "        # Get file extension\n",
        "        _, file_extension = os.path.splitext(filename)\n",
        "        file_extension = file_extension.lower()\n",
        "\n",
        "        # Save the file to the temporary directory\n",
        "        temp_file_path = os.path.join(self.temp_dir, filename)\n",
        "        with open(temp_file_path, 'wb') as f:\n",
        "            f.write(file_obj.read())\n",
        "\n",
        "        # Select appropriate loader based on file extension\n",
        "        loader = None\n",
        "        if file_extension == '.txt':\n",
        "            loader = TextLoader(temp_file_path)\n",
        "        elif file_extension == '.pdf':\n",
        "            loader = PDFMinerLoader(temp_file_path)\n",
        "        elif file_extension == '.md':\n",
        "            loader = UnstructuredMarkdownLoader(temp_file_path)\n",
        "        elif file_extension == '.csv':\n",
        "            loader = CSVLoader(temp_file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "        # Load and process the document\n",
        "        documents = loader.load()\n",
        "        chunks = self.process_documents(documents)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks for embedding.\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for doc in documents:\n",
        "            try:\n",
        "                doc_chunks = self.text_splitter.split_documents([doc])\n",
        "                chunks.extend(doc_chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {doc.metadata.get('source', 'unknown')}: {e}\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Remove temporary files when done.\"\"\"\n",
        "        if self.temp_dir and os.path.exists(self.temp_dir):\n",
        "            import shutil\n",
        "            shutil.rmtree(self.temp_dir)\n",
        "            self.temp_dir = None\n",
        "\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Vector store for document storage and retrieval\n",
        "class VectorStore:\n",
        "    def __init__(self, config: RAGConfig, embedding_model: SentenceTransformerEmbedding):\n",
        "        self.config = config\n",
        "        self.embedding_model = embedding_model\n",
        "        self.vector_store = None\n",
        "\n",
        "        # Define a persistent directory for ChromaDB\n",
        "        self.persist_directory = self.config.vector_db_path\n",
        "\n",
        "    def create_vector_store(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Create a vector store from documents.\"\"\"\n",
        "        # Use Chroma.from_documents method\n",
        "        self.vector_store = Chroma.from_documents(\n",
        "            documents,\n",
        "            self.embedding_model,\n",
        "            persist_directory=self.persist_directory\n",
        "        )\n",
        "\n",
        "        # Persist the data\n",
        "        self.vector_store.persist()\n",
        "\n",
        "    def add_documents(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Add documents to an existing vector store.\"\"\"\n",
        "        if self.vector_store is None:\n",
        "            # If no vector store exists, create a new one\n",
        "            self.create_vector_store(documents)\n",
        "        else:\n",
        "            # Add documents to existing vector store\n",
        "            self.vector_store.add_documents(documents)\n",
        "            # Persist the updated vector store\n",
        "            self.vector_store.persist()\n",
        "\n",
        "    def load_vector_store(self) -> bool:\n",
        "        \"\"\"Load the vector store if it exists.\"\"\"\n",
        "        # Check if the persist directory exists\n",
        "        if os.path.exists(self.persist_directory):\n",
        "            try:\n",
        "                self.vector_store = Chroma(\n",
        "                    persist_directory=self.persist_directory,\n",
        "                    embedding_function=self.embedding_model\n",
        "                )\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading vector store: {e}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "    def similarity_search(self, query: str) -> List[Document]:\n",
        "        \"\"\"Search for similar documents to the query.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not initialized. Please create or load a vector store first.\")\n",
        "\n",
        "        # Use similarity_search_with_score method\n",
        "        results = self.vector_store.similarity_search_with_score(\n",
        "            query,\n",
        "            k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        # Filter results by similarity threshold\n",
        "        # Note: ChromaDB returns distance (lower is better), similar to FAISS\n",
        "        filtered_results = [\n",
        "            doc for doc, score in results\n",
        "            if 1.0 / (1.0 + score) >= self.config.similarity_threshold  # Convert distance to similarity\n",
        "        ]\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "# Local LLM for RAG\n",
        "class LocalLLM:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading LLM model {config.llm_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.llm_model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            config.llm_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(f\"LLM model loaded successfully on {config.device}\")\n",
        "\n",
        "    def generate(self, prompt: str, system_message: str = None) -> str:\n",
        "        \"\"\"Generate text using local model.\"\"\"\n",
        "        # Format the messages\n",
        "        if system_message:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        else:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        try:\n",
        "            # Format messages for chat format\n",
        "            formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        except Exception as e:\n",
        "            # Fallback if the model doesn't support chat templates\n",
        "            print(f\"Chat template error: {e}, using simple prompt formatting\")\n",
        "            formatted_prompt = system_message + \"\\n\\n\" + prompt if system_message else prompt\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.config.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=300,  # Adjust based on use case\n",
        "                temperature=0.5,  # Lower temperature for faster convergence\n",
        "                do_sample=False,  # Use deterministic generation\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        # Decode the output, removing the input tokens\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        response_tokens = outputs[0][input_length:]\n",
        "        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "\n",
        "\n",
        "# Agentic RAG system that combines all components\n",
        "class AgenticRAG:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.embedding_model = SentenceTransformerEmbedding(config)\n",
        "        self.document_processor = DocumentProcessor(config)\n",
        "        self.vector_store = VectorStore(config, self.embedding_model)\n",
        "        self.llm = LocalLLM(config)\n",
        "\n",
        "    def ingest_uploaded_file(self, file_obj: BinaryIO, filename: str) -> None:\n",
        "        \"\"\"Ingest a single uploaded file.\"\"\"\n",
        "        print(f\"Processing uploaded file: {filename}...\")\n",
        "\n",
        "        try:\n",
        "            # Process the uploaded file\n",
        "            chunks = self.document_processor.process_uploaded_file(file_obj, filename)\n",
        "            print(f\"Created {len(chunks)} chunks from {filename}.\")\n",
        "\n",
        "            # Try to load the vector store first\n",
        "            vector_store_exists = self.vector_store.load_vector_store()\n",
        "\n",
        "            if vector_store_exists:\n",
        "                # Add the new documents to the existing vector store\n",
        "                print(\"Adding documents to existing vector store...\")\n",
        "                self.vector_store.add_documents(chunks)\n",
        "            else:\n",
        "                # Create a new vector store if none exists\n",
        "                print(\"Creating new vector store...\")\n",
        "                self.vector_store.create_vector_store(chunks)\n",
        "\n",
        "            print(f\"Successfully ingested {filename}.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error ingesting file {filename}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def query(self, user_query: str) -> str:\n",
        "        # Existing code...\n",
        "\n",
        "        # Retrieve documents with scores\n",
        "        results = self.vector_store.vector_store.similarity_search_with_score(\n",
        "            user_query,\n",
        "            k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        # Display retrieved documents with scores\n",
        "        print(\"\\nRetrieved Documents:\")\n",
        "        if results:\n",
        "            for i, (doc, score) in enumerate(results):\n",
        "                print(f\"\\nDocument {i+1} (Score: {score}):\")\n",
        "                print(doc.page_content)\n",
        "                if hasattr(doc, 'metadata') and doc.metadata:\n",
        "                    print(f\"Metadata: {doc.metadata}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "            # Filter for the actual processing\n",
        "            retrieved_docs = [doc for doc, score in results\n",
        "                            if 1.0 / (1.0 + score) >= self.config.similarity_threshold]\n",
        "        else:\n",
        "            print(\"No relevant documents found.\")\n",
        "            retrieved_docs = []\n",
        "\n",
        "        # Agentic thinking process\n",
        "        response = self._agentic_process(user_query, retrieved_docs)\n",
        "\n",
        "        return response\n",
        "    def _agentic_process(self, user_query: str, retrieved_docs=None) -> str:\n",
        "        \"\"\"Execute the agentic process for responding to queries.\"\"\"\n",
        "        system_message = \"\"\"You are an intelligent agent with access to a knowledge base.\n",
        "        Your task is to provide accurate, relevant information based on the query and the retrieved context.\n",
        "        Think step by step and analyze the retrieved information carefully before formulating your final response.\"\"\"\n",
        "\n",
        "        # Initial retrieval if not provided\n",
        "        if retrieved_docs is None:\n",
        "            retrieved_docs = self.vector_store.similarity_search(user_query)\n",
        "\n",
        "    # Rest of the method remains the same...\n",
        "        if not retrieved_docs:\n",
        "            # Handle the case when no relevant documents are found\n",
        "            prompt = f\"\"\"Query: {user_query}\n",
        "\n",
        "            No relevant documents were found in the knowledge base. Please provide a general response based on your knowledge.\n",
        "            \"\"\"\n",
        "            return self.llm.generate(prompt, system_message)\n",
        "\n",
        "        # For agentic reasoning, we'll use a multi-step process\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "        iteration_responses = []\n",
        "        current_query = user_query\n",
        "\n",
        "        for iteration in range(self.config.max_iterations):\n",
        "            print(f\"Iteration {iteration+1}/{self.config.max_iterations}\")\n",
        "\n",
        "            # Check if we should continue\n",
        "            if iteration > 0 and not self._should_continue(current_query, iteration_responses[-1]):\n",
        "                break\n",
        "\n",
        "            # Generate thinking steps if enabled\n",
        "            thinking = \"\"\n",
        "            if self.config.thinking_steps:\n",
        "                thinking_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Think step by step about this query. What are the key points to address? What information from the context is most relevant? What additional information might be needed?\n",
        "                \"\"\"\n",
        "                thinking = self.llm.generate(thinking_prompt, system_message)\n",
        "\n",
        "            # Generate the response\n",
        "            response_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            {thinking if thinking else \"\"}\n",
        "\n",
        "            Based on the context provided, please answer the query. If the context doesn't contain enough information, acknowledge this and provide the best answer you can.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.llm.generate(response_prompt, system_message)\n",
        "            iteration_responses.append(response)\n",
        "\n",
        "            # Generate follow-up questions or refinements\n",
        "            refinement_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Your current response:\n",
        "            {response}\n",
        "\n",
        "            Are there aspects of the query that haven't been fully addressed? What follow-up questions would help provide a more complete answer? How could the search be refined?\n",
        "            \"\"\"\n",
        "\n",
        "            refinement = self.llm.generate(refinement_prompt, system_message)\n",
        "\n",
        "            # Extract a new query for the next iteration\n",
        "            new_query_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "            Current response:\n",
        "            {response}\n",
        "\n",
        "            Refinement thoughts:\n",
        "            {refinement}\n",
        "\n",
        "            Based on the above, formulate a new search query that would help address any gaps in the current response. Return ONLY the new query without any explanation.\n",
        "            If you believe the query has been fully addressed, return \"COMPLETE\".\n",
        "            \"\"\"\n",
        "\n",
        "            new_query = self.llm.generate(new_query_prompt, system_message).strip()\n",
        "\n",
        "            if new_query == \"COMPLETE\" or new_query.upper().startswith(\"COMPLETE\"):\n",
        "                break\n",
        "\n",
        "            # Perform a new search with the refined query\n",
        "            current_query = new_query\n",
        "            # Inside the iteration loop in _agentic_process\n",
        "            new_docs = self.vector_store.similarity_search(current_query)\n",
        "\n",
        "            if new_docs:\n",
        "                print(f\"\\nAdditional Documents for Iteration {iteration+1}:\")\n",
        "                for i, doc in enumerate(new_docs):\n",
        "                    print(f\"\\nDocument {i+1}:\")\n",
        "                    print(doc.page_content)\n",
        "                    print(f\"Score: {doc.metadata.get('score', 'N/A')}\")\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "                new_context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(new_docs)])\n",
        "                # Update context with new information\n",
        "                context = f\"{context}\\n\\nAdditional Context:\\n{new_context}\"\n",
        "\n",
        "        # Final synthesis\n",
        "        final_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "        Iterations of responses:\n",
        "        {' '.join([f\"Iteration {i+1}: {resp}\" for i, resp in enumerate(iteration_responses)])}\n",
        "\n",
        "        Please provide a final, comprehensive response to the original query that synthesizes all the information gathered across iterations.\n",
        "        \"\"\"\n",
        "\n",
        "        final_response = self.llm.generate(final_prompt, system_message)\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    def _should_continue(self, query: str, last_response: str) -> bool:\n",
        "        \"\"\"Determine if the agent should continue iterating.\"\"\"\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Current response:\n",
        "        {last_response}\n",
        "\n",
        "        Does this response fully address the query? If yes, respond with \"COMPLETE\". If not, respond with \"CONTINUE\" and briefly explain why.\n",
        "        \"\"\"\n",
        "\n",
        "        decision = self.llm.generate(prompt)\n",
        "        return \"CONTINUE\" in decision.upper()\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up temporary files.\"\"\"\n",
        "        self.document_processor.cleanup()\n",
        "\n",
        "\n",
        "# Google Colab integration for file upload and RAG system\n",
        "def run_in_colab():\n",
        "    from google.colab import files\n",
        "    import io\n",
        "\n",
        "    config = RAGConfig()\n",
        "    print(f\"Initializing AgenticRAG with models on {config.device}\")\n",
        "    rag_system = AgenticRAG(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nAgenticRAG with Local LLM\")\n",
        "        print(\"1. Upload and ingest a file\")\n",
        "        print(\"2. Ask a question\")\n",
        "        print(\"3. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-3): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            try:\n",
        "                print(\"Please select a file to upload...\")\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                for filename, content in uploaded.items():\n",
        "                    file_obj = io.BytesIO(content)\n",
        "                    rag_system.ingest_uploaded_file(file_obj, filename)\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading and ingesting file: {e}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            query = input(\"Enter your question: \")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                response = rag_system.query(query)\n",
        "                end_time = time.time()\n",
        "\n",
        "                print(f\"\\nResponse (took {end_time - start_time:.2f} seconds):\")\n",
        "                print(response)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing query: {e}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Thank you for using AgenticRAG with Local LLM. Goodbye!\")\n",
        "            # Clean up any temporary files\n",
        "            rag_system.cleanup()\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "\n",
        "# For standalone execution outside of Colab\n",
        "def main():\n",
        "    try:\n",
        "        # Check if running in Google Colab\n",
        "        import google.colab\n",
        "        print(\"Running in Google Colab environment\")\n",
        "        run_in_colab()\n",
        "    except ImportError:\n",
        "        print(\"Not running in Google Colab, falling back to command-line interface\")\n",
        "        # Original command-line interface code here\n",
        "        # (You can keep the original code if needed, but it's not the focus now)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "twCszzE3TEw5",
        "outputId": "e6151563-d13b-4995-bf8b-28973c742933"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e042ec5e003c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-e042ec5e003c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running in Google Colab environment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mrun_in_colab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not running in Google Colab, falling back to command-line interface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-e042ec5e003c>\u001b[0m in \u001b[0;36mrun_in_colab\u001b[0;34m()\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3. Exit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your choice (1-3): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}